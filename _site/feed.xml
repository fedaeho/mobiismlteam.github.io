<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="ko" /><updated>2020-10-26T17:38:36+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Mobiis ML Team - Tech blog</title><subtitle>Mobiis Machine Learning Team - Tech blog
</subtitle><author><name>mobiismlteam</name></author><entry><title type="html">GAN Basic</title><link href="http://localhost:4000/2020/10/26/GAN_basic.html" rel="alternate" type="text/html" title="GAN Basic" /><published>2020-10-26T00:00:00+09:00</published><updated>2020-10-26T00:00:00+09:00</updated><id>http://localhost:4000/2020/10/26/GAN_basic</id><content type="html" xml:base="http://localhost:4000/2020/10/26/GAN_basic.html">&lt;h1 id=&quot;gan&quot;&gt;GAN&lt;/h1&gt;
&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GANs : Generative Adversarial Networks
    &lt;ul&gt;
      &lt;li&gt;Generative : 생성의&lt;/li&gt;
      &lt;li&gt;Adversarial : 대립 관계의, 적대적인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generator(생성자) 와 Discriminator(감별자) 두 개의 네트워크로 구성되어 있음
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://3.bp.blogspot.com/-BgYz6OQc4WU/WchaisOCgOI/AAAAAAAACI0/ONloRtdmVisug_HbkotMbP9tr2hkyfg-ACK4BGAYYCw/s1600/kakao_report2.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;일반적인 비유 : 지폐 위조범과 경찰
        &lt;ul&gt;
          &lt;li&gt;위조범은 더욱 진짜같은 가짜 위조지폐를 만드려고 한다 : Generator&lt;/li&gt;
          &lt;li&gt;경찰은 지폐 감별능력을 높여 위조지폐를 잡아야 한다 : Discriminator&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;최종 : 위조범의 능력이 정점에 달하면, 경찰은 진짜와 위조 지폐를 찍어서 맞추는 수밖에 없다.($D(x)=\frac{1}{2},p=0.5$)
    &lt;ul&gt;
      &lt;li&gt;경찰의 능력이 좋아지면, 위조범은 더 정밀한 위폐를 만들어야한다. -&amp;gt; 위조범의 능력이 좋아지면 경찰도 감별능력을 높여야한다. -&amp;gt; 피드백&lt;/li&gt;
      &lt;li&gt;실제로는 이렇게 이상적으로 돌아가지 않는다..
        &lt;ul&gt;
          &lt;li&gt;mode-collapse : 위조범이 1000원만 기가막히게 만들었고 경찰이 구분을 못한다. -&amp;gt; 계속 1000원만 만들것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;equations&quot;&gt;Equations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$x$ : sample from real data&lt;/li&gt;
  &lt;li&gt;$z \sim p_z$ : latent variable ($p_z$ : generally gaussian)&lt;/li&gt;
  &lt;li&gt;$G(z) \sim p_g$&lt;/li&gt;
  &lt;li&gt;objective
    &lt;ul&gt;
      &lt;li&gt;Generator : $p_g=p_{data}$&lt;/li&gt;
      &lt;li&gt;Discriminator : $D(x)=1, D(G(z))=0$
\(V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D(x;\theta_D)}]+
       \mathbb{E}_{z\sim p_{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]\)&lt;/li&gt;
      &lt;li&gt;D가 완벽할 때
        &lt;ul&gt;
          &lt;li&gt;$x \sim p_{data}(x), D(x)=0 \implies 1st~term=0$&lt;/li&gt;
          &lt;li&gt;$z \sim p_{z}(z), D(G(z))=0 \implies 2nd~term=0$&lt;/li&gt;
          &lt;li&gt;$\therefore V(D,G)=0 \implies \max_{D} V(D,G)$&lt;/li&gt;
          &lt;li&gt;D의 목적은 V 값의 최대화&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;G가 완벽할 때
        &lt;ul&gt;
          &lt;li&gt;1st term 은 z와 관계없으므로 constant&lt;/li&gt;
          &lt;li&gt;$z \sim p_{z}(z), D(G(z))=1 \implies 2nd~term=-\infty$&lt;/li&gt;
          &lt;li&gt;$\therefore V(D,G)=-\infty \implies \min_{G} V(D,G)$&lt;/li&gt;
          &lt;li&gt;G의 목적은 V값의 최소화&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;re-cap objective : find $G,D$ which satisfy $\min_{G} \max_{D} V(D,G)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;임의의 G에 대한 최적의 D : $D^{*}&lt;em&gt;G(x) = \frac{p&lt;/em&gt;{data}(x)}{p_{data}(x) + p_g(x)}$&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$\max_D V(D,G) = C(G) = -\log(4) + 2 \cdot JSD(p_{data}&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;p_g)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;re-cap objective : $\min_G C(G) : \min_G JSD(p_{data}&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;p_g)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;ul&gt;
      &lt;li&gt;JSD(Jensen Shannon Divergence) : sum of commuted KL-divergence&lt;/li&gt;
      &lt;li&gt;JSD 측도를 최소화하는 문제와 동일&lt;/li&gt;
      &lt;li&gt;G는 임의 형태의 pdf -&amp;gt; non-parametric 한 G를 만들기 위해 NN을 사용 -&amp;gt; GANs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proscons&quot;&gt;Pros/Cons&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GAN : sampler
    &lt;ul&gt;
      &lt;li&gt;최적화시에 $p_g = p_{data}$ 가 되도록하는 $G(z) \sim p_g$을 만든다.
        &lt;ul&gt;
          &lt;li&gt;$z$의 mapping function 을 구하는 것이 목적이므로, $x \sim p_{data}$ 를 직접 구하는 것과는 다르다. : 데이터 분포를 직접 구하는 것이 아니다.&lt;/li&gt;
          &lt;li&gt;따라서, 데이터 분포 자체를 구하기 위해 tracktable likelihood 를 가정하는 다른 모델과 다르게, likelihood-free 하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GAN 학습의 어려움
    &lt;ul&gt;
      &lt;li&gt;Convergence
        &lt;ul&gt;
          &lt;li&gt;D,G 를 동시에 구한다 -&amp;gt; saddle point 를 고려하지 않으면 학습이 영원이 이루어지지 않을 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;mode collapse
        &lt;ul&gt;
          &lt;li&gt;NN으로 푸는 현실적인 GAN의 경우, 매 단계마다 최적의 $D^{*}$ 를 구할 수 없다. 따라서, value function 을 G와 D에 대해 번갈아가면서 풀어야한다.
            &lt;ul&gt;
              &lt;li&gt;$G^{*} = \min_{G} \max_{D} V(G,D)$&lt;/li&gt;
              &lt;li&gt;G,D 에 대해 번갈아가며 풀경우, 위 식은 $G^{*} = \max_{D} \min_{G} V(G,D)$ 와 다르지 않다.
                &lt;ul&gt;
                  &lt;li&gt;$\min_{G}$ 를 먼저 푼다. : D가 가장 헷갈려 할만한 샘플 하나만 만들면 땡 -&amp;gt; 쉬운것만 만드는 G가 된다. -&amp;gt; latent variable $z$에 대한 변화가 크지 않은 $G(z)$가 만들어진다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$\max_{D} V(G,D)$ 를 먼저 푼다.
    &lt;ul&gt;
      &lt;li&gt;고정된 G를 두고 다음과 같은 데이터를 D에게 제공한다.
        &lt;ul&gt;
          &lt;li&gt;생성기 데이터와 라벨 : (G(z), 0.0)&lt;/li&gt;
          &lt;li&gt;진짜 데이터와 라벨 : (x,1.0)&lt;/li&gt;
          &lt;li&gt;Discriminator 에만 back-prop&lt;/li&gt;
          &lt;li&gt;Loss function : $L_D (\theta_G, \theta_D) = -V(G,D) = - \mathbb{E}&lt;em&gt;{x\sim p&lt;/em&gt;{data}(x)}[\log{D(x;\theta_D)}] - \mathbb{E}&lt;em&gt;{z\sim p&lt;/em&gt;{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$ : binary crossentropy 와 동일함&lt;/li&gt;
          &lt;li&gt;$\theta_D$ 만 업데이트&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;고정된 D를 두고 G를 업데이트 한다
    &lt;ul&gt;
      &lt;li&gt;Loss function : $L_G (\theta_G, \theta_D) = - \mathbb{E}&lt;em&gt;{z\sim p&lt;/em&gt;{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$&lt;/li&gt;
      &lt;li&gt;위 식으로 풀면($G=arg\min_{G}L_G$) G가 처음에 만드는 G(z)는 당연히 이상할 확률이 높으므로, 학습 초기에 값이 잘 변하지 않는다.&lt;/li&gt;
      &lt;li&gt;따라서, 함수를 $L_G (\theta_G, \theta_D) = - \mathbb{E}&lt;em&gt;{z\sim p&lt;/em&gt;{z}(z)}[\log{D(G(z;\theta_G);\theta_D)}]$ 로 바꾸어, $G=arg\max_{G}L_G$ 찾는 문제로 변경한다.&lt;/li&gt;
      &lt;li&gt;이후 $\theta_G$ 만 업데이트&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DCGAN(Deep-convolutional GAN)
    &lt;ul&gt;
      &lt;li&gt;Maxpooling, Upsampling 대신 strides&amp;gt;1 convolution 을 사용하여 feature map 크기를 조정하는 방법을 학습하게 함&lt;/li&gt;
      &lt;li&gt;Dense 는 z 받을때만&lt;/li&gt;
      &lt;li&gt;Batch normalization 을 하지만, G 출력과 D 입력에는 사용&lt;/li&gt;
      &lt;li&gt;Generator에서는 출력에 tanh(MNIST는 sigmoid), 나머지는 ReLU&lt;/li&gt;
      &lt;li&gt;Discriminator에서는 전무 Leaky ReLU, alpha=0.2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>mobiismlteam</name></author><category term="GAN" /><summary type="html">GAN Definition GANs : Generative Adversarial Networks Generative : 생성의 Adversarial : 대립 관계의, 적대적인 Generator(생성자) 와 Discriminator(감별자) 두 개의 네트워크로 구성되어 있음 일반적인 비유 : 지폐 위조범과 경찰 위조범은 더욱 진짜같은 가짜 위조지폐를 만드려고 한다 : Generator 경찰은 지폐 감별능력을 높여 위조지폐를 잡아야 한다 : Discriminator 최종 : 위조범의 능력이 정점에 달하면, 경찰은 진짜와 위조 지폐를 찍어서 맞추는 수밖에 없다.($D(x)=\frac{1}{2},p=0.5$) 경찰의 능력이 좋아지면, 위조범은 더 정밀한 위폐를 만들어야한다. -&amp;gt; 위조범의 능력이 좋아지면 경찰도 감별능력을 높여야한다. -&amp;gt; 피드백 실제로는 이렇게 이상적으로 돌아가지 않는다.. mode-collapse : 위조범이 1000원만 기가막히게 만들었고 경찰이 구분을 못한다. -&amp;gt; 계속 1000원만 만들것이다. Equations $x$ : sample from real data $z \sim p_z$ : latent variable ($p_z$ : generally gaussian) $G(z) \sim p_g$ objective Generator : $p_g=p_{data}$ Discriminator : $D(x)=1, D(G(z))=0$ \(V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D(x;\theta_D)}]+ \mathbb{E}_{z\sim p_{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]\) D가 완벽할 때 $x \sim p_{data}(x), D(x)=0 \implies 1st~term=0$ $z \sim p_{z}(z), D(G(z))=0 \implies 2nd~term=0$ $\therefore V(D,G)=0 \implies \max_{D} V(D,G)$ D의 목적은 V 값의 최대화 G가 완벽할 때 1st term 은 z와 관계없으므로 constant $z \sim p_{z}(z), D(G(z))=1 \implies 2nd~term=-\infty$ $\therefore V(D,G)=-\infty \implies \min_{G} V(D,G)$ G의 목적은 V값의 최소화 re-cap objective : find $G,D$ which satisfy $\min_{G} \max_{D} V(D,G)$ 임의의 G에 대한 최적의 D : $D^{*}G(x) = \frac{p{data}(x)}{p_{data}(x) + p_g(x)}$ $\max_D V(D,G) = C(G) = -\log(4) + 2 \cdot JSD(p_{data}   p_g)$ re-cap objective : $\min_G C(G) : \min_G JSD(p_{data}   p_g)$ JSD(Jensen Shannon Divergence) : sum of commuted KL-divergence JSD 측도를 최소화하는 문제와 동일 G는 임의 형태의 pdf -&amp;gt; non-parametric 한 G를 만들기 위해 NN을 사용 -&amp;gt; GANs Pros/Cons GAN : sampler 최적화시에 $p_g = p_{data}$ 가 되도록하는 $G(z) \sim p_g$을 만든다. $z$의 mapping function 을 구하는 것이 목적이므로, $x \sim p_{data}$ 를 직접 구하는 것과는 다르다. : 데이터 분포를 직접 구하는 것이 아니다. 따라서, 데이터 분포 자체를 구하기 위해 tracktable likelihood 를 가정하는 다른 모델과 다르게, likelihood-free 하다. GAN 학습의 어려움 Convergence D,G 를 동시에 구한다 -&amp;gt; saddle point 를 고려하지 않으면 학습이 영원이 이루어지지 않을 수 있다. mode collapse NN으로 푸는 현실적인 GAN의 경우, 매 단계마다 최적의 $D^{*}$ 를 구할 수 없다. 따라서, value function 을 G와 D에 대해 번갈아가면서 풀어야한다. $G^{*} = \min_{G} \max_{D} V(G,D)$ G,D 에 대해 번갈아가며 풀경우, 위 식은 $G^{*} = \max_{D} \min_{G} V(G,D)$ 와 다르지 않다. $\min_{G}$ 를 먼저 푼다. : D가 가장 헷갈려 할만한 샘플 하나만 만들면 땡 -&amp;gt; 쉬운것만 만드는 G가 된다. -&amp;gt; latent variable $z$에 대한 변화가 크지 않은 $G(z)$가 만들어진다. Training $\max_{D} V(G,D)$ 를 먼저 푼다. 고정된 G를 두고 다음과 같은 데이터를 D에게 제공한다. 생성기 데이터와 라벨 : (G(z), 0.0) 진짜 데이터와 라벨 : (x,1.0) Discriminator 에만 back-prop Loss function : $L_D (\theta_G, \theta_D) = -V(G,D) = - \mathbb{E}{x\sim p{data}(x)}[\log{D(x;\theta_D)}] - \mathbb{E}{z\sim p{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$ : binary crossentropy 와 동일함 $\theta_D$ 만 업데이트 고정된 D를 두고 G를 업데이트 한다 Loss function : $L_G (\theta_G, \theta_D) = - \mathbb{E}{z\sim p{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$ 위 식으로 풀면($G=arg\min_{G}L_G$) G가 처음에 만드는 G(z)는 당연히 이상할 확률이 높으므로, 학습 초기에 값이 잘 변하지 않는다. 따라서, 함수를 $L_G (\theta_G, \theta_D) = - \mathbb{E}{z\sim p{z}(z)}[\log{D(G(z;\theta_G);\theta_D)}]$ 로 바꾸어, $G=arg\max_{G}L_G$ 찾는 문제로 변경한다. 이후 $\theta_G$ 만 업데이트 Example DCGAN(Deep-convolutional GAN) Maxpooling, Upsampling 대신 strides&amp;gt;1 convolution 을 사용하여 feature map 크기를 조정하는 방법을 학습하게 함 Dense 는 z 받을때만 Batch normalization 을 하지만, G 출력과 D 입력에는 사용 Generator에서는 출력에 tanh(MNIST는 sigmoid), 나머지는 ReLU Discriminator에서는 전무 Leaky ReLU, alpha=0.2</summary></entry><entry><title type="html">ALOCC</title><link href="http://localhost:4000/2020/10/26/alocc.html" rel="alternate" type="text/html" title="ALOCC" /><published>2020-10-26T00:00:00+09:00</published><updated>2020-10-26T00:00:00+09:00</updated><id>http://localhost:4000/2020/10/26/alocc</id><content type="html" xml:base="http://localhost:4000/2020/10/26/alocc.html">&lt;h1 id=&quot;tutorial&quot;&gt;Tutorial&lt;/h1&gt;
&lt;h2 id=&quot;how-to-do-novelty-detection-in-keras-with-generative-adversarial-network--dlology&quot;&gt;&lt;a href=&quot;https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network-part-2/&quot;&gt;How to do Novelty Detection in Keras with Generative Adversarial Network&lt;/a&gt; | DLology&lt;/h2&gt;

&lt;p&gt;This notebook is for test phase Novelty Detection. To Train the model, run this first.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python models.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is recommended to understand how the model works in general before continuing the implementation.&lt;/p&gt;

&lt;p&gt;→ &lt;a href=&quot;https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network/&quot;&gt;How to do Novelty Detection in Keras with Generative Adversarial Network (Part 1)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;kh_tools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;models&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;imp&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.utils.vis_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.losses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_crossentropy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Using TensorFlow backend.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mnist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#build alocc model
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;generator
Model: &quot;R&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z (InputLayer)               (None, 28, 28, 1)         0         
_________________________________________________________________
g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
_________________________________________________________________
batch_normalization_10 (Batc (None, 14, 14, 32)        128       
_________________________________________________________________
leaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 32)        0         
_________________________________________________________________
g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
_________________________________________________________________
batch_normalization_11 (Batc (None, 7, 7, 64)          256       
_________________________________________________________________
leaky_re_lu_13 (LeakyReLU)   (None, 7, 7, 64)          0         
_________________________________________________________________
g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
_________________________________________________________________
batch_normalization_12 (Batc (None, 4, 4, 128)         512       
_________________________________________________________________
leaky_re_lu_14 (LeakyReLU)   (None, 4, 4, 128)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 4, 4, 16)          51216     
_________________________________________________________________
up_sampling2d_4 (UpSampling2 (None, 8, 8, 16)          0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 8, 8, 16)          6416      
_________________________________________________________________
up_sampling2d_5 (UpSampling2 (None, 16, 16, 16)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 14, 14, 32)        4640      
_________________________________________________________________
up_sampling2d_6 (UpSampling2 (None, 28, 28, 32)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 28, 1)         801       
=================================================================
Total params: 320,993
Trainable params: 320,545
Non-trainable params: 448
_________________________________________________________________

discriminator
Model: &quot;D&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
d_input (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 16)        0         
_________________________________________________________________
d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
_________________________________________________________________
batch_normalization_7 (Batch (None, 7, 7, 32)          128       
_________________________________________________________________
leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 32)          0         
_________________________________________________________________
d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
_________________________________________________________________
batch_normalization_8 (Batch (None, 4, 4, 64)          256       
_________________________________________________________________
leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 64)          0         
_________________________________________________________________
d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
_________________________________________________________________
batch_normalization_9 (Batch (None, 2, 2, 128)         512       
_________________________________________________________________
leaky_re_lu_11 (LeakyReLU)   (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 512)               0         
_________________________________________________________________
d_h3_lin (Dense)             (None, 1)                 513       
=================================================================
Total params: 541,250
Trainable params: 270,401
Non-trainable params: 270,849
_________________________________________________________________

adversarial_model
Model: &quot;model_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
R (Model)                    (None, 28, 28, 1)         320993    
_________________________________________________________________
D (Model)                    (None, 1)                 270849    
=================================================================
Total params: 591,842
Trainable params: 320,545
Non-trainable params: 271,297
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch (0/5)-------------------------------------------------


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'


Epoch:[0]-[0/52] --&amp;gt; d_loss_real: 1.568, d_loss_fake: 15.802, g_loss:0.140, g_recon_loss:0.560
Epoch:[0]-[1/52] --&amp;gt; d_loss_real: 0.003, d_loss_fake: 0.289, g_loss:14.704, g_recon_loss:4.606
Epoch:[0]-[2/52] --&amp;gt; d_loss_real: 2.517, d_loss_fake: 0.004, g_loss:1.125, g_recon_loss:3.327
Epoch:[0]-[3/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.040, g_loss:5.021, g_recon_loss:2.537


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'


Epoch:[0]-[4/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 1.368, g_loss:10.722, g_recon_loss:1.165
Epoch:[0]-[5/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 3.883, g_loss:0.332, g_recon_loss:0.501
Epoch:[0]-[6/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 2.541, g_loss:2.681, g_recon_loss:1.133
Epoch:[0]-[7/52] --&amp;gt; d_loss_real: 4.097, d_loss_fake: 2.061, g_loss:1.831, g_recon_loss:0.559
Epoch:[0]-[8/52] --&amp;gt; d_loss_real: 0.246, d_loss_fake: 1.715, g_loss:3.590, g_recon_loss:0.496
Epoch:[0]-[9/52] --&amp;gt; d_loss_real: 3.208, d_loss_fake: 0.835, g_loss:1.735, g_recon_loss:0.363
Epoch:[0]-[10/52] --&amp;gt; d_loss_real: 1.503, d_loss_fake: 0.857, g_loss:1.453, g_recon_loss:0.385
Epoch:[0]-[11/52] --&amp;gt; d_loss_real: 1.292, d_loss_fake: 0.790, g_loss:1.403, g_recon_loss:0.438
Epoch:[0]-[12/52] --&amp;gt; d_loss_real: 0.958, d_loss_fake: 0.600, g_loss:1.202, g_recon_loss:0.484
Epoch:[0]-[13/52] --&amp;gt; d_loss_real: 0.562, d_loss_fake: 0.416, g_loss:1.478, g_recon_loss:0.567
Epoch:[0]-[14/52] --&amp;gt; d_loss_real: 0.396, d_loss_fake: 0.565, g_loss:1.847, g_recon_loss:0.345
Epoch:[0]-[15/52] --&amp;gt; d_loss_real: 1.056, d_loss_fake: 0.523, g_loss:0.148, g_recon_loss:0.480
Epoch:[0]-[16/52] --&amp;gt; d_loss_real: 0.061, d_loss_fake: 1.098, g_loss:3.442, g_recon_loss:0.356
Epoch:[0]-[17/52] --&amp;gt; d_loss_real: 3.898, d_loss_fake: 0.921, g_loss:0.794, g_recon_loss:0.328
Epoch:[0]-[18/52] --&amp;gt; d_loss_real: 0.732, d_loss_fake: 1.429, g_loss:1.608, g_recon_loss:0.282
Epoch:[0]-[19/52] --&amp;gt; d_loss_real: 1.315, d_loss_fake: 0.591, g_loss:0.848, g_recon_loss:0.305
Epoch:[0]-[20/52] --&amp;gt; d_loss_real: 0.672, d_loss_fake: 0.728, g_loss:0.950, g_recon_loss:0.268
Epoch:[0]-[21/52] --&amp;gt; d_loss_real: 0.845, d_loss_fake: 0.930, g_loss:1.257, g_recon_loss:0.362
Epoch:[0]-[22/52] --&amp;gt; d_loss_real: 0.944, d_loss_fake: 0.565, g_loss:0.750, g_recon_loss:0.431
Epoch:[0]-[23/52] --&amp;gt; d_loss_real: 0.349, d_loss_fake: 0.072, g_loss:0.113, g_recon_loss:0.469
Epoch:[0]-[24/52] --&amp;gt; d_loss_real: 0.009, d_loss_fake: 0.034, g_loss:0.079, g_recon_loss:0.330
Epoch:[0]-[25/52] --&amp;gt; d_loss_real: 0.016, d_loss_fake: 0.054, g_loss:0.072, g_recon_loss:0.241
Epoch:[0]-[26/52] --&amp;gt; d_loss_real: 0.039, d_loss_fake: 0.018, g_loss:0.058, g_recon_loss:0.268
Epoch:[0]-[27/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 0.005, g_loss:0.051, g_recon_loss:0.242
Epoch:[0]-[28/52] --&amp;gt; d_loss_real: 0.013, d_loss_fake: 0.009, g_loss:0.046, g_recon_loss:0.218
Epoch:[0]-[29/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 0.002, g_loss:0.040, g_recon_loss:0.193
Epoch:[0]-[30/52] --&amp;gt; d_loss_real: 0.004, d_loss_fake: 0.002, g_loss:0.038, g_recon_loss:0.186
Epoch:[0]-[31/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.034, g_recon_loss:0.163
Epoch:[0]-[32/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.036, g_recon_loss:0.177
Epoch:[0]-[33/52] --&amp;gt; d_loss_real: 0.003, d_loss_fake: 0.001, g_loss:0.033, g_recon_loss:0.160
Epoch:[0]-[34/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.133
Epoch:[0]-[35/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.132
Epoch:[0]-[36/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.028, g_recon_loss:0.138
Epoch:[0]-[37/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.000, g_loss:0.023, g_recon_loss:0.115
Epoch:[0]-[38/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.019, g_recon_loss:0.096
Epoch:[0]-[39/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.021, g_recon_loss:0.105
Epoch:[0]-[40/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.020, g_recon_loss:0.100
Epoch:[0]-[41/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087
Epoch:[0]-[42/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.087
Epoch:[0]-[43/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087
Epoch:[0]-[44/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.084
Epoch:[0]-[45/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.082
Epoch:[0]-[46/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.083
Epoch:[0]-[47/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.074
Epoch:[0]-[48/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078
Epoch:[0]-[49/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078
Epoch:[0]-[50/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.014, g_recon_loss:0.069
Epoch:[0]-[51/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.090
Epoch (1/5)-------------------------------------------------
Epoch:[1]-[0/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.024, g_recon_loss:0.117
Epoch:[1]-[1/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.082
Epoch:[1]-[2/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073
Epoch:[1]-[3/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073
Epoch:[1]-[4/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.080
Epoch:[1]-[5/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.029, g_recon_loss:0.127
Epoch:[1]-[6/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.004, g_loss:1.339, g_recon_loss:1.790
Epoch:[1]-[7/52] --&amp;gt; d_loss_real: 1.568, d_loss_fake: 8.227, g_loss:0.794, g_recon_loss:0.825
Epoch:[1]-[8/52] --&amp;gt; d_loss_real: 0.275, d_loss_fake: 2.447, g_loss:1.562, g_recon_loss:0.372
Epoch:[1]-[9/52] --&amp;gt; d_loss_real: 1.099, d_loss_fake: 1.998, g_loss:1.119, g_recon_loss:0.339
Epoch:[1]-[10/52] --&amp;gt; d_loss_real: 1.060, d_loss_fake: 1.677, g_loss:1.042, g_recon_loss:0.434
Epoch:[1]-[11/52] --&amp;gt; d_loss_real: 1.158, d_loss_fake: 1.530, g_loss:1.140, g_recon_loss:0.437
Epoch:[1]-[12/52] --&amp;gt; d_loss_real: 1.099, d_loss_fake: 1.001, g_loss:1.057, g_recon_loss:0.348
Epoch:[1]-[13/52] --&amp;gt; d_loss_real: 1.024, d_loss_fake: 1.001, g_loss:1.008, g_recon_loss:0.305
Epoch:[1]-[14/52] --&amp;gt; d_loss_real: 1.029, d_loss_fake: 0.949, g_loss:1.013, g_recon_loss:0.362
Epoch:[1]-[15/52] --&amp;gt; d_loss_real: 0.975, d_loss_fake: 1.022, g_loss:0.967, g_recon_loss:0.292
Epoch:[1]-[16/52] --&amp;gt; d_loss_real: 1.007, d_loss_fake: 0.976, g_loss:0.887, g_recon_loss:0.373
Epoch:[1]-[17/52] --&amp;gt; d_loss_real: 1.004, d_loss_fake: 0.983, g_loss:1.300, g_recon_loss:1.357
Epoch:[1]-[18/52] --&amp;gt; d_loss_real: 0.853, d_loss_fake: 1.114, g_loss:1.058, g_recon_loss:0.367
Epoch:[1]-[19/52] --&amp;gt; d_loss_real: 0.947, d_loss_fake: 0.921, g_loss:1.021, g_recon_loss:0.264
Epoch:[1]-[20/52] --&amp;gt; d_loss_real: 0.949, d_loss_fake: 0.949, g_loss:0.969, g_recon_loss:0.214
Epoch:[1]-[21/52] --&amp;gt; d_loss_real: 0.934, d_loss_fake: 0.953, g_loss:0.944, g_recon_loss:0.207
Epoch:[1]-[22/52] --&amp;gt; d_loss_real: 0.928, d_loss_fake: 0.922, g_loss:0.934, g_recon_loss:0.180
Epoch:[1]-[23/52] --&amp;gt; d_loss_real: 0.934, d_loss_fake: 0.894, g_loss:0.935, g_recon_loss:0.226
Epoch:[1]-[24/52] --&amp;gt; d_loss_real: 0.925, d_loss_fake: 0.892, g_loss:0.940, g_recon_loss:0.245
Epoch:[1]-[25/52] --&amp;gt; d_loss_real: 0.906, d_loss_fake: 0.907, g_loss:0.964, g_recon_loss:0.223
Epoch:[1]-[26/52] --&amp;gt; d_loss_real: 0.907, d_loss_fake: 0.908, g_loss:0.935, g_recon_loss:0.171
Epoch:[1]-[27/52] --&amp;gt; d_loss_real: 0.898, d_loss_fake: 0.880, g_loss:0.950, g_recon_loss:0.185
Epoch:[1]-[28/52] --&amp;gt; d_loss_real: 0.871, d_loss_fake: 0.872, g_loss:0.951, g_recon_loss:0.172
Epoch:[1]-[29/52] --&amp;gt; d_loss_real: 0.887, d_loss_fake: 0.929, g_loss:0.964, g_recon_loss:0.205
Epoch:[1]-[30/52] --&amp;gt; d_loss_real: 0.938, d_loss_fake: 0.907, g_loss:0.946, g_recon_loss:0.193
Epoch:[1]-[31/52] --&amp;gt; d_loss_real: 0.940, d_loss_fake: 0.887, g_loss:1.039, g_recon_loss:0.439
Epoch:[1]-[32/52] --&amp;gt; d_loss_real: 0.984, d_loss_fake: 0.947, g_loss:1.002, g_recon_loss:0.168
Epoch:[1]-[33/52] --&amp;gt; d_loss_real: 0.984, d_loss_fake: 0.883, g_loss:0.949, g_recon_loss:0.197
Epoch:[1]-[34/52] --&amp;gt; d_loss_real: 0.899, d_loss_fake: 0.906, g_loss:0.908, g_recon_loss:0.131
Epoch:[1]-[35/52] --&amp;gt; d_loss_real: 0.862, d_loss_fake: 0.887, g_loss:0.911, g_recon_loss:0.136
Epoch:[1]-[36/52] --&amp;gt; d_loss_real: 0.864, d_loss_fake: 0.882, g_loss:0.921, g_recon_loss:0.125
Epoch:[1]-[37/52] --&amp;gt; d_loss_real: 0.873, d_loss_fake: 0.873, g_loss:0.872, g_recon_loss:0.160
Epoch:[1]-[38/52] --&amp;gt; d_loss_real: 0.881, d_loss_fake: 0.878, g_loss:0.990, g_recon_loss:0.303
Epoch:[1]-[39/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 1.022, g_loss:0.993, g_recon_loss:0.335
Epoch:[1]-[40/52] --&amp;gt; d_loss_real: 0.919, d_loss_fake: 0.852, g_loss:0.965, g_recon_loss:0.190
Epoch:[1]-[41/52] --&amp;gt; d_loss_real: 0.897, d_loss_fake: 0.827, g_loss:0.966, g_recon_loss:0.210
Epoch:[1]-[42/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.837, g_loss:0.939, g_recon_loss:0.226
Epoch:[1]-[43/52] --&amp;gt; d_loss_real: 0.880, d_loss_fake: 0.793, g_loss:1.035, g_recon_loss:0.405
Epoch:[1]-[44/52] --&amp;gt; d_loss_real: 0.866, d_loss_fake: 0.927, g_loss:0.969, g_recon_loss:0.387
Epoch:[1]-[45/52] --&amp;gt; d_loss_real: 0.709, d_loss_fake: 0.952, g_loss:0.959, g_recon_loss:0.199
Epoch:[1]-[46/52] --&amp;gt; d_loss_real: 0.776, d_loss_fake: 1.326, g_loss:0.999, g_recon_loss:0.159
Epoch:[1]-[47/52] --&amp;gt; d_loss_real: 0.945, d_loss_fake: 0.751, g_loss:0.923, g_recon_loss:0.218
Epoch:[1]-[48/52] --&amp;gt; d_loss_real: 0.871, d_loss_fake: 1.031, g_loss:0.897, g_recon_loss:0.191
Epoch:[1]-[49/52] --&amp;gt; d_loss_real: 0.933, d_loss_fake: 0.926, g_loss:0.928, g_recon_loss:0.198
Epoch:[1]-[50/52] --&amp;gt; d_loss_real: 0.938, d_loss_fake: 0.875, g_loss:0.951, g_recon_loss:0.208
Epoch:[1]-[51/52] --&amp;gt; d_loss_real: 0.900, d_loss_fake: 0.853, g_loss:0.919, g_recon_loss:0.160
Epoch (2/5)-------------------------------------------------
Epoch:[2]-[0/52] --&amp;gt; d_loss_real: 0.924, d_loss_fake: 0.857, g_loss:0.924, g_recon_loss:0.155
Epoch:[2]-[1/52] --&amp;gt; d_loss_real: 0.920, d_loss_fake: 0.840, g_loss:0.927, g_recon_loss:0.180
Epoch:[2]-[2/52] --&amp;gt; d_loss_real: 0.910, d_loss_fake: 0.840, g_loss:0.915, g_recon_loss:0.157
Epoch:[2]-[3/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.879, g_loss:0.885, g_recon_loss:0.147
Epoch:[2]-[4/52] --&amp;gt; d_loss_real: 0.855, d_loss_fake: 0.847, g_loss:0.906, g_recon_loss:0.202
Epoch:[2]-[5/52] --&amp;gt; d_loss_real: 0.842, d_loss_fake: 0.860, g_loss:0.884, g_recon_loss:0.131
Epoch:[2]-[6/52] --&amp;gt; d_loss_real: 0.827, d_loss_fake: 0.892, g_loss:0.901, g_recon_loss:0.169
Epoch:[2]-[7/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.852, g_loss:0.913, g_recon_loss:0.238
Epoch:[2]-[8/52] --&amp;gt; d_loss_real: 0.887, d_loss_fake: 0.839, g_loss:0.925, g_recon_loss:0.258
Epoch:[2]-[9/52] --&amp;gt; d_loss_real: 0.941, d_loss_fake: 0.850, g_loss:0.920, g_recon_loss:0.170
Epoch:[2]-[10/52] --&amp;gt; d_loss_real: 0.893, d_loss_fake: 0.905, g_loss:0.853, g_recon_loss:0.145
Epoch:[2]-[11/52] --&amp;gt; d_loss_real: 0.832, d_loss_fake: 0.898, g_loss:0.900, g_recon_loss:0.292
Epoch:[2]-[12/52] --&amp;gt; d_loss_real: 0.828, d_loss_fake: 0.923, g_loss:0.862, g_recon_loss:0.147
Epoch:[2]-[13/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.868, g_loss:0.871, g_recon_loss:0.157
Epoch:[2]-[14/52] --&amp;gt; d_loss_real: 0.864, d_loss_fake: 0.864, g_loss:0.877, g_recon_loss:0.129
Epoch:[2]-[15/52] --&amp;gt; d_loss_real: 0.860, d_loss_fake: 0.875, g_loss:0.885, g_recon_loss:0.112
Epoch:[2]-[16/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.846, g_loss:0.882, g_recon_loss:0.164
Epoch:[2]-[17/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.833, g_loss:0.906, g_recon_loss:0.161
Epoch:[2]-[18/52] --&amp;gt; d_loss_real: 0.915, d_loss_fake: 0.832, g_loss:0.904, g_recon_loss:0.195
Epoch:[2]-[19/52] --&amp;gt; d_loss_real: 0.881, d_loss_fake: 0.915, g_loss:0.884, g_recon_loss:0.110
Epoch:[2]-[20/52] --&amp;gt; d_loss_real: 0.855, d_loss_fake: 0.878, g_loss:0.862, g_recon_loss:0.102
Epoch:[2]-[21/52] --&amp;gt; d_loss_real: 0.848, d_loss_fake: 0.856, g_loss:0.874, g_recon_loss:0.130
Epoch:[2]-[22/52] --&amp;gt; d_loss_real: 0.850, d_loss_fake: 0.860, g_loss:0.860, g_recon_loss:0.106
Epoch:[2]-[23/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 0.843, g_loss:0.872, g_recon_loss:0.139
Epoch:[2]-[24/52] --&amp;gt; d_loss_real: 0.862, d_loss_fake: 0.908, g_loss:0.881, g_recon_loss:0.156
Epoch:[2]-[25/52] --&amp;gt; d_loss_real: 0.860, d_loss_fake: 0.876, g_loss:0.882, g_recon_loss:0.100
Epoch:[2]-[26/52] --&amp;gt; d_loss_real: 0.865, d_loss_fake: 0.852, g_loss:0.879, g_recon_loss:0.112
Epoch:[2]-[27/52] --&amp;gt; d_loss_real: 0.868, d_loss_fake: 0.840, g_loss:0.871, g_recon_loss:0.114
Epoch:[2]-[28/52] --&amp;gt; d_loss_real: 0.853, d_loss_fake: 0.857, g_loss:0.874, g_recon_loss:0.114
Epoch:[2]-[29/52] --&amp;gt; d_loss_real: 0.859, d_loss_fake: 0.839, g_loss:0.867, g_recon_loss:0.112
Epoch:[2]-[30/52] --&amp;gt; d_loss_real: 0.858, d_loss_fake: 0.818, g_loss:0.843, g_recon_loss:0.200
Epoch:[2]-[31/52] --&amp;gt; d_loss_real: 0.837, d_loss_fake: 0.901, g_loss:0.856, g_recon_loss:0.086
Epoch:[2]-[32/52] --&amp;gt; d_loss_real: 0.852, d_loss_fake: 0.867, g_loss:0.855, g_recon_loss:0.100
Epoch:[2]-[33/52] --&amp;gt; d_loss_real: 0.836, d_loss_fake: 0.878, g_loss:0.856, g_recon_loss:0.104
Epoch:[2]-[34/52] --&amp;gt; d_loss_real: 0.841, d_loss_fake: 0.859, g_loss:0.853, g_recon_loss:0.081
Epoch:[2]-[35/52] --&amp;gt; d_loss_real: 0.844, d_loss_fake: 0.835, g_loss:0.853, g_recon_loss:0.103
Epoch:[2]-[36/52] --&amp;gt; d_loss_real: 0.842, d_loss_fake: 0.868, g_loss:0.861, g_recon_loss:0.097
Epoch:[2]-[37/52] --&amp;gt; d_loss_real: 0.848, d_loss_fake: 0.827, g_loss:0.852, g_recon_loss:0.104
Epoch:[2]-[38/52] --&amp;gt; d_loss_real: 0.850, d_loss_fake: 0.833, g_loss:0.902, g_recon_loss:0.216
Epoch:[2]-[39/52] --&amp;gt; d_loss_real: 0.918, d_loss_fake: 0.835, g_loss:0.892, g_recon_loss:0.144
Epoch:[2]-[40/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 0.835, g_loss:0.832, g_recon_loss:0.140
Epoch:[2]-[41/52] --&amp;gt; d_loss_real: 0.804, d_loss_fake: 0.893, g_loss:0.826, g_recon_loss:0.074
Epoch:[2]-[42/52] --&amp;gt; d_loss_real: 0.809, d_loss_fake: 0.866, g_loss:0.822, g_recon_loss:0.088
Epoch:[2]-[43/52] --&amp;gt; d_loss_real: 0.807, d_loss_fake: 0.846, g_loss:0.821, g_recon_loss:0.095
Epoch:[2]-[44/52] --&amp;gt; d_loss_real: 0.812, d_loss_fake: 0.850, g_loss:0.841, g_recon_loss:0.074
Epoch:[2]-[45/52] --&amp;gt; d_loss_real: 0.824, d_loss_fake: 0.822, g_loss:0.839, g_recon_loss:0.292
Epoch:[2]-[46/52] --&amp;gt; d_loss_real: 0.841, d_loss_fake: 0.902, g_loss:0.872, g_recon_loss:0.107
Epoch:[2]-[47/52] --&amp;gt; d_loss_real: 0.839, d_loss_fake: 0.857, g_loss:0.859, g_recon_loss:0.088
Epoch:[2]-[48/52] --&amp;gt; d_loss_real: 0.833, d_loss_fake: 0.827, g_loss:0.841, g_recon_loss:0.091
Epoch:[2]-[49/52] --&amp;gt; d_loss_real: 0.821, d_loss_fake: 0.818, g_loss:0.834, g_recon_loss:0.089
Epoch:[2]-[50/52] --&amp;gt; d_loss_real: 0.819, d_loss_fake: 0.829, g_loss:0.832, g_recon_loss:0.102
Epoch:[2]-[51/52] --&amp;gt; d_loss_real: 0.835, d_loss_fake: 0.840, g_loss:0.808, g_recon_loss:0.092
Epoch (3/5)-------------------------------------------------
Epoch:[3]-[0/52] --&amp;gt; d_loss_real: 0.804, d_loss_fake: 0.869, g_loss:0.826, g_recon_loss:0.089
Epoch:[3]-[1/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.856, g_loss:0.849, g_recon_loss:0.087
Epoch:[3]-[2/52] --&amp;gt; d_loss_real: 0.835, d_loss_fake: 0.820, g_loss:0.833, g_recon_loss:0.113
Epoch:[3]-[3/52] --&amp;gt; d_loss_real: 0.831, d_loss_fake: 0.856, g_loss:0.872, g_recon_loss:0.086
Epoch:[3]-[4/52] --&amp;gt; d_loss_real: 0.847, d_loss_fake: 0.816, g_loss:0.829, g_recon_loss:0.129
Epoch:[3]-[5/52] --&amp;gt; d_loss_real: 0.817, d_loss_fake: 0.863, g_loss:0.826, g_recon_loss:0.088
Epoch:[3]-[6/52] --&amp;gt; d_loss_real: 0.809, d_loss_fake: 0.792, g_loss:0.796, g_recon_loss:0.127
Epoch:[3]-[7/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.891, g_loss:0.850, g_recon_loss:0.087
Epoch:[3]-[8/52] --&amp;gt; d_loss_real: 0.810, d_loss_fake: 0.841, g_loss:0.792, g_recon_loss:0.079
Epoch:[3]-[9/52] --&amp;gt; d_loss_real: 0.788, d_loss_fake: 0.872, g_loss:0.814, g_recon_loss:0.092
Epoch:[3]-[10/52] --&amp;gt; d_loss_real: 0.798, d_loss_fake: 0.854, g_loss:0.830, g_recon_loss:0.090
Epoch:[3]-[11/52] --&amp;gt; d_loss_real: 0.822, d_loss_fake: 0.840, g_loss:0.817, g_recon_loss:0.122
Epoch:[3]-[12/52] --&amp;gt; d_loss_real: 0.796, d_loss_fake: 0.838, g_loss:0.814, g_recon_loss:0.096
Epoch:[3]-[13/52] --&amp;gt; d_loss_real: 0.787, d_loss_fake: 0.861, g_loss:0.836, g_recon_loss:0.075
Epoch:[3]-[14/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.804, g_loss:0.828, g_recon_loss:0.207
Epoch:[3]-[15/52] --&amp;gt; d_loss_real: 0.880, d_loss_fake: 0.782, g_loss:0.939, g_recon_loss:0.113
Epoch:[3]-[16/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.762, g_loss:0.888, g_recon_loss:0.113
Epoch:[3]-[17/52] --&amp;gt; d_loss_real: 0.833, d_loss_fake: 0.776, g_loss:0.828, g_recon_loss:0.093
Epoch:[3]-[18/52] --&amp;gt; d_loss_real: 0.791, d_loss_fake: 0.902, g_loss:0.707, g_recon_loss:0.112
Epoch:[3]-[19/52] --&amp;gt; d_loss_real: 0.714, d_loss_fake: 0.940, g_loss:0.769, g_recon_loss:0.090
Epoch:[3]-[20/52] --&amp;gt; d_loss_real: 0.755, d_loss_fake: 0.927, g_loss:0.844, g_recon_loss:0.091
Epoch:[3]-[21/52] --&amp;gt; d_loss_real: 0.823, d_loss_fake: 0.844, g_loss:0.801, g_recon_loss:0.086
Epoch:[3]-[22/52] --&amp;gt; d_loss_real: 0.793, d_loss_fake: 0.828, g_loss:0.780, g_recon_loss:0.095
Epoch:[3]-[23/52] --&amp;gt; d_loss_real: 0.784, d_loss_fake: 0.850, g_loss:0.811, g_recon_loss:0.082
Epoch:[3]-[24/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.819, g_loss:0.780, g_recon_loss:0.079
Epoch:[3]-[25/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.865, g_loss:0.822, g_recon_loss:0.080
Epoch:[3]-[26/52] --&amp;gt; d_loss_real: 0.811, d_loss_fake: 0.803, g_loss:0.791, g_recon_loss:0.092
Epoch:[3]-[27/52] --&amp;gt; d_loss_real: 0.798, d_loss_fake: 0.849, g_loss:0.851, g_recon_loss:0.080
Epoch:[3]-[28/52] --&amp;gt; d_loss_real: 0.818, d_loss_fake: 0.781, g_loss:0.804, g_recon_loss:0.105
Epoch:[3]-[29/52] --&amp;gt; d_loss_real: 0.794, d_loss_fake: 0.822, g_loss:0.814, g_recon_loss:0.072
Epoch:[3]-[30/52] --&amp;gt; d_loss_real: 0.799, d_loss_fake: 0.762, g_loss:0.765, g_recon_loss:0.144
Epoch:[3]-[31/52] --&amp;gt; d_loss_real: 0.788, d_loss_fake: 0.838, g_loss:0.872, g_recon_loss:0.094
Epoch:[3]-[32/52] --&amp;gt; d_loss_real: 0.856, d_loss_fake: 0.792, g_loss:0.830, g_recon_loss:0.087
Epoch:[3]-[33/52] --&amp;gt; d_loss_real: 0.807, d_loss_fake: 0.795, g_loss:0.776, g_recon_loss:0.101
Epoch:[3]-[34/52] --&amp;gt; d_loss_real: 0.749, d_loss_fake: 0.859, g_loss:0.779, g_recon_loss:0.084
Epoch:[3]-[35/52] --&amp;gt; d_loss_real: 0.771, d_loss_fake: 0.891, g_loss:0.790, g_recon_loss:0.138
Epoch:[3]-[36/52] --&amp;gt; d_loss_real: 0.766, d_loss_fake: 0.852, g_loss:0.811, g_recon_loss:0.078
Epoch:[3]-[37/52] --&amp;gt; d_loss_real: 0.785, d_loss_fake: 0.809, g_loss:0.781, g_recon_loss:0.081
Epoch:[3]-[38/52] --&amp;gt; d_loss_real: 0.760, d_loss_fake: 0.882, g_loss:0.816, g_recon_loss:0.078
Epoch:[3]-[39/52] --&amp;gt; d_loss_real: 0.795, d_loss_fake: 0.781, g_loss:0.786, g_recon_loss:0.158
Epoch:[3]-[40/52] --&amp;gt; d_loss_real: 0.890, d_loss_fake: 0.672, g_loss:0.965, g_recon_loss:0.113
Epoch:[3]-[41/52] --&amp;gt; d_loss_real: 0.954, d_loss_fake: 0.734, g_loss:0.837, g_recon_loss:0.103
Epoch:[3]-[42/52] --&amp;gt; d_loss_real: 0.834, d_loss_fake: 0.999, g_loss:0.643, g_recon_loss:0.145
Epoch:[3]-[43/52] --&amp;gt; d_loss_real: 0.625, d_loss_fake: 1.018, g_loss:0.752, g_recon_loss:0.110
Epoch:[3]-[44/52] --&amp;gt; d_loss_real: 0.716, d_loss_fake: 0.868, g_loss:0.782, g_recon_loss:0.093
Epoch:[3]-[45/52] --&amp;gt; d_loss_real: 0.741, d_loss_fake: 0.839, g_loss:0.795, g_recon_loss:0.089
Epoch:[3]-[46/52] --&amp;gt; d_loss_real: 0.740, d_loss_fake: 0.844, g_loss:0.778, g_recon_loss:0.088
Epoch:[3]-[47/52] --&amp;gt; d_loss_real: 0.729, d_loss_fake: 1.006, g_loss:0.762, g_recon_loss:0.090
Epoch:[3]-[48/52] --&amp;gt; d_loss_real: 0.755, d_loss_fake: 0.832, g_loss:0.789, g_recon_loss:0.092
Epoch:[3]-[49/52] --&amp;gt; d_loss_real: 0.777, d_loss_fake: 0.767, g_loss:0.771, g_recon_loss:0.092
Epoch:[3]-[50/52] --&amp;gt; d_loss_real: 0.800, d_loss_fake: 0.789, g_loss:0.863, g_recon_loss:0.097
Epoch:[3]-[51/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.745, g_loss:0.750, g_recon_loss:0.120
Epoch (4/5)-------------------------------------------------
Epoch:[4]-[0/52] --&amp;gt; d_loss_real: 0.739, d_loss_fake: 0.839, g_loss:0.773, g_recon_loss:0.093
Epoch:[4]-[1/52] --&amp;gt; d_loss_real: 0.761, d_loss_fake: 0.824, g_loss:0.790, g_recon_loss:0.084
Epoch:[4]-[2/52] --&amp;gt; d_loss_real: 0.766, d_loss_fake: 0.800, g_loss:0.793, g_recon_loss:0.083
Epoch:[4]-[3/52] --&amp;gt; d_loss_real: 0.774, d_loss_fake: 0.665, g_loss:1.410, g_recon_loss:0.104
Epoch:[4]-[4/52] --&amp;gt; d_loss_real: 2.101, d_loss_fake: 0.606, g_loss:1.045, g_recon_loss:0.180
Epoch:[4]-[5/52] --&amp;gt; d_loss_real: 0.825, d_loss_fake: 0.564, g_loss:0.892, g_recon_loss:0.160
Epoch:[4]-[6/52] --&amp;gt; d_loss_real: 0.592, d_loss_fake: 1.084, g_loss:0.857, g_recon_loss:0.229
Epoch:[4]-[7/52] --&amp;gt; d_loss_real: 0.737, d_loss_fake: 0.689, g_loss:0.775, g_recon_loss:0.235
Epoch:[4]-[8/52] --&amp;gt; d_loss_real: 0.469, d_loss_fake: 1.200, g_loss:0.965, g_recon_loss:0.221
Epoch:[4]-[9/52] --&amp;gt; d_loss_real: 0.501, d_loss_fake: 1.126, g_loss:0.942, g_recon_loss:0.201
Epoch:[4]-[10/52] --&amp;gt; d_loss_real: 0.740, d_loss_fake: 0.869, g_loss:0.778, g_recon_loss:0.154
Epoch:[4]-[11/52] --&amp;gt; d_loss_real: 0.679, d_loss_fake: 0.880, g_loss:0.854, g_recon_loss:0.140
Epoch:[4]-[12/52] --&amp;gt; d_loss_real: 0.712, d_loss_fake: 0.789, g_loss:0.703, g_recon_loss:0.195
Epoch:[4]-[13/52] --&amp;gt; d_loss_real: 0.691, d_loss_fake: 1.092, g_loss:0.807, g_recon_loss:0.158
Epoch:[4]-[14/52] --&amp;gt; d_loss_real: 0.744, d_loss_fake: 0.731, g_loss:0.771, g_recon_loss:0.192
Epoch:[4]-[15/52] --&amp;gt; d_loss_real: 0.812, d_loss_fake: 0.771, g_loss:0.764, g_recon_loss:0.134
Epoch:[4]-[16/52] --&amp;gt; d_loss_real: 0.702, d_loss_fake: 0.897, g_loss:0.876, g_recon_loss:0.195
Epoch:[4]-[17/52] --&amp;gt; d_loss_real: 0.756, d_loss_fake: 0.773, g_loss:0.822, g_recon_loss:0.148
Epoch:[4]-[18/52] --&amp;gt; d_loss_real: 0.769, d_loss_fake: 0.762, g_loss:0.799, g_recon_loss:0.162
Epoch:[4]-[19/52] --&amp;gt; d_loss_real: 0.943, d_loss_fake: 0.843, g_loss:0.805, g_recon_loss:0.152
Epoch:[4]-[20/52] --&amp;gt; d_loss_real: 0.734, d_loss_fake: 0.885, g_loss:0.776, g_recon_loss:0.134
Epoch:[4]-[21/52] --&amp;gt; d_loss_real: 0.765, d_loss_fake: 0.863, g_loss:0.862, g_recon_loss:0.221
Epoch:[4]-[22/52] --&amp;gt; d_loss_real: 0.802, d_loss_fake: 0.745, g_loss:0.798, g_recon_loss:0.128
Epoch:[4]-[23/52] --&amp;gt; d_loss_real: 0.771, d_loss_fake: 0.704, g_loss:0.805, g_recon_loss:0.229
Epoch:[4]-[24/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.673, g_loss:0.860, g_recon_loss:0.113
Epoch:[4]-[25/52] --&amp;gt; d_loss_real: 0.738, d_loss_fake: 0.793, g_loss:0.762, g_recon_loss:0.168
Epoch:[4]-[26/52] --&amp;gt; d_loss_real: 0.775, d_loss_fake: 0.800, g_loss:0.858, g_recon_loss:0.172
Epoch:[4]-[27/52] --&amp;gt; d_loss_real: 0.699, d_loss_fake: 0.835, g_loss:0.811, g_recon_loss:0.147
Epoch:[4]-[28/52] --&amp;gt; d_loss_real: 0.828, d_loss_fake: 0.718, g_loss:0.834, g_recon_loss:0.127
Epoch:[4]-[29/52] --&amp;gt; d_loss_real: 0.693, d_loss_fake: 0.818, g_loss:0.803, g_recon_loss:0.188
Epoch:[4]-[30/52] --&amp;gt; d_loss_real: 0.747, d_loss_fake: 0.648, g_loss:0.846, g_recon_loss:0.134
Epoch:[4]-[31/52] --&amp;gt; d_loss_real: 0.696, d_loss_fake: 0.839, g_loss:0.792, g_recon_loss:0.211
Epoch:[4]-[32/52] --&amp;gt; d_loss_real: 0.661, d_loss_fake: 0.805, g_loss:0.815, g_recon_loss:0.354
Epoch:[4]-[33/52] --&amp;gt; d_loss_real: 0.669, d_loss_fake: 0.975, g_loss:0.867, g_recon_loss:0.178
Epoch:[4]-[34/52] --&amp;gt; d_loss_real: 0.744, d_loss_fake: 0.803, g_loss:0.828, g_recon_loss:0.205
Epoch:[4]-[35/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.627, g_loss:0.791, g_recon_loss:0.189
Epoch:[4]-[36/52] --&amp;gt; d_loss_real: 0.980, d_loss_fake: 0.673, g_loss:0.817, g_recon_loss:0.139
Epoch:[4]-[37/52] --&amp;gt; d_loss_real: 0.759, d_loss_fake: 0.788, g_loss:0.830, g_recon_loss:0.157
Epoch:[4]-[38/52] --&amp;gt; d_loss_real: 0.649, d_loss_fake: 0.710, g_loss:0.764, g_recon_loss:0.106
Epoch:[4]-[39/52] --&amp;gt; d_loss_real: 0.615, d_loss_fake: 0.859, g_loss:0.736, g_recon_loss:0.113
Epoch:[4]-[40/52] --&amp;gt; d_loss_real: 0.663, d_loss_fake: 0.939, g_loss:0.754, g_recon_loss:0.124
Epoch:[4]-[41/52] --&amp;gt; d_loss_real: 0.650, d_loss_fake: 1.127, g_loss:0.957, g_recon_loss:0.142
Epoch:[4]-[42/52] --&amp;gt; d_loss_real: 0.772, d_loss_fake: 0.753, g_loss:0.806, g_recon_loss:0.191
Epoch:[4]-[43/52] --&amp;gt; d_loss_real: 0.917, d_loss_fake: 0.609, g_loss:0.746, g_recon_loss:0.150
Epoch:[4]-[44/52] --&amp;gt; d_loss_real: 0.943, d_loss_fake: 0.756, g_loss:0.784, g_recon_loss:0.188
Epoch:[4]-[45/52] --&amp;gt; d_loss_real: 0.758, d_loss_fake: 0.768, g_loss:0.829, g_recon_loss:0.156
Epoch:[4]-[46/52] --&amp;gt; d_loss_real: 0.701, d_loss_fake: 0.685, g_loss:0.820, g_recon_loss:0.136
Epoch:[4]-[47/52] --&amp;gt; d_loss_real: 0.712, d_loss_fake: 0.610, g_loss:0.745, g_recon_loss:0.175
Epoch:[4]-[48/52] --&amp;gt; d_loss_real: 0.717, d_loss_fake: 0.911, g_loss:0.785, g_recon_loss:0.155
Epoch:[4]-[49/52] --&amp;gt; d_loss_real: 0.780, d_loss_fake: 0.824, g_loss:0.807, g_recon_loss:0.147
Epoch:[4]-[50/52] --&amp;gt; d_loss_real: 0.691, d_loss_fake: 0.809, g_loss:0.871, g_recon_loss:0.140
Epoch:[4]-[51/52] --&amp;gt; d_loss_real: 0.728, d_loss_fake: 0.734, g_loss:0.748, g_recon_loss:0.151
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_4_5.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choose-a-stopping-criterion&quot;&gt;Choose a stopping criterion&lt;/h2&gt;
&lt;p&gt;The training procedure is stopped when R successfully maps noisy images to clean images carrying the concept of the target class.  When R can reconstruct its input with minimum error. In the following case, we pick the epoch 3.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# This image was generated at the end of the models.py training procedure to help pick a ending epoch to load. 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'plot_g_recon_losses.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# Load the epoch #3 saved weights.
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./checkpoint/ALOCC_Model_3.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;test-the-reconstruction-loss-and-discriminator-output&quot;&gt;Test the reconstruction loss and Discriminator output&lt;/h2&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;abnormal&lt;/code&gt; image has a &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;larger&lt;/code&gt; reconstruction loss&lt;/strong&gt; and &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;smaller&lt;/code&gt; discriminator output value&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_reconstruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;specific_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;datas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;specific_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_predicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reconstructed_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_predicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Input'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Input'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reconstruction'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reconstructed_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reconstructed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    
    &lt;span class=&quot;c1&quot;&gt;# Compute the mean binary_crossentropy loss of reconstructed image.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reconstructed_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_crossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Average reconstruction loss:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Average discriminator Output:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_predicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;D_Outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_Outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Inlier&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;outlier_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sublist&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_Outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sublist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outlier_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Outlier&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Discriminator Output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Output multiple Histograms&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper right'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'paper_original_result.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;normal-case&quot;&gt;Normal case&lt;/h3&gt;
&lt;p&gt;The network was trained with label == 1.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;test_reconstruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_12_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average reconstruction loss: 0.13995047
Average discriminator Output: 0.46022063
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;abnormal-cases&quot;&gt;Abnormal cases&lt;/h2&gt;
&lt;p&gt;The network was not trained on those labels, so the Generator/R network find it hard to reconstruct the input images reflected in higher reconstruction loss values.&lt;/p&gt;

&lt;p&gt;Discriminator also outputs a lower value compared to normal ones.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;test_reconstruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_14_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average reconstruction loss: 1.0208124
Average discriminator Output: 0.6488284
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;test_reconstruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_15_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average reconstruction loss: 1.1040308
Average discriminator Output: 0.6433686
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;test_reconstruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_16_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average reconstruction loss: 0.51506275
Average discriminator Output: 0.54887545
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;draw_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_17_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mobiis-추가-연구사항&quot;&gt;mobiis 추가 연구사항&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;D 모델을 조금더 복잡하게 구성&lt;/li&gt;
  &lt;li&gt;R 모델 학습을 조금 더디게 진행&lt;/li&gt;
  &lt;li&gt;노이즈 없이 학습&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-enhance-d-network---add-1-dense-layer&quot;&gt;1. Enhance D network - Add 1 dense layer&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;importlib&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'test/enhance_D/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;importlib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mnist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;generator
Model: &quot;R&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z (InputLayer)               (None, 28, 28, 1)         0         
_________________________________________________________________
g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
_________________________________________________________________
batch_normalization_70 (Batc (None, 14, 14, 32)        128       
_________________________________________________________________
leaky_re_lu_82 (LeakyReLU)   (None, 14, 14, 32)        0         
_________________________________________________________________
g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
_________________________________________________________________
batch_normalization_71 (Batc (None, 7, 7, 64)          256       
_________________________________________________________________
leaky_re_lu_83 (LeakyReLU)   (None, 7, 7, 64)          0         
_________________________________________________________________
g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
_________________________________________________________________
batch_normalization_72 (Batc (None, 4, 4, 128)         512       
_________________________________________________________________
leaky_re_lu_84 (LeakyReLU)   (None, 4, 4, 128)         0         
_________________________________________________________________
conv2d_45 (Conv2D)           (None, 4, 4, 16)          51216     
_________________________________________________________________
up_sampling2d_34 (UpSampling (None, 8, 8, 16)          0         
_________________________________________________________________
conv2d_46 (Conv2D)           (None, 8, 8, 16)          6416      
_________________________________________________________________
up_sampling2d_35 (UpSampling (None, 16, 16, 16)        0         
_________________________________________________________________
conv2d_47 (Conv2D)           (None, 14, 14, 32)        4640      
_________________________________________________________________
up_sampling2d_36 (UpSampling (None, 28, 28, 32)        0         
_________________________________________________________________
conv2d_48 (Conv2D)           (None, 28, 28, 1)         801       
=================================================================
Total params: 320,993
Trainable params: 320,545
Non-trainable params: 448
_________________________________________________________________

discriminator
Model: &quot;D&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
d_input (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
_________________________________________________________________
leaky_re_lu_78 (LeakyReLU)   (None, 14, 14, 16)        0         
_________________________________________________________________
d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
_________________________________________________________________
batch_normalization_67 (Batc (None, 7, 7, 32)          128       
_________________________________________________________________
leaky_re_lu_79 (LeakyReLU)   (None, 7, 7, 32)          0         
_________________________________________________________________
d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
_________________________________________________________________
batch_normalization_68 (Batc (None, 4, 4, 64)          256       
_________________________________________________________________
leaky_re_lu_80 (LeakyReLU)   (None, 4, 4, 64)          0         
_________________________________________________________________
d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
_________________________________________________________________
batch_normalization_69 (Batc (None, 2, 2, 128)         512       
_________________________________________________________________
leaky_re_lu_81 (LeakyReLU)   (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_12 (Flatten)         (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 32)                16416     
_________________________________________________________________
d_h3_lin (Dense)             (None, 1)                 33        
=================================================================
Total params: 573,122
Trainable params: 286,337
Non-trainable params: 286,785
_________________________________________________________________

adversarial_model
Model: &quot;model_12&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        (None, 28, 28, 1)         0         
_________________________________________________________________
R (Model)                    (None, 28, 28, 1)         320993    
_________________________________________________________________
D (Model)                    (None, 1)                 286785    
=================================================================
Total params: 607,778
Trainable params: 320,545
Non-trainable params: 287,233
_________________________________________________________________


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch (0/5)-------------------------------------------------


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'


Epoch (1/5)-------------------------------------------------
Epoch (2/5)-------------------------------------------------
Epoch (3/5)-------------------------------------------------
Epoch (4/5)-------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_21_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Load the epoch #3 saved weights.
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./checkpoint/ALOCC_Model_3.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;draw_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_23_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-slower-r-network-learning---기존-1-batch당-두번-학습에서-한번으로-변경&quot;&gt;2. Slower R network learning - 기존 1 batch당 두번 학습에서 한번으로 변경&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;importlib&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'test/slow_R/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;importlib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mnist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;generator
Model: &quot;R&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z (InputLayer)               (None, 28, 28, 1)         0         
_________________________________________________________________
g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
_________________________________________________________________
batch_normalization_58 (Batc (None, 14, 14, 32)        128       
_________________________________________________________________
leaky_re_lu_68 (LeakyReLU)   (None, 14, 14, 32)        0         
_________________________________________________________________
g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
_________________________________________________________________
batch_normalization_59 (Batc (None, 7, 7, 64)          256       
_________________________________________________________________
leaky_re_lu_69 (LeakyReLU)   (None, 7, 7, 64)          0         
_________________________________________________________________
g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
_________________________________________________________________
batch_normalization_60 (Batc (None, 4, 4, 128)         512       
_________________________________________________________________
leaky_re_lu_70 (LeakyReLU)   (None, 4, 4, 128)         0         
_________________________________________________________________
conv2d_37 (Conv2D)           (None, 4, 4, 16)          51216     
_________________________________________________________________
up_sampling2d_28 (UpSampling (None, 8, 8, 16)          0         
_________________________________________________________________
conv2d_38 (Conv2D)           (None, 8, 8, 16)          6416      
_________________________________________________________________
up_sampling2d_29 (UpSampling (None, 16, 16, 16)        0         
_________________________________________________________________
conv2d_39 (Conv2D)           (None, 14, 14, 32)        4640      
_________________________________________________________________
up_sampling2d_30 (UpSampling (None, 28, 28, 32)        0         
_________________________________________________________________
conv2d_40 (Conv2D)           (None, 28, 28, 1)         801       
=================================================================
Total params: 320,993
Trainable params: 320,545
Non-trainable params: 448
_________________________________________________________________

discriminator
Model: &quot;D&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
d_input (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
_________________________________________________________________
leaky_re_lu_64 (LeakyReLU)   (None, 14, 14, 16)        0         
_________________________________________________________________
d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
_________________________________________________________________
batch_normalization_55 (Batc (None, 7, 7, 32)          128       
_________________________________________________________________
leaky_re_lu_65 (LeakyReLU)   (None, 7, 7, 32)          0         
_________________________________________________________________
d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
_________________________________________________________________
batch_normalization_56 (Batc (None, 4, 4, 64)          256       
_________________________________________________________________
leaky_re_lu_66 (LeakyReLU)   (None, 4, 4, 64)          0         
_________________________________________________________________
d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
_________________________________________________________________
batch_normalization_57 (Batc (None, 2, 2, 128)         512       
_________________________________________________________________
leaky_re_lu_67 (LeakyReLU)   (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_10 (Flatten)         (None, 512)               0         
_________________________________________________________________
d_h3_lin (Dense)             (None, 1)                 513       
=================================================================
Total params: 541,250
Trainable params: 270,401
Non-trainable params: 270,849
_________________________________________________________________

adversarial_model
Model: &quot;model_10&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        (None, 28, 28, 1)         0         
_________________________________________________________________
R (Model)                    (None, 28, 28, 1)         320993    
_________________________________________________________________
D (Model)                    (None, 1)                 270849    
=================================================================
Total params: 591,842
Trainable params: 320,545
Non-trainable params: 271,297
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch (0/5)-------------------------------------------------


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'


Epoch (1/5)-------------------------------------------------
Epoch (2/5)-------------------------------------------------
Epoch (3/5)-------------------------------------------------
Epoch (4/5)-------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_27_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Load the epoch #4 saved weights.
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./checkpoint/ALOCC_Model_3.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;draw_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_29_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-learning-without-noise&quot;&gt;3. Learning without noise&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;importlib&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'test/without_noise/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;importlib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ALOCC_Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mnist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;generator
Model: &quot;R&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
z (InputLayer)               (None, 28, 28, 1)         0         
_________________________________________________________________
g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
_________________________________________________________________
batch_normalization_34 (Batc (None, 14, 14, 32)        128       
_________________________________________________________________
leaky_re_lu_40 (LeakyReLU)   (None, 14, 14, 32)        0         
_________________________________________________________________
g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
_________________________________________________________________
batch_normalization_35 (Batc (None, 7, 7, 64)          256       
_________________________________________________________________
leaky_re_lu_41 (LeakyReLU)   (None, 7, 7, 64)          0         
_________________________________________________________________
g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
_________________________________________________________________
batch_normalization_36 (Batc (None, 4, 4, 128)         512       
_________________________________________________________________
leaky_re_lu_42 (LeakyReLU)   (None, 4, 4, 128)         0         
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 4, 4, 16)          51216     
_________________________________________________________________
up_sampling2d_16 (UpSampling (None, 8, 8, 16)          0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 8, 8, 16)          6416      
_________________________________________________________________
up_sampling2d_17 (UpSampling (None, 16, 16, 16)        0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 14, 14, 32)        4640      
_________________________________________________________________
up_sampling2d_18 (UpSampling (None, 28, 28, 32)        0         
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 28, 28, 1)         801       
=================================================================
Total params: 320,993
Trainable params: 320,545
Non-trainable params: 448
_________________________________________________________________

discriminator
Model: &quot;D&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
d_input (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
_________________________________________________________________
leaky_re_lu_36 (LeakyReLU)   (None, 14, 14, 16)        0         
_________________________________________________________________
d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
_________________________________________________________________
batch_normalization_31 (Batc (None, 7, 7, 32)          128       
_________________________________________________________________
leaky_re_lu_37 (LeakyReLU)   (None, 7, 7, 32)          0         
_________________________________________________________________
d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
_________________________________________________________________
batch_normalization_32 (Batc (None, 4, 4, 64)          256       
_________________________________________________________________
leaky_re_lu_38 (LeakyReLU)   (None, 4, 4, 64)          0         
_________________________________________________________________
d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
_________________________________________________________________
batch_normalization_33 (Batc (None, 2, 2, 128)         512       
_________________________________________________________________
leaky_re_lu_39 (LeakyReLU)   (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 512)               0         
_________________________________________________________________
d_h3_lin (Dense)             (None, 1)                 513       
=================================================================
Total params: 541,250
Trainable params: 270,401
Non-trainable params: 270,849
_________________________________________________________________

adversarial_model
Model: &quot;model_6&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
R (Model)                    (None, 28, 28, 1)         320993    
_________________________________________________________________
D (Model)                    (None, 1)                 270849    
=================================================================
Total params: 591,842
Trainable params: 320,545
Non-trainable params: 271,297
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch (0/5)-------------------------------------------------


/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
/home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'


Epoch (1/5)-------------------------------------------------
Epoch (2/5)-------------------------------------------------
Epoch (3/5)-------------------------------------------------
Epoch (4/5)-------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_33_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Load the epoch #4 saved weights.
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adversarial_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./checkpoint/ALOCC_Model_3.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;draw_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/alocc_files/alocc_35_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;reconstruction&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;높혀서&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;트레이닝을&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;하면&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D모델을&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;속이려는&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;성향보다&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;이미지를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;깔끔하게&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;만드려는&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;성향이&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;강해지고&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reconstruction&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;낮혀서&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;트레이닝을&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;하면&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;이미지를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;깔끔하게&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;바꾸려는&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;성향보다&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;어떻게&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;하든&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D모델을&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;속이면&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;되는&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;쪽으로&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;학습이&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;된다&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>mobiismlteam</name></author><category term="NoveltyDetection" /><summary type="html">Tutorial How to do Novelty Detection in Keras with Generative Adversarial Network | DLology This notebook is for test phase Novelty Detection. To Train the model, run this first. python models.py It is recommended to understand how the model works in general before continuing the implementation. → How to do Novelty Detection in Keras with Generative Adversarial Network (Part 1) from utils import * from kh_tools import * import models import imp from keras.utils.vis_utils import plot_model imp.reload(models) from keras.datasets import mnist from keras.losses import binary_crossentropy from keras import backend as K import numpy as np import matplotlib.pyplot as plt %matplotlib inline (X_train, y_train), (_, _) = mnist.load_data() X_train = X_train / 255 Using TensorFlow backend. from models import ALOCC_Model self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28) #build alocc model generator Model: &quot;R&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= z (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ g_encoder_h0_conv (Conv2D) (None, 14, 14, 32) 832 _________________________________________________________________ batch_normalization_10 (Batc (None, 14, 14, 32) 128 _________________________________________________________________ leaky_re_lu_12 (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ g_encoder_h1_conv (Conv2D) (None, 7, 7, 64) 51264 _________________________________________________________________ batch_normalization_11 (Batc (None, 7, 7, 64) 256 _________________________________________________________________ leaky_re_lu_13 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ g_encoder_h2_conv (Conv2D) (None, 4, 4, 128) 204928 _________________________________________________________________ batch_normalization_12 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ leaky_re_lu_14 (LeakyReLU) (None, 4, 4, 128) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 4, 4, 16) 51216 _________________________________________________________________ up_sampling2d_4 (UpSampling2 (None, 8, 8, 16) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 16) 6416 _________________________________________________________________ up_sampling2d_5 (UpSampling2 (None, 16, 16, 16) 0 _________________________________________________________________ conv2d_7 (Conv2D) (None, 14, 14, 32) 4640 _________________________________________________________________ up_sampling2d_6 (UpSampling2 (None, 28, 28, 32) 0 _________________________________________________________________ conv2d_8 (Conv2D) (None, 28, 28, 1) 801 ================================================================= Total params: 320,993 Trainable params: 320,545 Non-trainable params: 448 _________________________________________________________________ discriminator Model: &quot;D&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= d_input (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ d_h0_conv (Conv2D) (None, 14, 14, 16) 416 _________________________________________________________________ leaky_re_lu_8 (LeakyReLU) (None, 14, 14, 16) 0 _________________________________________________________________ d_h1_conv (Conv2D) (None, 7, 7, 32) 12832 _________________________________________________________________ batch_normalization_7 (Batch (None, 7, 7, 32) 128 _________________________________________________________________ leaky_re_lu_9 (LeakyReLU) (None, 7, 7, 32) 0 _________________________________________________________________ d_h2_conv (Conv2D) (None, 4, 4, 64) 51264 _________________________________________________________________ batch_normalization_8 (Batch (None, 4, 4, 64) 256 _________________________________________________________________ leaky_re_lu_10 (LeakyReLU) (None, 4, 4, 64) 0 _________________________________________________________________ d_h3_conv (Conv2D) (None, 2, 2, 128) 204928 _________________________________________________________________ batch_normalization_9 (Batch (None, 2, 2, 128) 512 _________________________________________________________________ leaky_re_lu_11 (LeakyReLU) (None, 2, 2, 128) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 512) 0 _________________________________________________________________ d_h3_lin (Dense) (None, 1) 513 ================================================================= Total params: 541,250 Trainable params: 270,401 Non-trainable params: 270,849 _________________________________________________________________ adversarial_model Model: &quot;model_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ R (Model) (None, 28, 28, 1) 320993 _________________________________________________________________ D (Model) (None, 1) 270849 ================================================================= Total params: 591,842 Trainable params: 320,545 Non-trainable params: 271,297 _________________________________________________________________ Training self.train(epochs=5, batch_size=128, sample_interval=500) Epoch (0/5)------------------------------------------------- /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch:[0]-[0/52] --&amp;gt; d_loss_real: 1.568, d_loss_fake: 15.802, g_loss:0.140, g_recon_loss:0.560 Epoch:[0]-[1/52] --&amp;gt; d_loss_real: 0.003, d_loss_fake: 0.289, g_loss:14.704, g_recon_loss:4.606 Epoch:[0]-[2/52] --&amp;gt; d_loss_real: 2.517, d_loss_fake: 0.004, g_loss:1.125, g_recon_loss:3.327 Epoch:[0]-[3/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.040, g_loss:5.021, g_recon_loss:2.537 /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch:[0]-[4/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 1.368, g_loss:10.722, g_recon_loss:1.165 Epoch:[0]-[5/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 3.883, g_loss:0.332, g_recon_loss:0.501 Epoch:[0]-[6/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 2.541, g_loss:2.681, g_recon_loss:1.133 Epoch:[0]-[7/52] --&amp;gt; d_loss_real: 4.097, d_loss_fake: 2.061, g_loss:1.831, g_recon_loss:0.559 Epoch:[0]-[8/52] --&amp;gt; d_loss_real: 0.246, d_loss_fake: 1.715, g_loss:3.590, g_recon_loss:0.496 Epoch:[0]-[9/52] --&amp;gt; d_loss_real: 3.208, d_loss_fake: 0.835, g_loss:1.735, g_recon_loss:0.363 Epoch:[0]-[10/52] --&amp;gt; d_loss_real: 1.503, d_loss_fake: 0.857, g_loss:1.453, g_recon_loss:0.385 Epoch:[0]-[11/52] --&amp;gt; d_loss_real: 1.292, d_loss_fake: 0.790, g_loss:1.403, g_recon_loss:0.438 Epoch:[0]-[12/52] --&amp;gt; d_loss_real: 0.958, d_loss_fake: 0.600, g_loss:1.202, g_recon_loss:0.484 Epoch:[0]-[13/52] --&amp;gt; d_loss_real: 0.562, d_loss_fake: 0.416, g_loss:1.478, g_recon_loss:0.567 Epoch:[0]-[14/52] --&amp;gt; d_loss_real: 0.396, d_loss_fake: 0.565, g_loss:1.847, g_recon_loss:0.345 Epoch:[0]-[15/52] --&amp;gt; d_loss_real: 1.056, d_loss_fake: 0.523, g_loss:0.148, g_recon_loss:0.480 Epoch:[0]-[16/52] --&amp;gt; d_loss_real: 0.061, d_loss_fake: 1.098, g_loss:3.442, g_recon_loss:0.356 Epoch:[0]-[17/52] --&amp;gt; d_loss_real: 3.898, d_loss_fake: 0.921, g_loss:0.794, g_recon_loss:0.328 Epoch:[0]-[18/52] --&amp;gt; d_loss_real: 0.732, d_loss_fake: 1.429, g_loss:1.608, g_recon_loss:0.282 Epoch:[0]-[19/52] --&amp;gt; d_loss_real: 1.315, d_loss_fake: 0.591, g_loss:0.848, g_recon_loss:0.305 Epoch:[0]-[20/52] --&amp;gt; d_loss_real: 0.672, d_loss_fake: 0.728, g_loss:0.950, g_recon_loss:0.268 Epoch:[0]-[21/52] --&amp;gt; d_loss_real: 0.845, d_loss_fake: 0.930, g_loss:1.257, g_recon_loss:0.362 Epoch:[0]-[22/52] --&amp;gt; d_loss_real: 0.944, d_loss_fake: 0.565, g_loss:0.750, g_recon_loss:0.431 Epoch:[0]-[23/52] --&amp;gt; d_loss_real: 0.349, d_loss_fake: 0.072, g_loss:0.113, g_recon_loss:0.469 Epoch:[0]-[24/52] --&amp;gt; d_loss_real: 0.009, d_loss_fake: 0.034, g_loss:0.079, g_recon_loss:0.330 Epoch:[0]-[25/52] --&amp;gt; d_loss_real: 0.016, d_loss_fake: 0.054, g_loss:0.072, g_recon_loss:0.241 Epoch:[0]-[26/52] --&amp;gt; d_loss_real: 0.039, d_loss_fake: 0.018, g_loss:0.058, g_recon_loss:0.268 Epoch:[0]-[27/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 0.005, g_loss:0.051, g_recon_loss:0.242 Epoch:[0]-[28/52] --&amp;gt; d_loss_real: 0.013, d_loss_fake: 0.009, g_loss:0.046, g_recon_loss:0.218 Epoch:[0]-[29/52] --&amp;gt; d_loss_real: 0.005, d_loss_fake: 0.002, g_loss:0.040, g_recon_loss:0.193 Epoch:[0]-[30/52] --&amp;gt; d_loss_real: 0.004, d_loss_fake: 0.002, g_loss:0.038, g_recon_loss:0.186 Epoch:[0]-[31/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.034, g_recon_loss:0.163 Epoch:[0]-[32/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.036, g_recon_loss:0.177 Epoch:[0]-[33/52] --&amp;gt; d_loss_real: 0.003, d_loss_fake: 0.001, g_loss:0.033, g_recon_loss:0.160 Epoch:[0]-[34/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.133 Epoch:[0]-[35/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.132 Epoch:[0]-[36/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.028, g_recon_loss:0.138 Epoch:[0]-[37/52] --&amp;gt; d_loss_real: 0.002, d_loss_fake: 0.000, g_loss:0.023, g_recon_loss:0.115 Epoch:[0]-[38/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.019, g_recon_loss:0.096 Epoch:[0]-[39/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.021, g_recon_loss:0.105 Epoch:[0]-[40/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.020, g_recon_loss:0.100 Epoch:[0]-[41/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087 Epoch:[0]-[42/52] --&amp;gt; d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.087 Epoch:[0]-[43/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087 Epoch:[0]-[44/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.084 Epoch:[0]-[45/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.082 Epoch:[0]-[46/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.083 Epoch:[0]-[47/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.074 Epoch:[0]-[48/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078 Epoch:[0]-[49/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078 Epoch:[0]-[50/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.014, g_recon_loss:0.069 Epoch:[0]-[51/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.090 Epoch (1/5)------------------------------------------------- Epoch:[1]-[0/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.024, g_recon_loss:0.117 Epoch:[1]-[1/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.082 Epoch:[1]-[2/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073 Epoch:[1]-[3/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073 Epoch:[1]-[4/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.080 Epoch:[1]-[5/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.029, g_recon_loss:0.127 Epoch:[1]-[6/52] --&amp;gt; d_loss_real: 0.000, d_loss_fake: 0.004, g_loss:1.339, g_recon_loss:1.790 Epoch:[1]-[7/52] --&amp;gt; d_loss_real: 1.568, d_loss_fake: 8.227, g_loss:0.794, g_recon_loss:0.825 Epoch:[1]-[8/52] --&amp;gt; d_loss_real: 0.275, d_loss_fake: 2.447, g_loss:1.562, g_recon_loss:0.372 Epoch:[1]-[9/52] --&amp;gt; d_loss_real: 1.099, d_loss_fake: 1.998, g_loss:1.119, g_recon_loss:0.339 Epoch:[1]-[10/52] --&amp;gt; d_loss_real: 1.060, d_loss_fake: 1.677, g_loss:1.042, g_recon_loss:0.434 Epoch:[1]-[11/52] --&amp;gt; d_loss_real: 1.158, d_loss_fake: 1.530, g_loss:1.140, g_recon_loss:0.437 Epoch:[1]-[12/52] --&amp;gt; d_loss_real: 1.099, d_loss_fake: 1.001, g_loss:1.057, g_recon_loss:0.348 Epoch:[1]-[13/52] --&amp;gt; d_loss_real: 1.024, d_loss_fake: 1.001, g_loss:1.008, g_recon_loss:0.305 Epoch:[1]-[14/52] --&amp;gt; d_loss_real: 1.029, d_loss_fake: 0.949, g_loss:1.013, g_recon_loss:0.362 Epoch:[1]-[15/52] --&amp;gt; d_loss_real: 0.975, d_loss_fake: 1.022, g_loss:0.967, g_recon_loss:0.292 Epoch:[1]-[16/52] --&amp;gt; d_loss_real: 1.007, d_loss_fake: 0.976, g_loss:0.887, g_recon_loss:0.373 Epoch:[1]-[17/52] --&amp;gt; d_loss_real: 1.004, d_loss_fake: 0.983, g_loss:1.300, g_recon_loss:1.357 Epoch:[1]-[18/52] --&amp;gt; d_loss_real: 0.853, d_loss_fake: 1.114, g_loss:1.058, g_recon_loss:0.367 Epoch:[1]-[19/52] --&amp;gt; d_loss_real: 0.947, d_loss_fake: 0.921, g_loss:1.021, g_recon_loss:0.264 Epoch:[1]-[20/52] --&amp;gt; d_loss_real: 0.949, d_loss_fake: 0.949, g_loss:0.969, g_recon_loss:0.214 Epoch:[1]-[21/52] --&amp;gt; d_loss_real: 0.934, d_loss_fake: 0.953, g_loss:0.944, g_recon_loss:0.207 Epoch:[1]-[22/52] --&amp;gt; d_loss_real: 0.928, d_loss_fake: 0.922, g_loss:0.934, g_recon_loss:0.180 Epoch:[1]-[23/52] --&amp;gt; d_loss_real: 0.934, d_loss_fake: 0.894, g_loss:0.935, g_recon_loss:0.226 Epoch:[1]-[24/52] --&amp;gt; d_loss_real: 0.925, d_loss_fake: 0.892, g_loss:0.940, g_recon_loss:0.245 Epoch:[1]-[25/52] --&amp;gt; d_loss_real: 0.906, d_loss_fake: 0.907, g_loss:0.964, g_recon_loss:0.223 Epoch:[1]-[26/52] --&amp;gt; d_loss_real: 0.907, d_loss_fake: 0.908, g_loss:0.935, g_recon_loss:0.171 Epoch:[1]-[27/52] --&amp;gt; d_loss_real: 0.898, d_loss_fake: 0.880, g_loss:0.950, g_recon_loss:0.185 Epoch:[1]-[28/52] --&amp;gt; d_loss_real: 0.871, d_loss_fake: 0.872, g_loss:0.951, g_recon_loss:0.172 Epoch:[1]-[29/52] --&amp;gt; d_loss_real: 0.887, d_loss_fake: 0.929, g_loss:0.964, g_recon_loss:0.205 Epoch:[1]-[30/52] --&amp;gt; d_loss_real: 0.938, d_loss_fake: 0.907, g_loss:0.946, g_recon_loss:0.193 Epoch:[1]-[31/52] --&amp;gt; d_loss_real: 0.940, d_loss_fake: 0.887, g_loss:1.039, g_recon_loss:0.439 Epoch:[1]-[32/52] --&amp;gt; d_loss_real: 0.984, d_loss_fake: 0.947, g_loss:1.002, g_recon_loss:0.168 Epoch:[1]-[33/52] --&amp;gt; d_loss_real: 0.984, d_loss_fake: 0.883, g_loss:0.949, g_recon_loss:0.197 Epoch:[1]-[34/52] --&amp;gt; d_loss_real: 0.899, d_loss_fake: 0.906, g_loss:0.908, g_recon_loss:0.131 Epoch:[1]-[35/52] --&amp;gt; d_loss_real: 0.862, d_loss_fake: 0.887, g_loss:0.911, g_recon_loss:0.136 Epoch:[1]-[36/52] --&amp;gt; d_loss_real: 0.864, d_loss_fake: 0.882, g_loss:0.921, g_recon_loss:0.125 Epoch:[1]-[37/52] --&amp;gt; d_loss_real: 0.873, d_loss_fake: 0.873, g_loss:0.872, g_recon_loss:0.160 Epoch:[1]-[38/52] --&amp;gt; d_loss_real: 0.881, d_loss_fake: 0.878, g_loss:0.990, g_recon_loss:0.303 Epoch:[1]-[39/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 1.022, g_loss:0.993, g_recon_loss:0.335 Epoch:[1]-[40/52] --&amp;gt; d_loss_real: 0.919, d_loss_fake: 0.852, g_loss:0.965, g_recon_loss:0.190 Epoch:[1]-[41/52] --&amp;gt; d_loss_real: 0.897, d_loss_fake: 0.827, g_loss:0.966, g_recon_loss:0.210 Epoch:[1]-[42/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.837, g_loss:0.939, g_recon_loss:0.226 Epoch:[1]-[43/52] --&amp;gt; d_loss_real: 0.880, d_loss_fake: 0.793, g_loss:1.035, g_recon_loss:0.405 Epoch:[1]-[44/52] --&amp;gt; d_loss_real: 0.866, d_loss_fake: 0.927, g_loss:0.969, g_recon_loss:0.387 Epoch:[1]-[45/52] --&amp;gt; d_loss_real: 0.709, d_loss_fake: 0.952, g_loss:0.959, g_recon_loss:0.199 Epoch:[1]-[46/52] --&amp;gt; d_loss_real: 0.776, d_loss_fake: 1.326, g_loss:0.999, g_recon_loss:0.159 Epoch:[1]-[47/52] --&amp;gt; d_loss_real: 0.945, d_loss_fake: 0.751, g_loss:0.923, g_recon_loss:0.218 Epoch:[1]-[48/52] --&amp;gt; d_loss_real: 0.871, d_loss_fake: 1.031, g_loss:0.897, g_recon_loss:0.191 Epoch:[1]-[49/52] --&amp;gt; d_loss_real: 0.933, d_loss_fake: 0.926, g_loss:0.928, g_recon_loss:0.198 Epoch:[1]-[50/52] --&amp;gt; d_loss_real: 0.938, d_loss_fake: 0.875, g_loss:0.951, g_recon_loss:0.208 Epoch:[1]-[51/52] --&amp;gt; d_loss_real: 0.900, d_loss_fake: 0.853, g_loss:0.919, g_recon_loss:0.160 Epoch (2/5)------------------------------------------------- Epoch:[2]-[0/52] --&amp;gt; d_loss_real: 0.924, d_loss_fake: 0.857, g_loss:0.924, g_recon_loss:0.155 Epoch:[2]-[1/52] --&amp;gt; d_loss_real: 0.920, d_loss_fake: 0.840, g_loss:0.927, g_recon_loss:0.180 Epoch:[2]-[2/52] --&amp;gt; d_loss_real: 0.910, d_loss_fake: 0.840, g_loss:0.915, g_recon_loss:0.157 Epoch:[2]-[3/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.879, g_loss:0.885, g_recon_loss:0.147 Epoch:[2]-[4/52] --&amp;gt; d_loss_real: 0.855, d_loss_fake: 0.847, g_loss:0.906, g_recon_loss:0.202 Epoch:[2]-[5/52] --&amp;gt; d_loss_real: 0.842, d_loss_fake: 0.860, g_loss:0.884, g_recon_loss:0.131 Epoch:[2]-[6/52] --&amp;gt; d_loss_real: 0.827, d_loss_fake: 0.892, g_loss:0.901, g_recon_loss:0.169 Epoch:[2]-[7/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.852, g_loss:0.913, g_recon_loss:0.238 Epoch:[2]-[8/52] --&amp;gt; d_loss_real: 0.887, d_loss_fake: 0.839, g_loss:0.925, g_recon_loss:0.258 Epoch:[2]-[9/52] --&amp;gt; d_loss_real: 0.941, d_loss_fake: 0.850, g_loss:0.920, g_recon_loss:0.170 Epoch:[2]-[10/52] --&amp;gt; d_loss_real: 0.893, d_loss_fake: 0.905, g_loss:0.853, g_recon_loss:0.145 Epoch:[2]-[11/52] --&amp;gt; d_loss_real: 0.832, d_loss_fake: 0.898, g_loss:0.900, g_recon_loss:0.292 Epoch:[2]-[12/52] --&amp;gt; d_loss_real: 0.828, d_loss_fake: 0.923, g_loss:0.862, g_recon_loss:0.147 Epoch:[2]-[13/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.868, g_loss:0.871, g_recon_loss:0.157 Epoch:[2]-[14/52] --&amp;gt; d_loss_real: 0.864, d_loss_fake: 0.864, g_loss:0.877, g_recon_loss:0.129 Epoch:[2]-[15/52] --&amp;gt; d_loss_real: 0.860, d_loss_fake: 0.875, g_loss:0.885, g_recon_loss:0.112 Epoch:[2]-[16/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.846, g_loss:0.882, g_recon_loss:0.164 Epoch:[2]-[17/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.833, g_loss:0.906, g_recon_loss:0.161 Epoch:[2]-[18/52] --&amp;gt; d_loss_real: 0.915, d_loss_fake: 0.832, g_loss:0.904, g_recon_loss:0.195 Epoch:[2]-[19/52] --&amp;gt; d_loss_real: 0.881, d_loss_fake: 0.915, g_loss:0.884, g_recon_loss:0.110 Epoch:[2]-[20/52] --&amp;gt; d_loss_real: 0.855, d_loss_fake: 0.878, g_loss:0.862, g_recon_loss:0.102 Epoch:[2]-[21/52] --&amp;gt; d_loss_real: 0.848, d_loss_fake: 0.856, g_loss:0.874, g_recon_loss:0.130 Epoch:[2]-[22/52] --&amp;gt; d_loss_real: 0.850, d_loss_fake: 0.860, g_loss:0.860, g_recon_loss:0.106 Epoch:[2]-[23/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 0.843, g_loss:0.872, g_recon_loss:0.139 Epoch:[2]-[24/52] --&amp;gt; d_loss_real: 0.862, d_loss_fake: 0.908, g_loss:0.881, g_recon_loss:0.156 Epoch:[2]-[25/52] --&amp;gt; d_loss_real: 0.860, d_loss_fake: 0.876, g_loss:0.882, g_recon_loss:0.100 Epoch:[2]-[26/52] --&amp;gt; d_loss_real: 0.865, d_loss_fake: 0.852, g_loss:0.879, g_recon_loss:0.112 Epoch:[2]-[27/52] --&amp;gt; d_loss_real: 0.868, d_loss_fake: 0.840, g_loss:0.871, g_recon_loss:0.114 Epoch:[2]-[28/52] --&amp;gt; d_loss_real: 0.853, d_loss_fake: 0.857, g_loss:0.874, g_recon_loss:0.114 Epoch:[2]-[29/52] --&amp;gt; d_loss_real: 0.859, d_loss_fake: 0.839, g_loss:0.867, g_recon_loss:0.112 Epoch:[2]-[30/52] --&amp;gt; d_loss_real: 0.858, d_loss_fake: 0.818, g_loss:0.843, g_recon_loss:0.200 Epoch:[2]-[31/52] --&amp;gt; d_loss_real: 0.837, d_loss_fake: 0.901, g_loss:0.856, g_recon_loss:0.086 Epoch:[2]-[32/52] --&amp;gt; d_loss_real: 0.852, d_loss_fake: 0.867, g_loss:0.855, g_recon_loss:0.100 Epoch:[2]-[33/52] --&amp;gt; d_loss_real: 0.836, d_loss_fake: 0.878, g_loss:0.856, g_recon_loss:0.104 Epoch:[2]-[34/52] --&amp;gt; d_loss_real: 0.841, d_loss_fake: 0.859, g_loss:0.853, g_recon_loss:0.081 Epoch:[2]-[35/52] --&amp;gt; d_loss_real: 0.844, d_loss_fake: 0.835, g_loss:0.853, g_recon_loss:0.103 Epoch:[2]-[36/52] --&amp;gt; d_loss_real: 0.842, d_loss_fake: 0.868, g_loss:0.861, g_recon_loss:0.097 Epoch:[2]-[37/52] --&amp;gt; d_loss_real: 0.848, d_loss_fake: 0.827, g_loss:0.852, g_recon_loss:0.104 Epoch:[2]-[38/52] --&amp;gt; d_loss_real: 0.850, d_loss_fake: 0.833, g_loss:0.902, g_recon_loss:0.216 Epoch:[2]-[39/52] --&amp;gt; d_loss_real: 0.918, d_loss_fake: 0.835, g_loss:0.892, g_recon_loss:0.144 Epoch:[2]-[40/52] --&amp;gt; d_loss_real: 0.877, d_loss_fake: 0.835, g_loss:0.832, g_recon_loss:0.140 Epoch:[2]-[41/52] --&amp;gt; d_loss_real: 0.804, d_loss_fake: 0.893, g_loss:0.826, g_recon_loss:0.074 Epoch:[2]-[42/52] --&amp;gt; d_loss_real: 0.809, d_loss_fake: 0.866, g_loss:0.822, g_recon_loss:0.088 Epoch:[2]-[43/52] --&amp;gt; d_loss_real: 0.807, d_loss_fake: 0.846, g_loss:0.821, g_recon_loss:0.095 Epoch:[2]-[44/52] --&amp;gt; d_loss_real: 0.812, d_loss_fake: 0.850, g_loss:0.841, g_recon_loss:0.074 Epoch:[2]-[45/52] --&amp;gt; d_loss_real: 0.824, d_loss_fake: 0.822, g_loss:0.839, g_recon_loss:0.292 Epoch:[2]-[46/52] --&amp;gt; d_loss_real: 0.841, d_loss_fake: 0.902, g_loss:0.872, g_recon_loss:0.107 Epoch:[2]-[47/52] --&amp;gt; d_loss_real: 0.839, d_loss_fake: 0.857, g_loss:0.859, g_recon_loss:0.088 Epoch:[2]-[48/52] --&amp;gt; d_loss_real: 0.833, d_loss_fake: 0.827, g_loss:0.841, g_recon_loss:0.091 Epoch:[2]-[49/52] --&amp;gt; d_loss_real: 0.821, d_loss_fake: 0.818, g_loss:0.834, g_recon_loss:0.089 Epoch:[2]-[50/52] --&amp;gt; d_loss_real: 0.819, d_loss_fake: 0.829, g_loss:0.832, g_recon_loss:0.102 Epoch:[2]-[51/52] --&amp;gt; d_loss_real: 0.835, d_loss_fake: 0.840, g_loss:0.808, g_recon_loss:0.092 Epoch (3/5)------------------------------------------------- Epoch:[3]-[0/52] --&amp;gt; d_loss_real: 0.804, d_loss_fake: 0.869, g_loss:0.826, g_recon_loss:0.089 Epoch:[3]-[1/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.856, g_loss:0.849, g_recon_loss:0.087 Epoch:[3]-[2/52] --&amp;gt; d_loss_real: 0.835, d_loss_fake: 0.820, g_loss:0.833, g_recon_loss:0.113 Epoch:[3]-[3/52] --&amp;gt; d_loss_real: 0.831, d_loss_fake: 0.856, g_loss:0.872, g_recon_loss:0.086 Epoch:[3]-[4/52] --&amp;gt; d_loss_real: 0.847, d_loss_fake: 0.816, g_loss:0.829, g_recon_loss:0.129 Epoch:[3]-[5/52] --&amp;gt; d_loss_real: 0.817, d_loss_fake: 0.863, g_loss:0.826, g_recon_loss:0.088 Epoch:[3]-[6/52] --&amp;gt; d_loss_real: 0.809, d_loss_fake: 0.792, g_loss:0.796, g_recon_loss:0.127 Epoch:[3]-[7/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.891, g_loss:0.850, g_recon_loss:0.087 Epoch:[3]-[8/52] --&amp;gt; d_loss_real: 0.810, d_loss_fake: 0.841, g_loss:0.792, g_recon_loss:0.079 Epoch:[3]-[9/52] --&amp;gt; d_loss_real: 0.788, d_loss_fake: 0.872, g_loss:0.814, g_recon_loss:0.092 Epoch:[3]-[10/52] --&amp;gt; d_loss_real: 0.798, d_loss_fake: 0.854, g_loss:0.830, g_recon_loss:0.090 Epoch:[3]-[11/52] --&amp;gt; d_loss_real: 0.822, d_loss_fake: 0.840, g_loss:0.817, g_recon_loss:0.122 Epoch:[3]-[12/52] --&amp;gt; d_loss_real: 0.796, d_loss_fake: 0.838, g_loss:0.814, g_recon_loss:0.096 Epoch:[3]-[13/52] --&amp;gt; d_loss_real: 0.787, d_loss_fake: 0.861, g_loss:0.836, g_recon_loss:0.075 Epoch:[3]-[14/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.804, g_loss:0.828, g_recon_loss:0.207 Epoch:[3]-[15/52] --&amp;gt; d_loss_real: 0.880, d_loss_fake: 0.782, g_loss:0.939, g_recon_loss:0.113 Epoch:[3]-[16/52] --&amp;gt; d_loss_real: 0.885, d_loss_fake: 0.762, g_loss:0.888, g_recon_loss:0.113 Epoch:[3]-[17/52] --&amp;gt; d_loss_real: 0.833, d_loss_fake: 0.776, g_loss:0.828, g_recon_loss:0.093 Epoch:[3]-[18/52] --&amp;gt; d_loss_real: 0.791, d_loss_fake: 0.902, g_loss:0.707, g_recon_loss:0.112 Epoch:[3]-[19/52] --&amp;gt; d_loss_real: 0.714, d_loss_fake: 0.940, g_loss:0.769, g_recon_loss:0.090 Epoch:[3]-[20/52] --&amp;gt; d_loss_real: 0.755, d_loss_fake: 0.927, g_loss:0.844, g_recon_loss:0.091 Epoch:[3]-[21/52] --&amp;gt; d_loss_real: 0.823, d_loss_fake: 0.844, g_loss:0.801, g_recon_loss:0.086 Epoch:[3]-[22/52] --&amp;gt; d_loss_real: 0.793, d_loss_fake: 0.828, g_loss:0.780, g_recon_loss:0.095 Epoch:[3]-[23/52] --&amp;gt; d_loss_real: 0.784, d_loss_fake: 0.850, g_loss:0.811, g_recon_loss:0.082 Epoch:[3]-[24/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.819, g_loss:0.780, g_recon_loss:0.079 Epoch:[3]-[25/52] --&amp;gt; d_loss_real: 0.782, d_loss_fake: 0.865, g_loss:0.822, g_recon_loss:0.080 Epoch:[3]-[26/52] --&amp;gt; d_loss_real: 0.811, d_loss_fake: 0.803, g_loss:0.791, g_recon_loss:0.092 Epoch:[3]-[27/52] --&amp;gt; d_loss_real: 0.798, d_loss_fake: 0.849, g_loss:0.851, g_recon_loss:0.080 Epoch:[3]-[28/52] --&amp;gt; d_loss_real: 0.818, d_loss_fake: 0.781, g_loss:0.804, g_recon_loss:0.105 Epoch:[3]-[29/52] --&amp;gt; d_loss_real: 0.794, d_loss_fake: 0.822, g_loss:0.814, g_recon_loss:0.072 Epoch:[3]-[30/52] --&amp;gt; d_loss_real: 0.799, d_loss_fake: 0.762, g_loss:0.765, g_recon_loss:0.144 Epoch:[3]-[31/52] --&amp;gt; d_loss_real: 0.788, d_loss_fake: 0.838, g_loss:0.872, g_recon_loss:0.094 Epoch:[3]-[32/52] --&amp;gt; d_loss_real: 0.856, d_loss_fake: 0.792, g_loss:0.830, g_recon_loss:0.087 Epoch:[3]-[33/52] --&amp;gt; d_loss_real: 0.807, d_loss_fake: 0.795, g_loss:0.776, g_recon_loss:0.101 Epoch:[3]-[34/52] --&amp;gt; d_loss_real: 0.749, d_loss_fake: 0.859, g_loss:0.779, g_recon_loss:0.084 Epoch:[3]-[35/52] --&amp;gt; d_loss_real: 0.771, d_loss_fake: 0.891, g_loss:0.790, g_recon_loss:0.138 Epoch:[3]-[36/52] --&amp;gt; d_loss_real: 0.766, d_loss_fake: 0.852, g_loss:0.811, g_recon_loss:0.078 Epoch:[3]-[37/52] --&amp;gt; d_loss_real: 0.785, d_loss_fake: 0.809, g_loss:0.781, g_recon_loss:0.081 Epoch:[3]-[38/52] --&amp;gt; d_loss_real: 0.760, d_loss_fake: 0.882, g_loss:0.816, g_recon_loss:0.078 Epoch:[3]-[39/52] --&amp;gt; d_loss_real: 0.795, d_loss_fake: 0.781, g_loss:0.786, g_recon_loss:0.158 Epoch:[3]-[40/52] --&amp;gt; d_loss_real: 0.890, d_loss_fake: 0.672, g_loss:0.965, g_recon_loss:0.113 Epoch:[3]-[41/52] --&amp;gt; d_loss_real: 0.954, d_loss_fake: 0.734, g_loss:0.837, g_recon_loss:0.103 Epoch:[3]-[42/52] --&amp;gt; d_loss_real: 0.834, d_loss_fake: 0.999, g_loss:0.643, g_recon_loss:0.145 Epoch:[3]-[43/52] --&amp;gt; d_loss_real: 0.625, d_loss_fake: 1.018, g_loss:0.752, g_recon_loss:0.110 Epoch:[3]-[44/52] --&amp;gt; d_loss_real: 0.716, d_loss_fake: 0.868, g_loss:0.782, g_recon_loss:0.093 Epoch:[3]-[45/52] --&amp;gt; d_loss_real: 0.741, d_loss_fake: 0.839, g_loss:0.795, g_recon_loss:0.089 Epoch:[3]-[46/52] --&amp;gt; d_loss_real: 0.740, d_loss_fake: 0.844, g_loss:0.778, g_recon_loss:0.088 Epoch:[3]-[47/52] --&amp;gt; d_loss_real: 0.729, d_loss_fake: 1.006, g_loss:0.762, g_recon_loss:0.090 Epoch:[3]-[48/52] --&amp;gt; d_loss_real: 0.755, d_loss_fake: 0.832, g_loss:0.789, g_recon_loss:0.092 Epoch:[3]-[49/52] --&amp;gt; d_loss_real: 0.777, d_loss_fake: 0.767, g_loss:0.771, g_recon_loss:0.092 Epoch:[3]-[50/52] --&amp;gt; d_loss_real: 0.800, d_loss_fake: 0.789, g_loss:0.863, g_recon_loss:0.097 Epoch:[3]-[51/52] --&amp;gt; d_loss_real: 0.892, d_loss_fake: 0.745, g_loss:0.750, g_recon_loss:0.120 Epoch (4/5)------------------------------------------------- Epoch:[4]-[0/52] --&amp;gt; d_loss_real: 0.739, d_loss_fake: 0.839, g_loss:0.773, g_recon_loss:0.093 Epoch:[4]-[1/52] --&amp;gt; d_loss_real: 0.761, d_loss_fake: 0.824, g_loss:0.790, g_recon_loss:0.084 Epoch:[4]-[2/52] --&amp;gt; d_loss_real: 0.766, d_loss_fake: 0.800, g_loss:0.793, g_recon_loss:0.083 Epoch:[4]-[3/52] --&amp;gt; d_loss_real: 0.774, d_loss_fake: 0.665, g_loss:1.410, g_recon_loss:0.104 Epoch:[4]-[4/52] --&amp;gt; d_loss_real: 2.101, d_loss_fake: 0.606, g_loss:1.045, g_recon_loss:0.180 Epoch:[4]-[5/52] --&amp;gt; d_loss_real: 0.825, d_loss_fake: 0.564, g_loss:0.892, g_recon_loss:0.160 Epoch:[4]-[6/52] --&amp;gt; d_loss_real: 0.592, d_loss_fake: 1.084, g_loss:0.857, g_recon_loss:0.229 Epoch:[4]-[7/52] --&amp;gt; d_loss_real: 0.737, d_loss_fake: 0.689, g_loss:0.775, g_recon_loss:0.235 Epoch:[4]-[8/52] --&amp;gt; d_loss_real: 0.469, d_loss_fake: 1.200, g_loss:0.965, g_recon_loss:0.221 Epoch:[4]-[9/52] --&amp;gt; d_loss_real: 0.501, d_loss_fake: 1.126, g_loss:0.942, g_recon_loss:0.201 Epoch:[4]-[10/52] --&amp;gt; d_loss_real: 0.740, d_loss_fake: 0.869, g_loss:0.778, g_recon_loss:0.154 Epoch:[4]-[11/52] --&amp;gt; d_loss_real: 0.679, d_loss_fake: 0.880, g_loss:0.854, g_recon_loss:0.140 Epoch:[4]-[12/52] --&amp;gt; d_loss_real: 0.712, d_loss_fake: 0.789, g_loss:0.703, g_recon_loss:0.195 Epoch:[4]-[13/52] --&amp;gt; d_loss_real: 0.691, d_loss_fake: 1.092, g_loss:0.807, g_recon_loss:0.158 Epoch:[4]-[14/52] --&amp;gt; d_loss_real: 0.744, d_loss_fake: 0.731, g_loss:0.771, g_recon_loss:0.192 Epoch:[4]-[15/52] --&amp;gt; d_loss_real: 0.812, d_loss_fake: 0.771, g_loss:0.764, g_recon_loss:0.134 Epoch:[4]-[16/52] --&amp;gt; d_loss_real: 0.702, d_loss_fake: 0.897, g_loss:0.876, g_recon_loss:0.195 Epoch:[4]-[17/52] --&amp;gt; d_loss_real: 0.756, d_loss_fake: 0.773, g_loss:0.822, g_recon_loss:0.148 Epoch:[4]-[18/52] --&amp;gt; d_loss_real: 0.769, d_loss_fake: 0.762, g_loss:0.799, g_recon_loss:0.162 Epoch:[4]-[19/52] --&amp;gt; d_loss_real: 0.943, d_loss_fake: 0.843, g_loss:0.805, g_recon_loss:0.152 Epoch:[4]-[20/52] --&amp;gt; d_loss_real: 0.734, d_loss_fake: 0.885, g_loss:0.776, g_recon_loss:0.134 Epoch:[4]-[21/52] --&amp;gt; d_loss_real: 0.765, d_loss_fake: 0.863, g_loss:0.862, g_recon_loss:0.221 Epoch:[4]-[22/52] --&amp;gt; d_loss_real: 0.802, d_loss_fake: 0.745, g_loss:0.798, g_recon_loss:0.128 Epoch:[4]-[23/52] --&amp;gt; d_loss_real: 0.771, d_loss_fake: 0.704, g_loss:0.805, g_recon_loss:0.229 Epoch:[4]-[24/52] --&amp;gt; d_loss_real: 0.863, d_loss_fake: 0.673, g_loss:0.860, g_recon_loss:0.113 Epoch:[4]-[25/52] --&amp;gt; d_loss_real: 0.738, d_loss_fake: 0.793, g_loss:0.762, g_recon_loss:0.168 Epoch:[4]-[26/52] --&amp;gt; d_loss_real: 0.775, d_loss_fake: 0.800, g_loss:0.858, g_recon_loss:0.172 Epoch:[4]-[27/52] --&amp;gt; d_loss_real: 0.699, d_loss_fake: 0.835, g_loss:0.811, g_recon_loss:0.147 Epoch:[4]-[28/52] --&amp;gt; d_loss_real: 0.828, d_loss_fake: 0.718, g_loss:0.834, g_recon_loss:0.127 Epoch:[4]-[29/52] --&amp;gt; d_loss_real: 0.693, d_loss_fake: 0.818, g_loss:0.803, g_recon_loss:0.188 Epoch:[4]-[30/52] --&amp;gt; d_loss_real: 0.747, d_loss_fake: 0.648, g_loss:0.846, g_recon_loss:0.134 Epoch:[4]-[31/52] --&amp;gt; d_loss_real: 0.696, d_loss_fake: 0.839, g_loss:0.792, g_recon_loss:0.211 Epoch:[4]-[32/52] --&amp;gt; d_loss_real: 0.661, d_loss_fake: 0.805, g_loss:0.815, g_recon_loss:0.354 Epoch:[4]-[33/52] --&amp;gt; d_loss_real: 0.669, d_loss_fake: 0.975, g_loss:0.867, g_recon_loss:0.178 Epoch:[4]-[34/52] --&amp;gt; d_loss_real: 0.744, d_loss_fake: 0.803, g_loss:0.828, g_recon_loss:0.205 Epoch:[4]-[35/52] --&amp;gt; d_loss_real: 0.815, d_loss_fake: 0.627, g_loss:0.791, g_recon_loss:0.189 Epoch:[4]-[36/52] --&amp;gt; d_loss_real: 0.980, d_loss_fake: 0.673, g_loss:0.817, g_recon_loss:0.139 Epoch:[4]-[37/52] --&amp;gt; d_loss_real: 0.759, d_loss_fake: 0.788, g_loss:0.830, g_recon_loss:0.157 Epoch:[4]-[38/52] --&amp;gt; d_loss_real: 0.649, d_loss_fake: 0.710, g_loss:0.764, g_recon_loss:0.106 Epoch:[4]-[39/52] --&amp;gt; d_loss_real: 0.615, d_loss_fake: 0.859, g_loss:0.736, g_recon_loss:0.113 Epoch:[4]-[40/52] --&amp;gt; d_loss_real: 0.663, d_loss_fake: 0.939, g_loss:0.754, g_recon_loss:0.124 Epoch:[4]-[41/52] --&amp;gt; d_loss_real: 0.650, d_loss_fake: 1.127, g_loss:0.957, g_recon_loss:0.142 Epoch:[4]-[42/52] --&amp;gt; d_loss_real: 0.772, d_loss_fake: 0.753, g_loss:0.806, g_recon_loss:0.191 Epoch:[4]-[43/52] --&amp;gt; d_loss_real: 0.917, d_loss_fake: 0.609, g_loss:0.746, g_recon_loss:0.150 Epoch:[4]-[44/52] --&amp;gt; d_loss_real: 0.943, d_loss_fake: 0.756, g_loss:0.784, g_recon_loss:0.188 Epoch:[4]-[45/52] --&amp;gt; d_loss_real: 0.758, d_loss_fake: 0.768, g_loss:0.829, g_recon_loss:0.156 Epoch:[4]-[46/52] --&amp;gt; d_loss_real: 0.701, d_loss_fake: 0.685, g_loss:0.820, g_recon_loss:0.136 Epoch:[4]-[47/52] --&amp;gt; d_loss_real: 0.712, d_loss_fake: 0.610, g_loss:0.745, g_recon_loss:0.175 Epoch:[4]-[48/52] --&amp;gt; d_loss_real: 0.717, d_loss_fake: 0.911, g_loss:0.785, g_recon_loss:0.155 Epoch:[4]-[49/52] --&amp;gt; d_loss_real: 0.780, d_loss_fake: 0.824, g_loss:0.807, g_recon_loss:0.147 Epoch:[4]-[50/52] --&amp;gt; d_loss_real: 0.691, d_loss_fake: 0.809, g_loss:0.871, g_recon_loss:0.140 Epoch:[4]-[51/52] --&amp;gt; d_loss_real: 0.728, d_loss_fake: 0.734, g_loss:0.748, g_recon_loss:0.151 Choose a stopping criterion The training procedure is stopped when R successfully maps noisy images to clean images carrying the concept of the target class. When R can reconstruct its input with minimum error. In the following case, we pick the epoch 3. # This image was generated at the end of the models.py training procedure to help pick a ending epoch to load. from IPython.display import Image Image(filename='plot_g_recon_losses.png') # Load the epoch #3 saved weights. self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5') Test the reconstruction loss and Discriminator output The abnormal image has a larger reconstruction loss and smaller discriminator output value. def test_reconstruction(label, data_index = 11): specific_idx = np.where(y_train == label)[0] if data_index &amp;gt;= len(X_train): data_index = 0 datas = X_train[specific_idx].reshape(-1, 28, 28, 1)[data_index:data_index+5] model_predicts = self.adversarial_model.predict(datas) input_images = [datas[i].reshape((28, 28)) for i in range(5)] reconstructed_images = [model_predicts[0][i].reshape((28, 28)) for i in range(5)] fig= plt.figure(figsize=(50, 20)) columns = 5 rows = 2 for i in range(5): fig.add_subplot(rows, columns, i+1) plt.title('Input', fontsize = 20) plt.imshow(input_images[i], label='Input') fig.add_subplot(rows, columns, i+6) plt.title('Reconstruction', fontsize = 20) plt.imshow(reconstructed_images[i], label='Reconstructed') plt.show() # Compute the mean binary_crossentropy loss of reconstructed image. errors = list() for i in range(5): y_true = K.variable(reconstructed_images[i]) y_pred = K.variable(input_images[i]) errors.append(K.eval(binary_crossentropy(y_true, y_pred)).mean()) print('Average reconstruction loss:', np.array(errors).mean()) print('Average discriminator Output:', model_predicts[1].mean()) def draw_histogram(): data_list = [X_train[np.where(y_train == i+1)[0]].reshape(-1, 28, 28, 1) for i in range(9)] D_Outputs = [[self.adversarial_model.predict(n.reshape(-1,28,28,1))[1][0][0] for n in data_list[i]] for i in range(9)] plt.figure(figsize=(8,6)) plt.hist(D_Outputs[0], bins=100, alpha=0.7, fc='none', lw=1.5, histtype='step', label=&quot;Inlier&quot;) outlier_list = [item for sublist in D_Outputs[1:] for item in sublist] plt.hist(outlier_list, bins=100, alpha=0.7, fc='none', lw=1.5, histtype='step', label=&quot;Outlier&quot;) plt.xlabel(&quot;Discriminator Output&quot;, size=14) plt.ylabel(&quot;Count&quot;, size=14) plt.title(&quot;Output multiple Histograms&quot;) plt.legend(loc='upper right') plt.savefig('paper_original_result.png') Normal case The network was trained with label == 1. test_reconstruction(1) Average reconstruction loss: 0.13995047 Average discriminator Output: 0.46022063 Abnormal cases The network was not trained on those labels, so the Generator/R network find it hard to reconstruct the input images reflected in higher reconstruction loss values. Discriminator also outputs a lower value compared to normal ones. test_reconstruction(3) Average reconstruction loss: 1.0208124 Average discriminator Output: 0.6488284 test_reconstruction(5) Average reconstruction loss: 1.1040308 Average discriminator Output: 0.6433686 test_reconstruction(7) Average reconstruction loss: 0.51506275 Average discriminator Output: 0.54887545 draw_histogram() mobiis 추가 연구사항 D 모델을 조금더 복잡하게 구성 R 모델 학습을 조금 더디게 진행 노이즈 없이 학습 1. Enhance D network - Add 1 dense layer import sys import importlib sys.path.insert(1, 'test/enhance_D/') importlib.reload(models) from models import ALOCC_Model self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28) generator Model: &quot;R&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= z (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ g_encoder_h0_conv (Conv2D) (None, 14, 14, 32) 832 _________________________________________________________________ batch_normalization_70 (Batc (None, 14, 14, 32) 128 _________________________________________________________________ leaky_re_lu_82 (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ g_encoder_h1_conv (Conv2D) (None, 7, 7, 64) 51264 _________________________________________________________________ batch_normalization_71 (Batc (None, 7, 7, 64) 256 _________________________________________________________________ leaky_re_lu_83 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ g_encoder_h2_conv (Conv2D) (None, 4, 4, 128) 204928 _________________________________________________________________ batch_normalization_72 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ leaky_re_lu_84 (LeakyReLU) (None, 4, 4, 128) 0 _________________________________________________________________ conv2d_45 (Conv2D) (None, 4, 4, 16) 51216 _________________________________________________________________ up_sampling2d_34 (UpSampling (None, 8, 8, 16) 0 _________________________________________________________________ conv2d_46 (Conv2D) (None, 8, 8, 16) 6416 _________________________________________________________________ up_sampling2d_35 (UpSampling (None, 16, 16, 16) 0 _________________________________________________________________ conv2d_47 (Conv2D) (None, 14, 14, 32) 4640 _________________________________________________________________ up_sampling2d_36 (UpSampling (None, 28, 28, 32) 0 _________________________________________________________________ conv2d_48 (Conv2D) (None, 28, 28, 1) 801 ================================================================= Total params: 320,993 Trainable params: 320,545 Non-trainable params: 448 _________________________________________________________________ discriminator Model: &quot;D&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= d_input (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ d_h0_conv (Conv2D) (None, 14, 14, 16) 416 _________________________________________________________________ leaky_re_lu_78 (LeakyReLU) (None, 14, 14, 16) 0 _________________________________________________________________ d_h1_conv (Conv2D) (None, 7, 7, 32) 12832 _________________________________________________________________ batch_normalization_67 (Batc (None, 7, 7, 32) 128 _________________________________________________________________ leaky_re_lu_79 (LeakyReLU) (None, 7, 7, 32) 0 _________________________________________________________________ d_h2_conv (Conv2D) (None, 4, 4, 64) 51264 _________________________________________________________________ batch_normalization_68 (Batc (None, 4, 4, 64) 256 _________________________________________________________________ leaky_re_lu_80 (LeakyReLU) (None, 4, 4, 64) 0 _________________________________________________________________ d_h3_conv (Conv2D) (None, 2, 2, 128) 204928 _________________________________________________________________ batch_normalization_69 (Batc (None, 2, 2, 128) 512 _________________________________________________________________ leaky_re_lu_81 (LeakyReLU) (None, 2, 2, 128) 0 _________________________________________________________________ flatten_12 (Flatten) (None, 512) 0 _________________________________________________________________ dense_4 (Dense) (None, 32) 16416 _________________________________________________________________ d_h3_lin (Dense) (None, 1) 33 ================================================================= Total params: 573,122 Trainable params: 286,337 Non-trainable params: 286,785 _________________________________________________________________ adversarial_model Model: &quot;model_12&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_12 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ R (Model) (None, 28, 28, 1) 320993 _________________________________________________________________ D (Model) (None, 1) 286785 ================================================================= Total params: 607,778 Trainable params: 320,545 Non-trainable params: 287,233 _________________________________________________________________ /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' self.train(epochs=5, batch_size=128, sample_interval=500) Epoch (0/5)------------------------------------------------- /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch (1/5)------------------------------------------------- Epoch (2/5)------------------------------------------------- Epoch (3/5)------------------------------------------------- Epoch (4/5)------------------------------------------------- # Load the epoch #3 saved weights. self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5') draw_histogram() 2. Slower R network learning - 기존 1 batch당 두번 학습에서 한번으로 변경 import sys import importlib sys.path.insert(1, 'test/slow_R/') importlib.reload(models) from models import ALOCC_Model self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28) generator Model: &quot;R&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= z (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ g_encoder_h0_conv (Conv2D) (None, 14, 14, 32) 832 _________________________________________________________________ batch_normalization_58 (Batc (None, 14, 14, 32) 128 _________________________________________________________________ leaky_re_lu_68 (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ g_encoder_h1_conv (Conv2D) (None, 7, 7, 64) 51264 _________________________________________________________________ batch_normalization_59 (Batc (None, 7, 7, 64) 256 _________________________________________________________________ leaky_re_lu_69 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ g_encoder_h2_conv (Conv2D) (None, 4, 4, 128) 204928 _________________________________________________________________ batch_normalization_60 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ leaky_re_lu_70 (LeakyReLU) (None, 4, 4, 128) 0 _________________________________________________________________ conv2d_37 (Conv2D) (None, 4, 4, 16) 51216 _________________________________________________________________ up_sampling2d_28 (UpSampling (None, 8, 8, 16) 0 _________________________________________________________________ conv2d_38 (Conv2D) (None, 8, 8, 16) 6416 _________________________________________________________________ up_sampling2d_29 (UpSampling (None, 16, 16, 16) 0 _________________________________________________________________ conv2d_39 (Conv2D) (None, 14, 14, 32) 4640 _________________________________________________________________ up_sampling2d_30 (UpSampling (None, 28, 28, 32) 0 _________________________________________________________________ conv2d_40 (Conv2D) (None, 28, 28, 1) 801 ================================================================= Total params: 320,993 Trainable params: 320,545 Non-trainable params: 448 _________________________________________________________________ discriminator Model: &quot;D&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= d_input (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ d_h0_conv (Conv2D) (None, 14, 14, 16) 416 _________________________________________________________________ leaky_re_lu_64 (LeakyReLU) (None, 14, 14, 16) 0 _________________________________________________________________ d_h1_conv (Conv2D) (None, 7, 7, 32) 12832 _________________________________________________________________ batch_normalization_55 (Batc (None, 7, 7, 32) 128 _________________________________________________________________ leaky_re_lu_65 (LeakyReLU) (None, 7, 7, 32) 0 _________________________________________________________________ d_h2_conv (Conv2D) (None, 4, 4, 64) 51264 _________________________________________________________________ batch_normalization_56 (Batc (None, 4, 4, 64) 256 _________________________________________________________________ leaky_re_lu_66 (LeakyReLU) (None, 4, 4, 64) 0 _________________________________________________________________ d_h3_conv (Conv2D) (None, 2, 2, 128) 204928 _________________________________________________________________ batch_normalization_57 (Batc (None, 2, 2, 128) 512 _________________________________________________________________ leaky_re_lu_67 (LeakyReLU) (None, 2, 2, 128) 0 _________________________________________________________________ flatten_10 (Flatten) (None, 512) 0 _________________________________________________________________ d_h3_lin (Dense) (None, 1) 513 ================================================================= Total params: 541,250 Trainable params: 270,401 Non-trainable params: 270,849 _________________________________________________________________ adversarial_model Model: &quot;model_10&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_10 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ R (Model) (None, 28, 28, 1) 320993 _________________________________________________________________ D (Model) (None, 1) 270849 ================================================================= Total params: 591,842 Trainable params: 320,545 Non-trainable params: 271,297 _________________________________________________________________ self.train(epochs=5, batch_size=128, sample_interval=500) Epoch (0/5)------------------------------------------------- /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch (1/5)------------------------------------------------- Epoch (2/5)------------------------------------------------- Epoch (3/5)------------------------------------------------- Epoch (4/5)------------------------------------------------- # Load the epoch #4 saved weights. self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5') draw_histogram() 3. Learning without noise import sys import importlib sys.path.insert(1, 'test/without_noise/') importlib.reload(models) from models import ALOCC_Model self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28) generator Model: &quot;R&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= z (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ g_encoder_h0_conv (Conv2D) (None, 14, 14, 32) 832 _________________________________________________________________ batch_normalization_34 (Batc (None, 14, 14, 32) 128 _________________________________________________________________ leaky_re_lu_40 (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ g_encoder_h1_conv (Conv2D) (None, 7, 7, 64) 51264 _________________________________________________________________ batch_normalization_35 (Batc (None, 7, 7, 64) 256 _________________________________________________________________ leaky_re_lu_41 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ g_encoder_h2_conv (Conv2D) (None, 4, 4, 128) 204928 _________________________________________________________________ batch_normalization_36 (Batc (None, 4, 4, 128) 512 _________________________________________________________________ leaky_re_lu_42 (LeakyReLU) (None, 4, 4, 128) 0 _________________________________________________________________ conv2d_21 (Conv2D) (None, 4, 4, 16) 51216 _________________________________________________________________ up_sampling2d_16 (UpSampling (None, 8, 8, 16) 0 _________________________________________________________________ conv2d_22 (Conv2D) (None, 8, 8, 16) 6416 _________________________________________________________________ up_sampling2d_17 (UpSampling (None, 16, 16, 16) 0 _________________________________________________________________ conv2d_23 (Conv2D) (None, 14, 14, 32) 4640 _________________________________________________________________ up_sampling2d_18 (UpSampling (None, 28, 28, 32) 0 _________________________________________________________________ conv2d_24 (Conv2D) (None, 28, 28, 1) 801 ================================================================= Total params: 320,993 Trainable params: 320,545 Non-trainable params: 448 _________________________________________________________________ discriminator Model: &quot;D&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= d_input (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ d_h0_conv (Conv2D) (None, 14, 14, 16) 416 _________________________________________________________________ leaky_re_lu_36 (LeakyReLU) (None, 14, 14, 16) 0 _________________________________________________________________ d_h1_conv (Conv2D) (None, 7, 7, 32) 12832 _________________________________________________________________ batch_normalization_31 (Batc (None, 7, 7, 32) 128 _________________________________________________________________ leaky_re_lu_37 (LeakyReLU) (None, 7, 7, 32) 0 _________________________________________________________________ d_h2_conv (Conv2D) (None, 4, 4, 64) 51264 _________________________________________________________________ batch_normalization_32 (Batc (None, 4, 4, 64) 256 _________________________________________________________________ leaky_re_lu_38 (LeakyReLU) (None, 4, 4, 64) 0 _________________________________________________________________ d_h3_conv (Conv2D) (None, 2, 2, 128) 204928 _________________________________________________________________ batch_normalization_33 (Batc (None, 2, 2, 128) 512 _________________________________________________________________ leaky_re_lu_39 (LeakyReLU) (None, 2, 2, 128) 0 _________________________________________________________________ flatten_6 (Flatten) (None, 512) 0 _________________________________________________________________ d_h3_lin (Dense) (None, 1) 513 ================================================================= Total params: 541,250 Trainable params: 270,401 Non-trainable params: 270,849 _________________________________________________________________ adversarial_model Model: &quot;model_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_6 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ R (Model) (None, 28, 28, 1) 320993 _________________________________________________________________ D (Model) (None, 1) 270849 ================================================================= Total params: 591,842 Trainable params: 320,545 Non-trainable params: 271,297 _________________________________________________________________ self.train(epochs=5, batch_size=128, sample_interval=500) Epoch (0/5)------------------------------------------------- /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch (1/5)------------------------------------------------- Epoch (2/5)------------------------------------------------- Epoch (3/5)------------------------------------------------- Epoch (4/5)------------------------------------------------- # Load the epoch #4 saved weights. self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5') draw_histogram() reconstruction loss의 loss_weight 를 높혀서 트레이닝을 하면 D모델을 속이려는 성향보다 이미지를 깔끔하게 만드려는 성향이 강해지고 reconstruction loss의 loss_weight 를 낮혀서 트레이닝을 하면 이미지를 깔끔하게 바꾸려는 성향보다 어떻게 하든 D모델을 속이면 되는 쪽으로 학습이 된다.</summary></entry><entry><title type="html">Fullconvnet</title><link href="http://localhost:4000/2020/10/26/fullconvnet.html" rel="alternate" type="text/html" title="Fullconvnet" /><published>2020-10-26T00:00:00+09:00</published><updated>2020-10-26T00:00:00+09:00</updated><id>http://localhost:4000/2020/10/26/fullconvnet</id><content type="html" xml:base="http://localhost:4000/2020/10/26/fullconvnet.html">&lt;h2 id=&quot;settings&quot;&gt;Settings&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_ext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;main&quot;&gt;Main&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ignite.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# loss 조정해
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#'acc': Accuracy(),
&lt;/span&gt;        &lt;span class=&quot;s&quot;&gt;'loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cuda:1'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;load-data&quot;&gt;Load Data&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCSegmentation&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CenterCrop&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ToTensor_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CenterCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PILToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CenterCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getbands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndenumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCSegmentation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PILToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCSegmentation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'val'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PILToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'# train data:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'# val data  :'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# train data: 1464
# val data  : 1449
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;construct-model&quot;&gt;Construct Model&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FullConvNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    cfg = [(n_channel, ), (), ...]
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FullConvNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vgg19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_currch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_nextch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConvTranspose2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_currch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_nextch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_currch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_nextch&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_currch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l{}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FullConvNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;train&quot;&gt;Train&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ignite.engine&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_evaluator&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non_blocking&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# =======================================
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{}: {:.2f} '&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCH_COMPLETED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_training_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epoch {}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Train - '&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCH_COMPLETED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_validation_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Val   - '&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch 1
Train - loss: 1.17 
Val   - loss: 1.20 
Epoch 2
Train - loss: 0.97 
Val   - loss: 1.02 
Epoch 3
Train - loss: 0.87 
Val   - loss: 0.95 
Epoch 4
Train - loss: 0.79 
Val   - loss: 0.92 
Epoch 5
Train - loss: 0.73 
Val   - loss: 0.94 
Epoch 6
Train - loss: 0.68 
Val   - loss: 0.98 
Epoch 7
Train - loss: 0.65 
Val   - loss: 1.08 
Epoch 8
Train - loss: 0.60 
Val   - loss: 1.15 
Epoch 9
Train - loss: 0.58 
Val   - loss: 1.27 
Epoch 10
Train - loss: 0.56 
Val   - loss: 1.29 
Epoch 11
Train - loss: 0.53 
Val   - loss: 1.38 
Epoch 12
Train - loss: 0.49 
Val   - loss: 1.47 
Epoch 13
Train - loss: 0.46 
Val   - loss: 1.48 
Epoch 14
Train - loss: 0.47 
Val   - loss: 1.57 
Epoch 15
Train - loss: 0.48 
Val   - loss: 1.67 
Epoch 16
Train - loss: 0.49 
Val   - loss: 1.66 
Epoch 17
Train - loss: 0.47 
Val   - loss: 1.76 
Epoch 18
Train - loss: 0.37 
Val   - loss: 1.74 
Epoch 19
Train - loss: 0.36 
Val   - loss: 1.80 
Epoch 20
Train - loss: 0.34 
Val   - loss: 1.77 
Epoch 21
Train - loss: 0.32 
Val   - loss: 1.83 
Epoch 22
Train - loss: 0.38 
Val   - loss: 1.95 
Epoch 23
Train - loss: 0.31 
Val   - loss: 1.91 
Epoch 24
Train - loss: 0.28 
Val   - loss: 1.89 
Epoch 25
Train - loss: 0.28 
Val   - loss: 2.05 
Epoch 26
Train - loss: 0.26 
Val   - loss: 1.98 
Epoch 27
Train - loss: 0.20 
Val   - loss: 1.93 
Epoch 28
Train - loss: 0.21 
Val   - loss: 2.01 
Epoch 29
Train - loss: 0.22 
Val   - loss: 2.20 
Epoch 30
Train - loss: 0.19 
Val   - loss: 2.15 


Engine run is terminating due to exception: .
Engine run is terminating due to exception: .



---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

&amp;lt;ipython-input-9-8ac1a0a79e01&amp;gt; in &amp;lt;module&amp;gt;
      3 trainer = train_net(model, opt, loss_fn, val_metrics,
      4         train_loader, val_loader, device)
----&amp;gt; 5 trainer.run(train_loader, max_epochs=max_epochs)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed)
    656 
    657         self.state.dataloader = data
--&amp;gt; 658         return self._internal_run()
    659 
    660     @staticmethod


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    720             self._dataloader_iter = None
    721             self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e))
--&amp;gt; 722             self._handle_exception(e)
    723 
    724         self._dataloader_iter = None


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e)
    435             self._fire_event(Events.EXCEPTION_RAISED, e)
    436         else:
--&amp;gt; 437             raise e
    438 
    439     @property


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    708                     self.logger.info(elapsed_time_message)
    709                     break
--&amp;gt; 710                 self._fire_event(Events.EPOCH_COMPLETED)
    711                 self.logger.info(elapsed_time_message)
    712 


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _fire_event(self, event_name, *event_args, **event_kwargs)
    391                 kwargs.update(event_kwargs)
    392                 first, others = ((args[0],), args[1:]) if (args and args[0] == self) else ((), args)
--&amp;gt; 393                 func(*first, *(event_args + others), **kwargs)
    394 
    395     def fire_event(self, event_name: Any) -&amp;gt; None:


&amp;lt;ipython-input-8-05b4a011b4ff&amp;gt; in log_training_results(trainer)
     15     @trainer.on(Events.EPOCH_COMPLETED)
     16     def log_training_results(trainer):
---&amp;gt; 17         evaluator.run(train_loader)
     18         print('Epoch {}'.format(trainer.state.epoch))
     19         message = 'Train - '


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed)
    656 
    657         self.state.dataloader = data
--&amp;gt; 658         return self._internal_run()
    659 
    660     @staticmethod


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    720             self._dataloader_iter = None
    721             self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e))
--&amp;gt; 722             self._handle_exception(e)
    723 
    724         self._dataloader_iter = None


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e)
    435             self._fire_event(Events.EXCEPTION_RAISED, e)
    436         else:
--&amp;gt; 437             raise e
    438 
    439     @property


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    695                     self._setup_engine()
    696 
--&amp;gt; 697                 time_taken = self._run_once_on_dataset()
    698                 self.state.times[Events.EPOCH_COMPLETED.name] = time_taken
    699                 hours, mins, secs = _to_hours_mins_secs(time_taken)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _run_once_on_dataset(self)
    737                     if self.last_event_name != Events.DATALOADER_STOP_ITERATION:
    738                         self._fire_event(Events.GET_BATCH_STARTED)
--&amp;gt; 739                     self.state.batch = next(self._dataloader_iter)
    740                     self._fire_event(Events.GET_BATCH_COMPLETED)
    741                     iter_counter += 1


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    343 
    344     def __next__(self):
--&amp;gt; 345         data = self._next_data()
    346         self._num_yielded += 1
    347         if self._dataset_kind == _DatasetKind.Iterable and \


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    383     def _next_data(self):
    384         index = self._next_index()  # may raise StopIteration
--&amp;gt; 385         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    386         if self._pin_memory:
    387             data = _utils.pin_memory.pin_memory(data)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&amp;gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in &amp;lt;listcomp&amp;gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&amp;gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/voc.py in __getitem__(self, index)
    123 
    124         if self.transforms is not None:
--&amp;gt; 125             img, target = self.transforms(img, target)
    126 
    127         return img, target


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/vision.py in __call__(self, input, target)
     61             input = self.transform(input)
     62         if self.target_transform is not None:
---&amp;gt; 63             target = self.target_transform(target)
     64         return input, target
     65 


&amp;lt;ipython-input-4-e0a946e58f28&amp;gt; in __call__(self, pic)
     18         img = img.permute((2, 0, 1))
     19         for i, x in np.ndenumerate(img):
---&amp;gt; 20             if x == 255:
     21                 img[i] = 21
     22         return img.long().view(-1)


KeyboardInterrupt: 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;samples&quot;&gt;Samples&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToPILImage&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tpi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToPILImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fullconvnet_files/fullconvnet_15_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>mobiismlteam</name></author><summary type="html">Settings %load_ext autoreload %autoreload 2 Main import torch from torch import nn from torch.nn import functional as F from torchvision import models from ignite.metrics import Accuracy, Loss batch_size = 1 loss_fn = nn.CrossEntropyLoss() # loss 조정해 opt_ = torch.optim.Adam lr = 0.001 val_metrics = { #'acc': Accuracy(), 'loss': Loss(loss_fn) } device = 'cuda:1' max_epochs = 1000 Load Data import numpy as np from torchvision.datasets import VOCSegmentation from torchvision.transforms import ToTensor, CenterCrop class ToTensor_(ToTensor): def __call__(self, pic): w, h = pic.size w, h = w - (w % 32), h - (h % 32) return ToTensor()(CenterCrop((h, w))(pic)) class PILToTensor(ToTensor): def __call__(self, pic): w, h = pic.size w, h = w - (w % 32), h - (h % 32) pic = CenterCrop((h, w))(pic) img = torch.as_tensor(np.asarray(pic)) img = img.view(pic.size[1], pic.size[0], len(pic.getbands())) img = img.permute((2, 0, 1)) for i, x in np.ndenumerate(img): if x == 255: img[i] = 21 return img.long().view(-1) train_dataset = VOCSegmentation(root='data/', image_set='train', transform=ToTensor_(), target_transform=PILToTensor(), download=False) val_dataset = VOCSegmentation(root='data/', image_set='val', transform=ToTensor_(), target_transform=PILToTensor(), download=False) print('# train data:', len(train_dataset)) print('# val data :', len(val_dataset)) # train data: 1464 # val data : 1449 from torch.utils.data import DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) val_loader = DataLoader(val_dataset, batch_size=batch_size) Construct Model class FullConvNet(nn.Module): &quot;&quot;&quot; cfg = [(n_channel, ), (), ...] &quot;&quot;&quot; def __init__(self, n_class): super(FullConvNet, self).__init__() self.layers = [] for layer in models.vgg19(pretrained=True).features: self.layers.append(layer) for param in layer.parameters(): param.requires_grad = False n_currch = 512 for n_nextch in [512, 256, 128, 64, 32]: self.layers.append(nn.ReLU(True)) self.layers.append(nn.ConvTranspose2d(n_currch, n_nextch, kernel_size=3, stride=2, padding=1, output_padding=1)) n_currch = n_nextch self.layers.append(nn.Conv2d(n_currch, n_class, kernel_size=1)) for i, layer in enumerate(self.layers): self.add_module('l{}'.format(i), layer) def forward(self, x): for layer in self.layers: x = layer(x) return x.view(1, 22, -1) model = FullConvNet(22) Train from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator def train_net(net, opt, loss_fn, val_metrics, train_loader, val_loader, device): net.to(device) def prepare_batch(batch, device, non_blocking=False): x, y = batch return x.to(device), y.to(device) def output_transform(x, y, y_pred, loss): return (y_pred.max(1)[1], y) # ======================================= trainer = create_supervised_trainer(net, opt, loss_fn, device, prepare_batch=prepare_batch, output_transform=output_transform) evaluator = create_supervised_evaluator(net, val_metrics, device, prepare_batch=prepare_batch) s = '{}: {:.2f} ' @trainer.on(Events.EPOCH_COMPLETED) def log_training_results(trainer): evaluator.run(train_loader) print('Epoch {}'.format(trainer.state.epoch)) message = 'Train - ' for m in val_metrics.keys(): message += s.format(m, evaluator.state.metrics[m]) print(message) @trainer.on(Events.EPOCH_COMPLETED) def log_validation_results(trainer): evaluator.run(val_loader) message = 'Val - ' for m in val_metrics.keys(): message += s.format(m, evaluator.state.metrics[m]) print(message) return trainer opt = opt_(model.parameters(), lr) trainer = train_net(model, opt, loss_fn, val_metrics, train_loader, val_loader, device) trainer.run(train_loader, max_epochs=max_epochs) Epoch 1 Train - loss: 1.17 Val - loss: 1.20 Epoch 2 Train - loss: 0.97 Val - loss: 1.02 Epoch 3 Train - loss: 0.87 Val - loss: 0.95 Epoch 4 Train - loss: 0.79 Val - loss: 0.92 Epoch 5 Train - loss: 0.73 Val - loss: 0.94 Epoch 6 Train - loss: 0.68 Val - loss: 0.98 Epoch 7 Train - loss: 0.65 Val - loss: 1.08 Epoch 8 Train - loss: 0.60 Val - loss: 1.15 Epoch 9 Train - loss: 0.58 Val - loss: 1.27 Epoch 10 Train - loss: 0.56 Val - loss: 1.29 Epoch 11 Train - loss: 0.53 Val - loss: 1.38 Epoch 12 Train - loss: 0.49 Val - loss: 1.47 Epoch 13 Train - loss: 0.46 Val - loss: 1.48 Epoch 14 Train - loss: 0.47 Val - loss: 1.57 Epoch 15 Train - loss: 0.48 Val - loss: 1.67 Epoch 16 Train - loss: 0.49 Val - loss: 1.66 Epoch 17 Train - loss: 0.47 Val - loss: 1.76 Epoch 18 Train - loss: 0.37 Val - loss: 1.74 Epoch 19 Train - loss: 0.36 Val - loss: 1.80 Epoch 20 Train - loss: 0.34 Val - loss: 1.77 Epoch 21 Train - loss: 0.32 Val - loss: 1.83 Epoch 22 Train - loss: 0.38 Val - loss: 1.95 Epoch 23 Train - loss: 0.31 Val - loss: 1.91 Epoch 24 Train - loss: 0.28 Val - loss: 1.89 Epoch 25 Train - loss: 0.28 Val - loss: 2.05 Epoch 26 Train - loss: 0.26 Val - loss: 1.98 Epoch 27 Train - loss: 0.20 Val - loss: 1.93 Epoch 28 Train - loss: 0.21 Val - loss: 2.01 Epoch 29 Train - loss: 0.22 Val - loss: 2.20 Epoch 30 Train - loss: 0.19 Val - loss: 2.15 Engine run is terminating due to exception: . Engine run is terminating due to exception: . --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) &amp;lt;ipython-input-9-8ac1a0a79e01&amp;gt; in &amp;lt;module&amp;gt; 3 trainer = train_net(model, opt, loss_fn, val_metrics, 4 train_loader, val_loader, device) ----&amp;gt; 5 trainer.run(train_loader, max_epochs=max_epochs) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed) 656 657 self.state.dataloader = data --&amp;gt; 658 return self._internal_run() 659 660 @staticmethod ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 720 self._dataloader_iter = None 721 self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e)) --&amp;gt; 722 self._handle_exception(e) 723 724 self._dataloader_iter = None ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e) 435 self._fire_event(Events.EXCEPTION_RAISED, e) 436 else: --&amp;gt; 437 raise e 438 439 @property ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 708 self.logger.info(elapsed_time_message) 709 break --&amp;gt; 710 self._fire_event(Events.EPOCH_COMPLETED) 711 self.logger.info(elapsed_time_message) 712 ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _fire_event(self, event_name, *event_args, **event_kwargs) 391 kwargs.update(event_kwargs) 392 first, others = ((args[0],), args[1:]) if (args and args[0] == self) else ((), args) --&amp;gt; 393 func(*first, *(event_args + others), **kwargs) 394 395 def fire_event(self, event_name: Any) -&amp;gt; None: &amp;lt;ipython-input-8-05b4a011b4ff&amp;gt; in log_training_results(trainer) 15 @trainer.on(Events.EPOCH_COMPLETED) 16 def log_training_results(trainer): ---&amp;gt; 17 evaluator.run(train_loader) 18 print('Epoch {}'.format(trainer.state.epoch)) 19 message = 'Train - ' ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed) 656 657 self.state.dataloader = data --&amp;gt; 658 return self._internal_run() 659 660 @staticmethod ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 720 self._dataloader_iter = None 721 self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e)) --&amp;gt; 722 self._handle_exception(e) 723 724 self._dataloader_iter = None ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e) 435 self._fire_event(Events.EXCEPTION_RAISED, e) 436 else: --&amp;gt; 437 raise e 438 439 @property ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 695 self._setup_engine() 696 --&amp;gt; 697 time_taken = self._run_once_on_dataset() 698 self.state.times[Events.EPOCH_COMPLETED.name] = time_taken 699 hours, mins, secs = _to_hours_mins_secs(time_taken) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _run_once_on_dataset(self) 737 if self.last_event_name != Events.DATALOADER_STOP_ITERATION: 738 self._fire_event(Events.GET_BATCH_STARTED) --&amp;gt; 739 self.state.batch = next(self._dataloader_iter) 740 self._fire_event(Events.GET_BATCH_COMPLETED) 741 iter_counter += 1 ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self) 343 344 def __next__(self): --&amp;gt; 345 data = self._next_data() 346 self._num_yielded += 1 347 if self._dataset_kind == _DatasetKind.Iterable and \ ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self) 383 def _next_data(self): 384 index = self._next_index() # may raise StopIteration --&amp;gt; 385 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 386 if self._pin_memory: 387 data = _utils.pin_memory.pin_memory(data) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index) 42 def fetch(self, possibly_batched_index): 43 if self.auto_collation: ---&amp;gt; 44 data = [self.dataset[idx] for idx in possibly_batched_index] 45 else: 46 data = self.dataset[possibly_batched_index] ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in &amp;lt;listcomp&amp;gt;(.0) 42 def fetch(self, possibly_batched_index): 43 if self.auto_collation: ---&amp;gt; 44 data = [self.dataset[idx] for idx in possibly_batched_index] 45 else: 46 data = self.dataset[possibly_batched_index] ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/voc.py in __getitem__(self, index) 123 124 if self.transforms is not None: --&amp;gt; 125 img, target = self.transforms(img, target) 126 127 return img, target ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/vision.py in __call__(self, input, target) 61 input = self.transform(input) 62 if self.target_transform is not None: ---&amp;gt; 63 target = self.target_transform(target) 64 return input, target 65 &amp;lt;ipython-input-4-e0a946e58f28&amp;gt; in __call__(self, pic) 18 img = img.permute((2, 0, 1)) 19 for i, x in np.ndenumerate(img): ---&amp;gt; 20 if x == 255: 21 img[i] = 21 22 return img.long().view(-1) KeyboardInterrupt: Samples from torchvision.transforms import ToPILImage tpi = ToPILImage() import matplotlib.pyplot as plt fig = plt.figure(figsize=(30, 15)) ax = fig.subplots(2, 4) model.to('cpu') model.eval() for i, (image_, _) in zip(range(4), val_loader): image = image_.squeeze() ax[0][i].imshow(tpi(image)) ax[1][i].imshow(torch.argmax(model(image_), dim=1).view([i for i in image.shape[1: ]])) fig.show()</summary></entry><entry><title type="html">Sppnet</title><link href="http://localhost:4000/2020/10/26/sppnet.html" rel="alternate" type="text/html" title="Sppnet" /><published>2020-10-26T00:00:00+09:00</published><updated>2020-10-26T00:00:00+09:00</updated><id>http://localhost:4000/2020/10/26/sppnet</id><content type="html" xml:base="http://localhost:4000/2020/10/26/sppnet.html">&lt;h2 id=&quot;settings&quot;&gt;Settings&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_ext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;main&quot;&gt;Main&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ignite.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'acc'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;load-data&quot;&gt;Load Data&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'number of training data: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'number of test data: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;number of training data:  60000
number of test data:  10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StratifiedShuffleSplit&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Subset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StratifiedShuffleSplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;construct-model&quot;&gt;Construct Model&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SPPLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    cfg = [(H1, W1), (H2, W2), ...]
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SPPLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdaptiveMaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l{}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
        
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SPPNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SPPNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPPLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_pool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPPNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ignite.engine&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_evaluator&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non_blocking&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_supervised_evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{}: {:.2f} '&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCH_COMPLETED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_training_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epoch {}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Train - '&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCH_COMPLETED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_validation_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Val   - '&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch 1
Train - acc: 0.26 loss: 2.28 
Val   - acc: 0.26 loss: 2.28 
Epoch 2
Train - acc: 0.22 loss: 2.24 
Val   - acc: 0.22 loss: 2.24 
Epoch 3
Train - acc: 0.32 loss: 2.20 
Val   - acc: 0.31 loss: 2.20 
Epoch 4
Train - acc: 0.55 loss: 2.13 
Val   - acc: 0.55 loss: 2.13 
Epoch 5
Train - acc: 0.65 loss: 2.03 
Val   - acc: 0.65 loss: 2.04 
Epoch 6
Train - acc: 0.67 loss: 1.91 
Val   - acc: 0.67 loss: 1.91 
Epoch 7
Train - acc: 0.70 loss: 1.74 
Val   - acc: 0.69 loss: 1.75 
Epoch 8
Train - acc: 0.71 loss: 1.55 
Val   - acc: 0.71 loss: 1.55 
Epoch 9
Train - acc: 0.73 loss: 1.34 
Val   - acc: 0.73 loss: 1.35 
Epoch 10
Train - acc: 0.75 loss: 1.15 
Val   - acc: 0.75 loss: 1.15 
Epoch 11
Train - acc: 0.77 loss: 0.97 
Val   - acc: 0.76 loss: 0.98 
Epoch 12
Train - acc: 0.79 loss: 0.83 
Val   - acc: 0.79 loss: 0.83 
Epoch 13
Train - acc: 0.81 loss: 0.71 
Val   - acc: 0.81 loss: 0.72 
Epoch 14
Train - acc: 0.83 loss: 0.62 
Val   - acc: 0.83 loss: 0.63 
Epoch 15
Train - acc: 0.85 loss: 0.55 
Val   - acc: 0.85 loss: 0.56 
Epoch 16
Train - acc: 0.87 loss: 0.50 
Val   - acc: 0.87 loss: 0.51 
Epoch 17
Train - acc: 0.88 loss: 0.45 
Val   - acc: 0.88 loss: 0.46 
Epoch 18
Train - acc: 0.89 loss: 0.41 
Val   - acc: 0.89 loss: 0.43 
Epoch 19
Train - acc: 0.90 loss: 0.38 
Val   - acc: 0.89 loss: 0.40 
Epoch 20
Train - acc: 0.90 loss: 0.36 
Val   - acc: 0.90 loss: 0.37 
Epoch 21
Train - acc: 0.91 loss: 0.34 
Val   - acc: 0.91 loss: 0.35 
Epoch 22
Train - acc: 0.91 loss: 0.32 
Val   - acc: 0.91 loss: 0.33 
Epoch 23
Train - acc: 0.92 loss: 0.30 
Val   - acc: 0.92 loss: 0.31 
Epoch 24
Train - acc: 0.92 loss: 0.29 
Val   - acc: 0.92 loss: 0.30 
Epoch 25
Train - acc: 0.92 loss: 0.27 
Val   - acc: 0.92 loss: 0.29 
Epoch 26
Train - acc: 0.93 loss: 0.26 
Val   - acc: 0.93 loss: 0.27 
Epoch 27
Train - acc: 0.93 loss: 0.25 
Val   - acc: 0.93 loss: 0.26 
Epoch 28
Train - acc: 0.93 loss: 0.24 
Val   - acc: 0.93 loss: 0.25 
Epoch 29
Train - acc: 0.93 loss: 0.23 
Val   - acc: 0.93 loss: 0.24 
Epoch 30
Train - acc: 0.94 loss: 0.23 
Val   - acc: 0.93 loss: 0.24 
Epoch 31
Train - acc: 0.94 loss: 0.22 
Val   - acc: 0.94 loss: 0.23 
Epoch 32
Train - acc: 0.94 loss: 0.21 
Val   - acc: 0.94 loss: 0.22 
Epoch 33
Train - acc: 0.94 loss: 0.20 
Val   - acc: 0.94 loss: 0.21 
Epoch 34
Train - acc: 0.94 loss: 0.20 
Val   - acc: 0.94 loss: 0.21 
Epoch 35
Train - acc: 0.94 loss: 0.19 
Val   - acc: 0.94 loss: 0.20 
Epoch 36
Train - acc: 0.95 loss: 0.19 
Val   - acc: 0.95 loss: 0.19 
Epoch 37
Train - acc: 0.95 loss: 0.18 
Val   - acc: 0.95 loss: 0.19 
Epoch 38
Train - acc: 0.95 loss: 0.17 
Val   - acc: 0.95 loss: 0.18 
Epoch 39
Train - acc: 0.95 loss: 0.17 
Val   - acc: 0.95 loss: 0.18 
Epoch 40
Train - acc: 0.95 loss: 0.17 
Val   - acc: 0.95 loss: 0.18 
Epoch 41
Train - acc: 0.95 loss: 0.16 
Val   - acc: 0.95 loss: 0.17 
Epoch 42
Train - acc: 0.95 loss: 0.16 
Val   - acc: 0.95 loss: 0.17 
Epoch 43
Train - acc: 0.95 loss: 0.15 
Val   - acc: 0.95 loss: 0.16 
Epoch 44
Train - acc: 0.96 loss: 0.15 
Val   - acc: 0.95 loss: 0.16 
Epoch 45
Train - acc: 0.96 loss: 0.15 
Val   - acc: 0.95 loss: 0.16 
Epoch 46
Train - acc: 0.96 loss: 0.14 
Val   - acc: 0.96 loss: 0.15 
Epoch 47
Train - acc: 0.96 loss: 0.14 
Val   - acc: 0.96 loss: 0.15 
Epoch 48
Train - acc: 0.96 loss: 0.14 
Val   - acc: 0.96 loss: 0.15 
Epoch 49
Train - acc: 0.96 loss: 0.14 
Val   - acc: 0.96 loss: 0.15 
Epoch 50
Train - acc: 0.96 loss: 0.13 
Val   - acc: 0.96 loss: 0.14 
Epoch 51
Train - acc: 0.96 loss: 0.13 
Val   - acc: 0.96 loss: 0.14 
Epoch 52
Train - acc: 0.96 loss: 0.13 
Val   - acc: 0.96 loss: 0.14 
Epoch 53
Train - acc: 0.96 loss: 0.13 
Val   - acc: 0.96 loss: 0.14 
Epoch 54
Train - acc: 0.96 loss: 0.13 
Val   - acc: 0.96 loss: 0.13 
Epoch 55
Train - acc: 0.96 loss: 0.12 
Val   - acc: 0.96 loss: 0.13 
Epoch 56
Train - acc: 0.96 loss: 0.12 
Val   - acc: 0.96 loss: 0.13 
Epoch 57
Train - acc: 0.96 loss: 0.12 
Val   - acc: 0.96 loss: 0.13 
Epoch 58
Train - acc: 0.96 loss: 0.12 
Val   - acc: 0.96 loss: 0.13 
Epoch 59
Train - acc: 0.96 loss: 0.12 
Val   - acc: 0.96 loss: 0.13 
Epoch 60
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 61
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 62
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 63
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 64
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 65
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 66
Train - acc: 0.97 loss: 0.11 
Val   - acc: 0.96 loss: 0.12 
Epoch 67
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.12 
Epoch 68
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 69
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 70
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 71
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 72
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 73
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 74
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 75
Train - acc: 0.97 loss: 0.10 
Val   - acc: 0.97 loss: 0.11 
Epoch 76
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.11 
Epoch 77
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 78
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 79
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 80
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 81
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 82
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 83
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 84
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 85
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 86
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 87
Train - acc: 0.97 loss: 0.09 
Val   - acc: 0.97 loss: 0.10 
Epoch 88
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.10 
Epoch 89
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.10 
Epoch 90
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 91
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 92
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 93
Train - acc: 0.97 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 94
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 95
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 96
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 97
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 98
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 99
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 100
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 101
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 102
Train - acc: 0.98 loss: 0.08 
Val   - acc: 0.97 loss: 0.09 
Epoch 103
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 104
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 105
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 106
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 107
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 108
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.09 
Epoch 109
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 110
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 111
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 112
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 113
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 114
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 115
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 116
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 117
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.97 loss: 0.08 
Epoch 118
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 119
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 120
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 121
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 122
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 123
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 124
Train - acc: 0.98 loss: 0.07 
Val   - acc: 0.98 loss: 0.08 
Epoch 125
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 126
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 127
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 128
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 129
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 130
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 131
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 132
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 133
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 134
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 135
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 136
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 137
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 138
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.08 
Epoch 139
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 140
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 141
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 142
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 143
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 144
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 145
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 146
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 147
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 148
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 149
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 150
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 151
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 152
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 153
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 154
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 155
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 156
Train - acc: 0.98 loss: 0.06 
Val   - acc: 0.98 loss: 0.07 
Epoch 157
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 158
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 159
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 160
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 161
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 162
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 163
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 164
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 165
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 166
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 167
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 168
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 169
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 170
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 171
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 172
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 173
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 174
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 175
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 176
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 177
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 178
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 179
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 180
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 181
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 182
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 183
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 184
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 185
Train - acc: 0.98 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 186
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 187
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 188
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 189
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 190
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 191
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 192
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 193
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.07 
Epoch 194
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 195
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 196
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 197
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 198
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 199
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 200
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 201
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 202
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 203
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 204
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 205
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 206
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 207
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 208
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 209
Train - acc: 0.99 loss: 0.05 
Val   - acc: 0.98 loss: 0.06 
Epoch 210
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 211
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 212
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 213
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 214
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 215
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 216
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 217
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 218
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 219
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 220
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 221
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 222
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 223
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 224
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 225
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 226
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 227
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 228
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 229
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 230
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 231
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 232
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 233
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 234
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 235
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 236
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 237
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 238
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 239
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 240
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 241
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 242
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 243
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 244
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 245
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 246
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 247
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 248
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 249
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 250
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 251
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 252
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 253
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 254
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 255
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 256
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 257
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 258
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 259
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 260
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 261
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 262
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 263
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 264
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 265
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 266
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 267
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 268
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 269
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 270
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 271
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 272
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 273
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 274
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 275
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 276
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 277
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 278
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 279
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 280
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 281
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 282
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 283
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 284
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 285
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 286
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 287
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 288
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 289
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 290
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 291
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 292
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 293
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 294
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 295
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 296
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 297
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 298
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 299
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 300
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 301
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 302
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 303
Train - acc: 0.99 loss: 0.04 
Val   - acc: 0.98 loss: 0.06 
Epoch 304
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 305
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 306
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 307
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 308
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 309
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 310
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 311
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 312
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 313
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 314
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 315
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 316
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 317
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 318
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 319
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 320
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 321
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 322
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 323
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 324
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 325
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 326
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 327
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 328
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 329
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.06 
Epoch 330
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 331
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 332
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 333
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 334
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 335
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 336
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 337
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 338
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 339
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 340
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 341
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 342
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 343
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 344
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 345
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 346
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 347
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 348
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 349
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 350
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 351
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 352
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 353
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 354
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 355
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 356
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 357
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 358
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 359
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 360
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 361
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 362
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 363
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 364
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 365
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 366
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 367
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 368
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 369
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 370
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 371
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 372
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 373
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 374
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 375
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 376
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 377
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 378
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 379
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 380
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.98 loss: 0.05 
Epoch 381
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 382
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 383
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 384
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 385
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 386
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 387
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 388
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 389
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 390
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 391
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 392
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 393
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 394
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 395
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 396
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 397
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 398
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 399
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 400
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 401
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 402
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 403
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 404
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 405
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 406
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 407
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 408
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 409
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 410
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 411
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 412
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 413
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 414
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 415
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 416
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 417
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 418
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 419
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 420
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 421
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 422
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 423
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 424
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 425
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 426
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 427
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 428
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 429
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 430
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 431
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 432
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 433
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 434
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 435
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 436
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 437
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 438
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 439
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 440
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 441
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 442
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 443
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 444
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 445
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 446
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 447
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 448
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 449
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 450
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 451
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 452
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 453
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 454
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 455
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 456
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 457
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 458
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 459
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 460
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 461
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 462
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 463
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 464
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 465
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 466
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 467
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 468
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 469
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 470
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 471
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 472
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 473
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 474
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 475
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 476
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 477
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 478
Train - acc: 0.99 loss: 0.03 
Val   - acc: 0.99 loss: 0.05 
Epoch 479
Train - acc: 0.99 loss: 0.02 
Val   - acc: 0.99 loss: 0.05 
Epoch 480
Train - acc: 0.99 loss: 0.02 
Val   - acc: 0.99 loss: 0.05 
Epoch 481
Train - acc: 0.99 loss: 0.02 
Val   - acc: 0.99 loss: 0.05 
Epoch 482
Train - acc: 0.99 loss: 0.02 
Val   - acc: 0.99 loss: 0.05 
Epoch 483
Train - acc: 0.99 loss: 0.02 
Val   - acc: 0.99 loss: 0.05 


Engine run is terminating due to exception: .
Engine run is terminating due to exception: .



---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

&amp;lt;ipython-input-10-8ac1a0a79e01&amp;gt; in &amp;lt;module&amp;gt;
      3 trainer = train_net(model, opt, loss_fn, val_metrics,
      4         train_loader, val_loader, device)
----&amp;gt; 5 trainer.run(train_loader, max_epochs=max_epochs)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed)
    656 
    657         self.state.dataloader = data
--&amp;gt; 658         return self._internal_run()
    659 
    660     @staticmethod


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    720             self._dataloader_iter = None
    721             self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e))
--&amp;gt; 722             self._handle_exception(e)
    723 
    724         self._dataloader_iter = None


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e)
    435             self._fire_event(Events.EXCEPTION_RAISED, e)
    436         else:
--&amp;gt; 437             raise e
    438 
    439     @property


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    708                     self.logger.info(elapsed_time_message)
    709                     break
--&amp;gt; 710                 self._fire_event(Events.EPOCH_COMPLETED)
    711                 self.logger.info(elapsed_time_message)
    712 


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _fire_event(self, event_name, *event_args, **event_kwargs)
    391                 kwargs.update(event_kwargs)
    392                 first, others = ((args[0],), args[1:]) if (args and args[0] == self) else ((), args)
--&amp;gt; 393                 func(*first, *(event_args + others), **kwargs)
    394 
    395     def fire_event(self, event_name: Any) -&amp;gt; None:


&amp;lt;ipython-input-9-05b4a011b4ff&amp;gt; in log_training_results(trainer)
     15     @trainer.on(Events.EPOCH_COMPLETED)
     16     def log_training_results(trainer):
---&amp;gt; 17         evaluator.run(train_loader)
     18         print('Epoch {}'.format(trainer.state.epoch))
     19         message = 'Train - '


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed)
    656 
    657         self.state.dataloader = data
--&amp;gt; 658         return self._internal_run()
    659 
    660     @staticmethod


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    720             self._dataloader_iter = None
    721             self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e))
--&amp;gt; 722             self._handle_exception(e)
    723 
    724         self._dataloader_iter = None


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e)
    435             self._fire_event(Events.EXCEPTION_RAISED, e)
    436         else:
--&amp;gt; 437             raise e
    438 
    439     @property


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self)
    695                     self._setup_engine()
    696 
--&amp;gt; 697                 time_taken = self._run_once_on_dataset()
    698                 self.state.times[Events.EPOCH_COMPLETED.name] = time_taken
    699                 hours, mins, secs = _to_hours_mins_secs(time_taken)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _run_once_on_dataset(self)
    737                     if self.last_event_name != Events.DATALOADER_STOP_ITERATION:
    738                         self._fire_event(Events.GET_BATCH_STARTED)
--&amp;gt; 739                     self.state.batch = next(self._dataloader_iter)
    740                     self._fire_event(Events.GET_BATCH_COMPLETED)
    741                     iter_counter += 1


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    343 
    344     def __next__(self):
--&amp;gt; 345         data = self._next_data()
    346         self._num_yielded += 1
    347         if self._dataset_kind == _DatasetKind.Iterable and \


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    383     def _next_data(self):
    384         index = self._next_index()  # may raise StopIteration
--&amp;gt; 385         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    386         if self._pin_memory:
    387             data = _utils.pin_memory.pin_memory(data)


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&amp;gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in &amp;lt;listcomp&amp;gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&amp;gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataset.py in __getitem__(self, idx)
    255 
    256     def __getitem__(self, idx):
--&amp;gt; 257         return self.dataset[self.indices[idx]]
    258 
    259     def __len__(self):


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/mnist.py in __getitem__(self, index)
     95 
     96         if self.transform is not None:
---&amp;gt; 97             img = self.transform(img)
     98 
     99         if self.target_transform is not None:


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/transforms/transforms.py in __call__(self, pic)
     99             Tensor: Converted image.
    100         &quot;&quot;&quot;
--&amp;gt; 101         return F.to_tensor(pic)
    102 
    103     def __repr__(self):


~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/transforms/functional.py in to_tensor(pic)
     98     img = img.transpose(0, 1).transpose(0, 2).contiguous()
     99     if isinstance(img, torch.ByteTensor):
--&amp;gt; 100         return img.float().div(255)
    101     else:
    102         return img


KeyboardInterrupt: 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>mobiismlteam</name></author><summary type="html">Settings %load_ext autoreload %autoreload 2 Main import torch from torch import nn from torch.nn import functional as F from ignite.metrics import Accuracy, Loss batch_size = 10000 loss_fn = nn.CrossEntropyLoss() opt_ = torch.optim.Adam lr = 0.001 val_metrics = { 'acc': Accuracy(), 'loss': Loss(loss_fn) } device = 'cuda:0' max_epochs = 1000 Load Data from torchvision.datasets import MNIST from torchvision import transforms train_data = MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True) test_data = MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True) print('number of training data: ', len(train_data)) print('number of test data: ', len(test_data)) number of training data: 60000 number of test data: 10000 from sklearn.model_selection import StratifiedShuffleSplit from torch.utils.data import Subset s = StratifiedShuffleSplit(n_splits=1, test_size=10000) for train_idx, val_idx in s.split(train_data.data, train_data.targets): train_data, val_data = Subset(train_data, train_idx), Subset(train_data, val_idx) from torch.utils.data import DataLoader train_loader = DataLoader(train_data, batch_size=batch_size) val_loader = DataLoader(val_data, batch_size=batch_size) test_loader = DataLoader(test_data, batch_size=batch_size) Construct Model class SPPLayer(nn.Module): &quot;&quot;&quot; cfg = [(H1, W1), (H2, W2), ...] &quot;&quot;&quot; def __init__(self, cfg): super(SPPLayer, self).__init__() self.layers = [] for size in cfg: self.layers.append(nn.AdaptiveMaxPool2d(size)) for i, l in enumerate(self.layers): self.add_module('l{}'.format(i), l) def forward(self, x): x = torch.cat([l(x).flatten(1, 3) for l in self.layers], 1) return x class SPPNet(nn.Module): def __init__(self, cfg): super(SPPNet, self).__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1) self.spp = SPPLayer(cfg) self.fc = nn.Linear(sum([h * w * 8 for h, w in cfg]), 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), 2) x = F.relu(self.conv2(x)) x = self.spp(x) x = self.fc(x) return x model = SPPNet([(4, 4), (2, 2), (1, 1)]) from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator def train_net(net, opt, loss_fn, val_metrics, train_loader, val_loader, device): net.to(device) def prepare_batch(batch, device, non_blocking=False): x, y = batch return x.to(device), y.to(device) def output_transform(x, y, y_pred, loss): return (y_pred.max(1)[1], y) trainer = create_supervised_trainer(net, opt, loss_fn, device, prepare_batch=prepare_batch, output_transform=output_transform) evaluator = create_supervised_evaluator(net, val_metrics, device, prepare_batch=prepare_batch) s = '{}: {:.2f} ' @trainer.on(Events.EPOCH_COMPLETED) def log_training_results(trainer): evaluator.run(train_loader) print('Epoch {}'.format(trainer.state.epoch)) message = 'Train - ' for m in val_metrics.keys(): message += s.format(m, evaluator.state.metrics[m]) print(message) @trainer.on(Events.EPOCH_COMPLETED) def log_validation_results(trainer): evaluator.run(val_loader) message = 'Val - ' for m in val_metrics.keys(): message += s.format(m, evaluator.state.metrics[m]) print(message) return trainer opt = opt_(model.parameters(), lr) trainer = train_net(model, opt, loss_fn, val_metrics, train_loader, val_loader, device) trainer.run(train_loader, max_epochs=max_epochs) Epoch 1 Train - acc: 0.26 loss: 2.28 Val - acc: 0.26 loss: 2.28 Epoch 2 Train - acc: 0.22 loss: 2.24 Val - acc: 0.22 loss: 2.24 Epoch 3 Train - acc: 0.32 loss: 2.20 Val - acc: 0.31 loss: 2.20 Epoch 4 Train - acc: 0.55 loss: 2.13 Val - acc: 0.55 loss: 2.13 Epoch 5 Train - acc: 0.65 loss: 2.03 Val - acc: 0.65 loss: 2.04 Epoch 6 Train - acc: 0.67 loss: 1.91 Val - acc: 0.67 loss: 1.91 Epoch 7 Train - acc: 0.70 loss: 1.74 Val - acc: 0.69 loss: 1.75 Epoch 8 Train - acc: 0.71 loss: 1.55 Val - acc: 0.71 loss: 1.55 Epoch 9 Train - acc: 0.73 loss: 1.34 Val - acc: 0.73 loss: 1.35 Epoch 10 Train - acc: 0.75 loss: 1.15 Val - acc: 0.75 loss: 1.15 Epoch 11 Train - acc: 0.77 loss: 0.97 Val - acc: 0.76 loss: 0.98 Epoch 12 Train - acc: 0.79 loss: 0.83 Val - acc: 0.79 loss: 0.83 Epoch 13 Train - acc: 0.81 loss: 0.71 Val - acc: 0.81 loss: 0.72 Epoch 14 Train - acc: 0.83 loss: 0.62 Val - acc: 0.83 loss: 0.63 Epoch 15 Train - acc: 0.85 loss: 0.55 Val - acc: 0.85 loss: 0.56 Epoch 16 Train - acc: 0.87 loss: 0.50 Val - acc: 0.87 loss: 0.51 Epoch 17 Train - acc: 0.88 loss: 0.45 Val - acc: 0.88 loss: 0.46 Epoch 18 Train - acc: 0.89 loss: 0.41 Val - acc: 0.89 loss: 0.43 Epoch 19 Train - acc: 0.90 loss: 0.38 Val - acc: 0.89 loss: 0.40 Epoch 20 Train - acc: 0.90 loss: 0.36 Val - acc: 0.90 loss: 0.37 Epoch 21 Train - acc: 0.91 loss: 0.34 Val - acc: 0.91 loss: 0.35 Epoch 22 Train - acc: 0.91 loss: 0.32 Val - acc: 0.91 loss: 0.33 Epoch 23 Train - acc: 0.92 loss: 0.30 Val - acc: 0.92 loss: 0.31 Epoch 24 Train - acc: 0.92 loss: 0.29 Val - acc: 0.92 loss: 0.30 Epoch 25 Train - acc: 0.92 loss: 0.27 Val - acc: 0.92 loss: 0.29 Epoch 26 Train - acc: 0.93 loss: 0.26 Val - acc: 0.93 loss: 0.27 Epoch 27 Train - acc: 0.93 loss: 0.25 Val - acc: 0.93 loss: 0.26 Epoch 28 Train - acc: 0.93 loss: 0.24 Val - acc: 0.93 loss: 0.25 Epoch 29 Train - acc: 0.93 loss: 0.23 Val - acc: 0.93 loss: 0.24 Epoch 30 Train - acc: 0.94 loss: 0.23 Val - acc: 0.93 loss: 0.24 Epoch 31 Train - acc: 0.94 loss: 0.22 Val - acc: 0.94 loss: 0.23 Epoch 32 Train - acc: 0.94 loss: 0.21 Val - acc: 0.94 loss: 0.22 Epoch 33 Train - acc: 0.94 loss: 0.20 Val - acc: 0.94 loss: 0.21 Epoch 34 Train - acc: 0.94 loss: 0.20 Val - acc: 0.94 loss: 0.21 Epoch 35 Train - acc: 0.94 loss: 0.19 Val - acc: 0.94 loss: 0.20 Epoch 36 Train - acc: 0.95 loss: 0.19 Val - acc: 0.95 loss: 0.19 Epoch 37 Train - acc: 0.95 loss: 0.18 Val - acc: 0.95 loss: 0.19 Epoch 38 Train - acc: 0.95 loss: 0.17 Val - acc: 0.95 loss: 0.18 Epoch 39 Train - acc: 0.95 loss: 0.17 Val - acc: 0.95 loss: 0.18 Epoch 40 Train - acc: 0.95 loss: 0.17 Val - acc: 0.95 loss: 0.18 Epoch 41 Train - acc: 0.95 loss: 0.16 Val - acc: 0.95 loss: 0.17 Epoch 42 Train - acc: 0.95 loss: 0.16 Val - acc: 0.95 loss: 0.17 Epoch 43 Train - acc: 0.95 loss: 0.15 Val - acc: 0.95 loss: 0.16 Epoch 44 Train - acc: 0.96 loss: 0.15 Val - acc: 0.95 loss: 0.16 Epoch 45 Train - acc: 0.96 loss: 0.15 Val - acc: 0.95 loss: 0.16 Epoch 46 Train - acc: 0.96 loss: 0.14 Val - acc: 0.96 loss: 0.15 Epoch 47 Train - acc: 0.96 loss: 0.14 Val - acc: 0.96 loss: 0.15 Epoch 48 Train - acc: 0.96 loss: 0.14 Val - acc: 0.96 loss: 0.15 Epoch 49 Train - acc: 0.96 loss: 0.14 Val - acc: 0.96 loss: 0.15 Epoch 50 Train - acc: 0.96 loss: 0.13 Val - acc: 0.96 loss: 0.14 Epoch 51 Train - acc: 0.96 loss: 0.13 Val - acc: 0.96 loss: 0.14 Epoch 52 Train - acc: 0.96 loss: 0.13 Val - acc: 0.96 loss: 0.14 Epoch 53 Train - acc: 0.96 loss: 0.13 Val - acc: 0.96 loss: 0.14 Epoch 54 Train - acc: 0.96 loss: 0.13 Val - acc: 0.96 loss: 0.13 Epoch 55 Train - acc: 0.96 loss: 0.12 Val - acc: 0.96 loss: 0.13 Epoch 56 Train - acc: 0.96 loss: 0.12 Val - acc: 0.96 loss: 0.13 Epoch 57 Train - acc: 0.96 loss: 0.12 Val - acc: 0.96 loss: 0.13 Epoch 58 Train - acc: 0.96 loss: 0.12 Val - acc: 0.96 loss: 0.13 Epoch 59 Train - acc: 0.96 loss: 0.12 Val - acc: 0.96 loss: 0.13 Epoch 60 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 61 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 62 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 63 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 64 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 65 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 66 Train - acc: 0.97 loss: 0.11 Val - acc: 0.96 loss: 0.12 Epoch 67 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.12 Epoch 68 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 69 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 70 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 71 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 72 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 73 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 74 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 75 Train - acc: 0.97 loss: 0.10 Val - acc: 0.97 loss: 0.11 Epoch 76 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.11 Epoch 77 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 78 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 79 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 80 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 81 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 82 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 83 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 84 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 85 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 86 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 87 Train - acc: 0.97 loss: 0.09 Val - acc: 0.97 loss: 0.10 Epoch 88 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.10 Epoch 89 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.10 Epoch 90 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 91 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 92 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 93 Train - acc: 0.97 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 94 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 95 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 96 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 97 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 98 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 99 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 100 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 101 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 102 Train - acc: 0.98 loss: 0.08 Val - acc: 0.97 loss: 0.09 Epoch 103 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 104 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 105 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 106 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 107 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 108 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.09 Epoch 109 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 110 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 111 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 112 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 113 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 114 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 115 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 116 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 117 Train - acc: 0.98 loss: 0.07 Val - acc: 0.97 loss: 0.08 Epoch 118 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 119 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 120 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 121 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 122 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 123 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 124 Train - acc: 0.98 loss: 0.07 Val - acc: 0.98 loss: 0.08 Epoch 125 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 126 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 127 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 128 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 129 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 130 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 131 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 132 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 133 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 134 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 135 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 136 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 137 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 138 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.08 Epoch 139 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 140 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 141 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 142 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 143 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 144 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 145 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 146 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 147 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 148 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 149 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 150 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 151 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 152 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 153 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 154 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 155 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 156 Train - acc: 0.98 loss: 0.06 Val - acc: 0.98 loss: 0.07 Epoch 157 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 158 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 159 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 160 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 161 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 162 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 163 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 164 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 165 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 166 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 167 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 168 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 169 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 170 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 171 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 172 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 173 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 174 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 175 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 176 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 177 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 178 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 179 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 180 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 181 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 182 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 183 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 184 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 185 Train - acc: 0.98 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 186 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 187 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 188 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 189 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 190 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 191 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 192 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 193 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.07 Epoch 194 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 195 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 196 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 197 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 198 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 199 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 200 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 201 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 202 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 203 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 204 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 205 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 206 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 207 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 208 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 209 Train - acc: 0.99 loss: 0.05 Val - acc: 0.98 loss: 0.06 Epoch 210 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 211 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 212 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 213 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 214 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 215 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 216 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 217 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 218 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 219 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 220 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 221 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 222 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 223 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 224 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 225 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 226 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 227 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 228 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 229 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 230 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 231 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 232 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 233 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 234 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 235 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 236 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 237 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 238 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 239 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 240 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 241 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 242 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 243 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 244 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 245 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 246 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 247 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 248 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 249 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 250 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 251 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 252 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 253 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 254 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 255 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 256 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 257 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 258 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 259 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 260 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 261 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 262 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 263 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 264 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 265 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 266 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 267 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 268 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 269 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 270 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 271 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 272 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 273 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 274 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 275 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 276 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 277 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 278 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 279 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 280 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 281 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 282 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 283 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 284 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 285 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 286 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 287 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 288 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 289 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 290 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 291 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 292 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 293 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 294 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 295 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 296 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 297 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 298 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 299 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 300 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 301 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 302 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 303 Train - acc: 0.99 loss: 0.04 Val - acc: 0.98 loss: 0.06 Epoch 304 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 305 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 306 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 307 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 308 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 309 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 310 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 311 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 312 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 313 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 314 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 315 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 316 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 317 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 318 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 319 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 320 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 321 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 322 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 323 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 324 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 325 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 326 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 327 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 328 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 329 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.06 Epoch 330 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 331 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 332 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 333 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 334 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 335 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 336 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 337 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 338 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 339 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 340 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 341 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 342 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 343 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 344 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 345 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 346 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 347 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 348 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 349 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 350 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 351 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 352 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 353 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 354 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 355 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 356 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 357 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 358 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 359 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 360 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 361 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 362 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 363 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 364 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 365 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 366 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 367 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 368 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 369 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 370 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 371 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 372 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 373 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 374 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 375 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 376 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 377 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 378 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 379 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 380 Train - acc: 0.99 loss: 0.03 Val - acc: 0.98 loss: 0.05 Epoch 381 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 382 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 383 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 384 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 385 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 386 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 387 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 388 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 389 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 390 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 391 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 392 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 393 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 394 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 395 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 396 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 397 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 398 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 399 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 400 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 401 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 402 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 403 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 404 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 405 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 406 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 407 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 408 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 409 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 410 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 411 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 412 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 413 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 414 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 415 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 416 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 417 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 418 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 419 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 420 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 421 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 422 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 423 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 424 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 425 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 426 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 427 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 428 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 429 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 430 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 431 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 432 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 433 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 434 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 435 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 436 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 437 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 438 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 439 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 440 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 441 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 442 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 443 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 444 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 445 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 446 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 447 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 448 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 449 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 450 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 451 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 452 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 453 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 454 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 455 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 456 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 457 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 458 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 459 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 460 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 461 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 462 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 463 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 464 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 465 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 466 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 467 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 468 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 469 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 470 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 471 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 472 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 473 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 474 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 475 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 476 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 477 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 478 Train - acc: 0.99 loss: 0.03 Val - acc: 0.99 loss: 0.05 Epoch 479 Train - acc: 0.99 loss: 0.02 Val - acc: 0.99 loss: 0.05 Epoch 480 Train - acc: 0.99 loss: 0.02 Val - acc: 0.99 loss: 0.05 Epoch 481 Train - acc: 0.99 loss: 0.02 Val - acc: 0.99 loss: 0.05 Epoch 482 Train - acc: 0.99 loss: 0.02 Val - acc: 0.99 loss: 0.05 Epoch 483 Train - acc: 0.99 loss: 0.02 Val - acc: 0.99 loss: 0.05 Engine run is terminating due to exception: . Engine run is terminating due to exception: . --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) &amp;lt;ipython-input-10-8ac1a0a79e01&amp;gt; in &amp;lt;module&amp;gt; 3 trainer = train_net(model, opt, loss_fn, val_metrics, 4 train_loader, val_loader, device) ----&amp;gt; 5 trainer.run(train_loader, max_epochs=max_epochs) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed) 656 657 self.state.dataloader = data --&amp;gt; 658 return self._internal_run() 659 660 @staticmethod ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 720 self._dataloader_iter = None 721 self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e)) --&amp;gt; 722 self._handle_exception(e) 723 724 self._dataloader_iter = None ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e) 435 self._fire_event(Events.EXCEPTION_RAISED, e) 436 else: --&amp;gt; 437 raise e 438 439 @property ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 708 self.logger.info(elapsed_time_message) 709 break --&amp;gt; 710 self._fire_event(Events.EPOCH_COMPLETED) 711 self.logger.info(elapsed_time_message) 712 ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _fire_event(self, event_name, *event_args, **event_kwargs) 391 kwargs.update(event_kwargs) 392 first, others = ((args[0],), args[1:]) if (args and args[0] == self) else ((), args) --&amp;gt; 393 func(*first, *(event_args + others), **kwargs) 394 395 def fire_event(self, event_name: Any) -&amp;gt; None: &amp;lt;ipython-input-9-05b4a011b4ff&amp;gt; in log_training_results(trainer) 15 @trainer.on(Events.EPOCH_COMPLETED) 16 def log_training_results(trainer): ---&amp;gt; 17 evaluator.run(train_loader) 18 print('Epoch {}'.format(trainer.state.epoch)) 19 message = 'Train - ' ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in run(self, data, max_epochs, epoch_length, seed) 656 657 self.state.dataloader = data --&amp;gt; 658 return self._internal_run() 659 660 @staticmethod ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 720 self._dataloader_iter = None 721 self.logger.error(&quot;Engine run is terminating due to exception: %s.&quot;, str(e)) --&amp;gt; 722 self._handle_exception(e) 723 724 self._dataloader_iter = None ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _handle_exception(self, e) 435 self._fire_event(Events.EXCEPTION_RAISED, e) 436 else: --&amp;gt; 437 raise e 438 439 @property ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _internal_run(self) 695 self._setup_engine() 696 --&amp;gt; 697 time_taken = self._run_once_on_dataset() 698 self.state.times[Events.EPOCH_COMPLETED.name] = time_taken 699 hours, mins, secs = _to_hours_mins_secs(time_taken) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/ignite/engine/engine.py in _run_once_on_dataset(self) 737 if self.last_event_name != Events.DATALOADER_STOP_ITERATION: 738 self._fire_event(Events.GET_BATCH_STARTED) --&amp;gt; 739 self.state.batch = next(self._dataloader_iter) 740 self._fire_event(Events.GET_BATCH_COMPLETED) 741 iter_counter += 1 ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self) 343 344 def __next__(self): --&amp;gt; 345 data = self._next_data() 346 self._num_yielded += 1 347 if self._dataset_kind == _DatasetKind.Iterable and \ ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self) 383 def _next_data(self): 384 index = self._next_index() # may raise StopIteration --&amp;gt; 385 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 386 if self._pin_memory: 387 data = _utils.pin_memory.pin_memory(data) ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index) 42 def fetch(self, possibly_batched_index): 43 if self.auto_collation: ---&amp;gt; 44 data = [self.dataset[idx] for idx in possibly_batched_index] 45 else: 46 data = self.dataset[possibly_batched_index] ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in &amp;lt;listcomp&amp;gt;(.0) 42 def fetch(self, possibly_batched_index): 43 if self.auto_collation: ---&amp;gt; 44 data = [self.dataset[idx] for idx in possibly_batched_index] 45 else: 46 data = self.dataset[possibly_batched_index] ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torch/utils/data/dataset.py in __getitem__(self, idx) 255 256 def __getitem__(self, idx): --&amp;gt; 257 return self.dataset[self.indices[idx]] 258 259 def __len__(self): ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/datasets/mnist.py in __getitem__(self, index) 95 96 if self.transform is not None: ---&amp;gt; 97 img = self.transform(img) 98 99 if self.target_transform is not None: ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/transforms/transforms.py in __call__(self, pic) 99 Tensor: Converted image. 100 &quot;&quot;&quot; --&amp;gt; 101 return F.to_tensor(pic) 102 103 def __repr__(self): ~/.virtualenvs/BaeJR_py36/lib/python3.6/site-packages/torchvision/transforms/functional.py in to_tensor(pic) 98 img = img.transpose(0, 1).transpose(0, 2).contiguous() 99 if isinstance(img, torch.ByteTensor): --&amp;gt; 100 return img.float().div(255) 101 else: 102 return img KeyboardInterrupt:</summary></entry></feed>