---
layout: article
title: Noise Constrative Estimation
mathjax: true
toc : true
tags :
  - NCE
  - Density estimation
  - AISTATS
---




Gutmann and Hyvarinen, 2010, **Noise-contrastive estimation: A new estimation principle for unnormalized statistical models**, *AISTATS*. [pdf](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)




# Density Estimation Problem
  * Given a sample of a random vector $\mathbf{x} \in \mathbb{R}^n$ from an unknown pdf $p_d(\cdot)$
  * Our model: a parameterized family of functions $\lbrace p_m(\cdot;\alpha) \rbrace_{\alpha}$
  * How to estimate $\alpha$ from the observed sample by maximizing some objective function?
  * Any solution to this estimation problem must yield a normalized density
    <br/>
    <br/>
    $$
    \begin{aligned}
    & p_m(\cdot;\alpha) = \frac{p_m^0(\cdot;\alpha)}{Z(\alpha)} \\[11pt]
    & Z(\alpha) = \int p_m^0(\mathbf{u};\alpha) d\mathbf{u}
    \end{aligned}
    $$
    <br/>
    <br/>
  * However, the calculation of $Z(\alpha)$ is very problematic




# Noise-Constrative Estimator
  * Learning by comparison: by discriminating between data and noise, learn properties of the data in the from of a statistical model
  * Definition
    + Including the $Z(\alpha)$ as  another parameter $c$
      <br/>
      <br/>
      $$
      \begin{aligned}
      & \log p_m(\cdot;\theta) = \log p_m^0(\cdot;\alpha) + c \\[11pt]
      & \theta = \lbrace \alpha, c \rbrace
      \end{aligned}
      $$
      <br/>
      <br/>
    + $c$: an estimate of the negative logarithm of $Z(\alpha)$
    + $p_m(\cdot;\theta)$ will only integrate to one for some specific choice of $c$
  * Observations
    + Observed data: $X = \lbrace \mathbf{x}_1, \cdots, \mathbf{x}_T \rbrace \sim p_m(\cdot;\theta)$
    + Noise (artificial generated): $Y = \lbrace \mathbf{y}_1, \cdots, \mathbf{y}_T \rbrace \sim p_n(\cdot)$
    + Union of $X$ and $Y$: $U = ( \mathbf{u} _1, \cdots, \mathbf{u} _{2T} )$
    + $C_t$: Binary class label of $\mathbf{u}_t$
      <br/>
      <br/>
      $$
      \begin{aligned}
      C_t = \begin{cases}
      1 & \mathbf{u}_t \in X \\[11pt]
      0 & \mathbf{u}_t \in Y
      \end{cases}
      \end{aligned}
      $$
      <br/>
      <br/>
  * Objective function $J_T(\theta)$
    <br/>
    <br/>
    $$
    \begin{aligned}
    & G(\mathbf{u} ; \theta)  = \log p_m(\mathbf{u} ; \theta) - \log p_n(\mathbf{u}) \\[11pt]
    & h(\mathbf{u} ; \theta) = \frac{1}{1 + \exp(-G(\mathbf{u} ; \theta))} \\[11pt]
    & J_T(\theta) = \frac{1}{2T} \sum_t \left( \log h(\mathbf{x}_t ; \theta) + \log (1 - h(\mathbf{y}_t ; \theta)) \right) \\[11pt]
    & \hat{\theta}_T = \text{argmax}_{\theta} J_T(\theta)
    \end{aligned}
    $$
    <br/>
    <br/>
    + Class-conditional probability
      <br/>
      <br/>
      $$
      \begin{aligned}
      & P( \mathbf{u} | C=1 ; \theta) = p_m(\mathbf{u};\theta) \\[11pt]
      & P(\mathbf{u} | C = 0) = p_n(\mathbf{u})
      \end{aligned}
      $$
      <br/>
      <br/>
    + Posterior probability
      <br/>
      <br/>
      $$
      \begin{aligned}
      P(C=1 | \mathbf{u} ; \theta) & = \frac{P( \mathbf{u} | C=1 ; \theta) P(C=1)}{P(\mathbf{u} ; \theta)} \\[11pt]
      & = \frac{p_m(\mathbf{u} ; \theta)}{p_m(\mathbf{u} ; \theta) + p_n(\mathbf{u})} \\[11pt]
      & = h(\mathbf{u} ; \theta) \\[11pt]
      P(C=0 | \mathbf{u} ; \theta) & = 1 - h(\mathbf{u} ; \theta)
      \end{aligned}
      $$
      <br/>
      <br/>
    + Log-likelihood for Bernoulli-distributed $C_t$
      <br/>
      <br/>
      $$
      \begin{aligned}
      \mathcal{l}(\theta) & = \sum_t (C_t \log P(C_t = 1|\mathbf{u}_t ; \theta) + (1 - C_t) \log P(C_t = 0|\mathbf{u}_t ; \theta)) \\[11pt]
      & = \sum_t \left( \log h(\mathbf{x}_t ; \theta) + \log (1 - h(\mathbf{y}_t ; \theta)) \right)
      \end{aligned}
      $$
      <br/>
      <br/>




# Properties of the Estimator
  * The behavior of the estimator $\hat{\theta}_T$ when the sample size $T$ becomes arbitrarily large
  * $J_T(\theta)$ converges in probability to $J$
    <br/>
    <br/>
    $$
    \begin{aligned}
    J(\theta) = \frac{1}{2} \mathbb{E} \left[ \log h(\mathbf{x} ; \theta) + \log (1 - h(\mathbf{y} ; \theta)) \right]
    \end{aligned}
    $$
    <br/>
    <br/>
  * Let $\tilde{J}$ denote the objective $J$ seen as a function of $f(\cdot) = \log p_m(\cdot ; \theta)$, where $r(\cdot)$ is logistic function
    <br/>
    <br/>
    $$
    \begin{aligned}
    \tilde{J}(\theta) = \frac{1}{2} \mathbb{E} \left[ \log r(f(\mathbf{x}) - \log p_n(\mathbf{x})) + \log (1 - r(f(\mathbf{x}) - \log p_n(\mathbf{x}))) \right]
    \end{aligned}
    $$
    <br/>
    <br/>
  * **Theorem 1** (Nonparametric estimation) $\tilde{J}$ attains a maximum at $f(\cdot) = \log p_d(\cdot)$. There are no other extrema if the noise density $p_n(\cdot)$ is chosen such it is nonzero whenever $p_d(\cdot)$ is nonzero
    + The data pdf $p_d(\cdot)$ can be found by maximization of $\tilde{J}$, i.e. by learning a classifier under the ideal situation of infinite amount of data
  * **Theorem 2** (Consistency) If conditions (a) to (c) are fulfilled then $\hat{\theta}_T$ converges in probability to $\theta^{\star}$
    <br/>
    <br/>
    $$
    \begin{aligned}
    \\
    & \text{(a) } p_n(\cdot) \text{ is nonzero whenever } p_d(\cdot) \text{ is nonzero} \\[11pt]
    & \text{(b) } \sup_{\theta} | J_T(\theta) - J(\theta) | \xrightarrow{P} 0 \\[11pt]
    & \text{(c) } I = \int \mathbf{g}(\mathbf{x}) \mathbf{g}(\mathbf{x})^T P(\mathbf{x}) p_d(\mathbf{x}) d\mathbf{x} \text{ has full rank}, \\[11pt]
    & \quad \quad \text{where } P(\mathbf{x}) = \frac{p_n(\mathbf{x})}{p_d(\mathbf{x}) + p_n(\mathbf{x})} \text{ and } \mathbf{g}(\mathbf{x}) = \nabla _{\theta} \log p_m(\mathbf{x} ; \theta) |_{\theta^{\star}}
    \end{aligned}
    $$
    <br/>
    <br/>
    + $\hat{\theta}_T$ that the value of $\theta$ which globally maximizes $J_T$ converges to $\theta^{\star}$ and leads to the correct estimate of $p_d(\cdot)$ as the sample size $T$ increases
  * **Theorem 3** (Asymptotic normality) $\sqrt{T}(\hat{\theta} - \theta^{\star})$ is asymptotically normal with mean zero and covariance matrix $\Sigma$
    <br/>
    <br/>
    $$
    \begin{aligned}
    \Sigma = I^{-1} - 2I^{-1} \left[ \int \mathbf{g}(\mathbf{x}) P(\mathbf{x}) p_d(\mathbf{x}) d\mathbf{x} \right] \left[ \int \mathbf{g}(\mathbf{x})^T P(\mathbf{x}) p_d(\mathbf{x}) d\mathbf{x} \right] I^{-1}
    \end{aligned}
    $$
    <br/>
    <br/>
    + This describes the distribution of the estimation error $(\hat{\theta} - \theta^{\star})$ for large sample sizes
  * **Corollary 1** For large sample size $T$, the mean squared error $\mathbb{E} \lVert \hat{\theta} - \theta^{\star} \rVert ^2$ behaves like $\text{tr}(\Sigma) / T$




# Choice of the Noise Distribution
  * In practice, we'd like to have a noise distribution which fulfills the following:
    1. It is easy to sample from, since the method relies on a set of samples $Y$ from the noise distribution
    2. It allows for an analytical expression for the log-pdf, so that we can evaluate the objective function without any problems
    3. It leads to a small jmean squared error $\mathbb{E} \lVert \hat{\theta} - \theta^{\star} \rVert ^2$
  * Noise distribution sould be close to the data distribution, because otherwise, the classification problem might be too easy and would not require the system to learn much about the structure of the data




# Simulations with Artificial Data
  * Data $\mathbf{x} \in \mathbb{R}^4$ is generated via the ICA model
    <br/>
    <br/>
    $$
    \begin{aligned}
    & \mathbf{s} \sim Laplace (0,1) \\[11pt]
    & A = (\mathbf{a}_1, \cdots, \mathbf{a}_4) \qquad (4 \times 4) \text{ mixing matrix} \\[11pt]
    & \log p_d(\mathbf{x}) = - \sum_{i=1}^4 \sqrt{2} |\mathbf{b}_i^{\star} \mathbf{x}| + (\log |\text{det}(B^{\star})| - \log 4) \\[11pt]
    & B^{\star} = A^{-1} \\[11pt]
    & \mathbf{b}_i^{\star}: i \text{-th row of } B^{\star} \\[11pt]
    & c^{\star} = \log |\text{det}(B^{\star})| - \log 4
    \end{aligned}
    $$
    <br/>
    <br/>
  * Model
    <br/>
    <br/>
    $$
    \begin{aligned}
    & \log p_m(\mathbf{x} ; \theta) = \log p_m^0(\mathbf{x} ; \alpha) + c \\[11pt]
    & \log p_m^0 (\mathbf{x} ; \alpha) = - \sum_{i=1}^4 \sqrt{2} |\mathbf{b}_i^{\star} \mathbf{x}| \\[11pt]
    & \text{Total set of parameters } \theta = \lbrace \alpha \in \mathbb{R}^{16}, c \rbrace
    \end{aligned}
    $$
    <br/>
    <br/>
  * Estimation method
    + Noise: Gaussian with the same mean and covariance matrix as $\mathbf{x}$
    + Using conjugate gradient algorithm
  * Results
<img src = "/assets/images/NCE_files/Fig_1.PNG">



# Further Study
  * Hinton, 2002, **Training products of experts by minimizing contrastive divergence**, *Neural Computation*. [pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.730&rep=rep1&type=pdf)
  * Teh et al., 2004, **Energy-based models for sparse overcomplete representations**, *Journal of Machine Learning Research*. [pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8380&rep=rep1&type=pdf)



