---
layout: article
title: ALOCC
mathjax: true
toc : true
tags : NoveltyDetection
---


# Tutorial
## [How to do Novelty Detection in Keras with Generative Adversarial Network](https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network-part-2/) | DLology

This notebook is for test phase Novelty Detection. To Train the model, run this first.
```bash
python models.py
```

It is recommended to understand how the model works in general before continuing the implementation. 

→ [How to do Novelty Detection in Keras with Generative Adversarial Network (Part 1)](https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network/)



```python
from utils import *
from kh_tools import *
import models
import imp
from keras.utils.vis_utils import plot_model
imp.reload(models)

from keras.datasets import mnist
from keras.losses import binary_crossentropy
from keras import backend as K

import numpy as np

import matplotlib.pyplot as plt
%matplotlib inline

(X_train, y_train), (_, _) = mnist.load_data()
X_train = X_train / 255
```

    Using TensorFlow backend.



```python
from models import ALOCC_Model
self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28)
#build alocc model
```

    
    generator
    Model: "R"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    z (InputLayer)               (None, 28, 28, 1)         0         
    _________________________________________________________________
    g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
    _________________________________________________________________
    batch_normalization_10 (Batc (None, 14, 14, 32)        128       
    _________________________________________________________________
    leaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 32)        0         
    _________________________________________________________________
    g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
    _________________________________________________________________
    batch_normalization_11 (Batc (None, 7, 7, 64)          256       
    _________________________________________________________________
    leaky_re_lu_13 (LeakyReLU)   (None, 7, 7, 64)          0         
    _________________________________________________________________
    g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
    _________________________________________________________________
    batch_normalization_12 (Batc (None, 4, 4, 128)         512       
    _________________________________________________________________
    leaky_re_lu_14 (LeakyReLU)   (None, 4, 4, 128)         0         
    _________________________________________________________________
    conv2d_5 (Conv2D)            (None, 4, 4, 16)          51216     
    _________________________________________________________________
    up_sampling2d_4 (UpSampling2 (None, 8, 8, 16)          0         
    _________________________________________________________________
    conv2d_6 (Conv2D)            (None, 8, 8, 16)          6416      
    _________________________________________________________________
    up_sampling2d_5 (UpSampling2 (None, 16, 16, 16)        0         
    _________________________________________________________________
    conv2d_7 (Conv2D)            (None, 14, 14, 32)        4640      
    _________________________________________________________________
    up_sampling2d_6 (UpSampling2 (None, 28, 28, 32)        0         
    _________________________________________________________________
    conv2d_8 (Conv2D)            (None, 28, 28, 1)         801       
    =================================================================
    Total params: 320,993
    Trainable params: 320,545
    Non-trainable params: 448
    _________________________________________________________________
    
    discriminator
    Model: "D"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    d_input (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
    _________________________________________________________________
    leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 16)        0         
    _________________________________________________________________
    d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
    _________________________________________________________________
    batch_normalization_7 (Batch (None, 7, 7, 32)          128       
    _________________________________________________________________
    leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 32)          0         
    _________________________________________________________________
    d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
    _________________________________________________________________
    batch_normalization_8 (Batch (None, 4, 4, 64)          256       
    _________________________________________________________________
    leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 64)          0         
    _________________________________________________________________
    d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
    _________________________________________________________________
    batch_normalization_9 (Batch (None, 2, 2, 128)         512       
    _________________________________________________________________
    leaky_re_lu_11 (LeakyReLU)   (None, 2, 2, 128)         0         
    _________________________________________________________________
    flatten_2 (Flatten)          (None, 512)               0         
    _________________________________________________________________
    d_h3_lin (Dense)             (None, 1)                 513       
    =================================================================
    Total params: 541,250
    Trainable params: 270,401
    Non-trainable params: 270,849
    _________________________________________________________________
    
    adversarial_model
    Model: "model_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_2 (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    R (Model)                    (None, 28, 28, 1)         320993    
    _________________________________________________________________
    D (Model)                    (None, 1)                 270849    
    =================================================================
    Total params: 591,842
    Trainable params: 320,545
    Non-trainable params: 271,297
    _________________________________________________________________


## Training 


```python
self.train(epochs=5, batch_size=128, sample_interval=500)
```

    Epoch (0/5)-------------------------------------------------


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'


    Epoch:[0]-[0/52] --> d_loss_real: 1.568, d_loss_fake: 15.802, g_loss:0.140, g_recon_loss:0.560
    Epoch:[0]-[1/52] --> d_loss_real: 0.003, d_loss_fake: 0.289, g_loss:14.704, g_recon_loss:4.606
    Epoch:[0]-[2/52] --> d_loss_real: 2.517, d_loss_fake: 0.004, g_loss:1.125, g_recon_loss:3.327
    Epoch:[0]-[3/52] --> d_loss_real: 0.000, d_loss_fake: 0.040, g_loss:5.021, g_recon_loss:2.537


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'


    Epoch:[0]-[4/52] --> d_loss_real: 0.000, d_loss_fake: 1.368, g_loss:10.722, g_recon_loss:1.165
    Epoch:[0]-[5/52] --> d_loss_real: 0.000, d_loss_fake: 3.883, g_loss:0.332, g_recon_loss:0.501
    Epoch:[0]-[6/52] --> d_loss_real: 0.005, d_loss_fake: 2.541, g_loss:2.681, g_recon_loss:1.133
    Epoch:[0]-[7/52] --> d_loss_real: 4.097, d_loss_fake: 2.061, g_loss:1.831, g_recon_loss:0.559
    Epoch:[0]-[8/52] --> d_loss_real: 0.246, d_loss_fake: 1.715, g_loss:3.590, g_recon_loss:0.496
    Epoch:[0]-[9/52] --> d_loss_real: 3.208, d_loss_fake: 0.835, g_loss:1.735, g_recon_loss:0.363
    Epoch:[0]-[10/52] --> d_loss_real: 1.503, d_loss_fake: 0.857, g_loss:1.453, g_recon_loss:0.385
    Epoch:[0]-[11/52] --> d_loss_real: 1.292, d_loss_fake: 0.790, g_loss:1.403, g_recon_loss:0.438
    Epoch:[0]-[12/52] --> d_loss_real: 0.958, d_loss_fake: 0.600, g_loss:1.202, g_recon_loss:0.484
    Epoch:[0]-[13/52] --> d_loss_real: 0.562, d_loss_fake: 0.416, g_loss:1.478, g_recon_loss:0.567
    Epoch:[0]-[14/52] --> d_loss_real: 0.396, d_loss_fake: 0.565, g_loss:1.847, g_recon_loss:0.345
    Epoch:[0]-[15/52] --> d_loss_real: 1.056, d_loss_fake: 0.523, g_loss:0.148, g_recon_loss:0.480
    Epoch:[0]-[16/52] --> d_loss_real: 0.061, d_loss_fake: 1.098, g_loss:3.442, g_recon_loss:0.356
    Epoch:[0]-[17/52] --> d_loss_real: 3.898, d_loss_fake: 0.921, g_loss:0.794, g_recon_loss:0.328
    Epoch:[0]-[18/52] --> d_loss_real: 0.732, d_loss_fake: 1.429, g_loss:1.608, g_recon_loss:0.282
    Epoch:[0]-[19/52] --> d_loss_real: 1.315, d_loss_fake: 0.591, g_loss:0.848, g_recon_loss:0.305
    Epoch:[0]-[20/52] --> d_loss_real: 0.672, d_loss_fake: 0.728, g_loss:0.950, g_recon_loss:0.268
    Epoch:[0]-[21/52] --> d_loss_real: 0.845, d_loss_fake: 0.930, g_loss:1.257, g_recon_loss:0.362
    Epoch:[0]-[22/52] --> d_loss_real: 0.944, d_loss_fake: 0.565, g_loss:0.750, g_recon_loss:0.431
    Epoch:[0]-[23/52] --> d_loss_real: 0.349, d_loss_fake: 0.072, g_loss:0.113, g_recon_loss:0.469
    Epoch:[0]-[24/52] --> d_loss_real: 0.009, d_loss_fake: 0.034, g_loss:0.079, g_recon_loss:0.330
    Epoch:[0]-[25/52] --> d_loss_real: 0.016, d_loss_fake: 0.054, g_loss:0.072, g_recon_loss:0.241
    Epoch:[0]-[26/52] --> d_loss_real: 0.039, d_loss_fake: 0.018, g_loss:0.058, g_recon_loss:0.268
    Epoch:[0]-[27/52] --> d_loss_real: 0.005, d_loss_fake: 0.005, g_loss:0.051, g_recon_loss:0.242
    Epoch:[0]-[28/52] --> d_loss_real: 0.013, d_loss_fake: 0.009, g_loss:0.046, g_recon_loss:0.218
    Epoch:[0]-[29/52] --> d_loss_real: 0.005, d_loss_fake: 0.002, g_loss:0.040, g_recon_loss:0.193
    Epoch:[0]-[30/52] --> d_loss_real: 0.004, d_loss_fake: 0.002, g_loss:0.038, g_recon_loss:0.186
    Epoch:[0]-[31/52] --> d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.034, g_recon_loss:0.163
    Epoch:[0]-[32/52] --> d_loss_real: 0.002, d_loss_fake: 0.001, g_loss:0.036, g_recon_loss:0.177
    Epoch:[0]-[33/52] --> d_loss_real: 0.003, d_loss_fake: 0.001, g_loss:0.033, g_recon_loss:0.160
    Epoch:[0]-[34/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.133
    Epoch:[0]-[35/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.027, g_recon_loss:0.132
    Epoch:[0]-[36/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.028, g_recon_loss:0.138
    Epoch:[0]-[37/52] --> d_loss_real: 0.002, d_loss_fake: 0.000, g_loss:0.023, g_recon_loss:0.115
    Epoch:[0]-[38/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.019, g_recon_loss:0.096
    Epoch:[0]-[39/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.021, g_recon_loss:0.105
    Epoch:[0]-[40/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.020, g_recon_loss:0.100
    Epoch:[0]-[41/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087
    Epoch:[0]-[42/52] --> d_loss_real: 0.001, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.087
    Epoch:[0]-[43/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.087
    Epoch:[0]-[44/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.084
    Epoch:[0]-[45/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.082
    Epoch:[0]-[46/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.017, g_recon_loss:0.083
    Epoch:[0]-[47/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.074
    Epoch:[0]-[48/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078
    Epoch:[0]-[49/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.078
    Epoch:[0]-[50/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.014, g_recon_loss:0.069
    Epoch:[0]-[51/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.018, g_recon_loss:0.090
    Epoch (1/5)-------------------------------------------------
    Epoch:[1]-[0/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.024, g_recon_loss:0.117
    Epoch:[1]-[1/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.082
    Epoch:[1]-[2/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073
    Epoch:[1]-[3/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.015, g_recon_loss:0.073
    Epoch:[1]-[4/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.016, g_recon_loss:0.080
    Epoch:[1]-[5/52] --> d_loss_real: 0.000, d_loss_fake: 0.000, g_loss:0.029, g_recon_loss:0.127
    Epoch:[1]-[6/52] --> d_loss_real: 0.000, d_loss_fake: 0.004, g_loss:1.339, g_recon_loss:1.790
    Epoch:[1]-[7/52] --> d_loss_real: 1.568, d_loss_fake: 8.227, g_loss:0.794, g_recon_loss:0.825
    Epoch:[1]-[8/52] --> d_loss_real: 0.275, d_loss_fake: 2.447, g_loss:1.562, g_recon_loss:0.372
    Epoch:[1]-[9/52] --> d_loss_real: 1.099, d_loss_fake: 1.998, g_loss:1.119, g_recon_loss:0.339
    Epoch:[1]-[10/52] --> d_loss_real: 1.060, d_loss_fake: 1.677, g_loss:1.042, g_recon_loss:0.434
    Epoch:[1]-[11/52] --> d_loss_real: 1.158, d_loss_fake: 1.530, g_loss:1.140, g_recon_loss:0.437
    Epoch:[1]-[12/52] --> d_loss_real: 1.099, d_loss_fake: 1.001, g_loss:1.057, g_recon_loss:0.348
    Epoch:[1]-[13/52] --> d_loss_real: 1.024, d_loss_fake: 1.001, g_loss:1.008, g_recon_loss:0.305
    Epoch:[1]-[14/52] --> d_loss_real: 1.029, d_loss_fake: 0.949, g_loss:1.013, g_recon_loss:0.362
    Epoch:[1]-[15/52] --> d_loss_real: 0.975, d_loss_fake: 1.022, g_loss:0.967, g_recon_loss:0.292
    Epoch:[1]-[16/52] --> d_loss_real: 1.007, d_loss_fake: 0.976, g_loss:0.887, g_recon_loss:0.373
    Epoch:[1]-[17/52] --> d_loss_real: 1.004, d_loss_fake: 0.983, g_loss:1.300, g_recon_loss:1.357
    Epoch:[1]-[18/52] --> d_loss_real: 0.853, d_loss_fake: 1.114, g_loss:1.058, g_recon_loss:0.367
    Epoch:[1]-[19/52] --> d_loss_real: 0.947, d_loss_fake: 0.921, g_loss:1.021, g_recon_loss:0.264
    Epoch:[1]-[20/52] --> d_loss_real: 0.949, d_loss_fake: 0.949, g_loss:0.969, g_recon_loss:0.214
    Epoch:[1]-[21/52] --> d_loss_real: 0.934, d_loss_fake: 0.953, g_loss:0.944, g_recon_loss:0.207
    Epoch:[1]-[22/52] --> d_loss_real: 0.928, d_loss_fake: 0.922, g_loss:0.934, g_recon_loss:0.180
    Epoch:[1]-[23/52] --> d_loss_real: 0.934, d_loss_fake: 0.894, g_loss:0.935, g_recon_loss:0.226
    Epoch:[1]-[24/52] --> d_loss_real: 0.925, d_loss_fake: 0.892, g_loss:0.940, g_recon_loss:0.245
    Epoch:[1]-[25/52] --> d_loss_real: 0.906, d_loss_fake: 0.907, g_loss:0.964, g_recon_loss:0.223
    Epoch:[1]-[26/52] --> d_loss_real: 0.907, d_loss_fake: 0.908, g_loss:0.935, g_recon_loss:0.171
    Epoch:[1]-[27/52] --> d_loss_real: 0.898, d_loss_fake: 0.880, g_loss:0.950, g_recon_loss:0.185
    Epoch:[1]-[28/52] --> d_loss_real: 0.871, d_loss_fake: 0.872, g_loss:0.951, g_recon_loss:0.172
    Epoch:[1]-[29/52] --> d_loss_real: 0.887, d_loss_fake: 0.929, g_loss:0.964, g_recon_loss:0.205
    Epoch:[1]-[30/52] --> d_loss_real: 0.938, d_loss_fake: 0.907, g_loss:0.946, g_recon_loss:0.193
    Epoch:[1]-[31/52] --> d_loss_real: 0.940, d_loss_fake: 0.887, g_loss:1.039, g_recon_loss:0.439
    Epoch:[1]-[32/52] --> d_loss_real: 0.984, d_loss_fake: 0.947, g_loss:1.002, g_recon_loss:0.168
    Epoch:[1]-[33/52] --> d_loss_real: 0.984, d_loss_fake: 0.883, g_loss:0.949, g_recon_loss:0.197
    Epoch:[1]-[34/52] --> d_loss_real: 0.899, d_loss_fake: 0.906, g_loss:0.908, g_recon_loss:0.131
    Epoch:[1]-[35/52] --> d_loss_real: 0.862, d_loss_fake: 0.887, g_loss:0.911, g_recon_loss:0.136
    Epoch:[1]-[36/52] --> d_loss_real: 0.864, d_loss_fake: 0.882, g_loss:0.921, g_recon_loss:0.125
    Epoch:[1]-[37/52] --> d_loss_real: 0.873, d_loss_fake: 0.873, g_loss:0.872, g_recon_loss:0.160
    Epoch:[1]-[38/52] --> d_loss_real: 0.881, d_loss_fake: 0.878, g_loss:0.990, g_recon_loss:0.303
    Epoch:[1]-[39/52] --> d_loss_real: 0.877, d_loss_fake: 1.022, g_loss:0.993, g_recon_loss:0.335
    Epoch:[1]-[40/52] --> d_loss_real: 0.919, d_loss_fake: 0.852, g_loss:0.965, g_recon_loss:0.190
    Epoch:[1]-[41/52] --> d_loss_real: 0.897, d_loss_fake: 0.827, g_loss:0.966, g_recon_loss:0.210
    Epoch:[1]-[42/52] --> d_loss_real: 0.892, d_loss_fake: 0.837, g_loss:0.939, g_recon_loss:0.226
    Epoch:[1]-[43/52] --> d_loss_real: 0.880, d_loss_fake: 0.793, g_loss:1.035, g_recon_loss:0.405
    Epoch:[1]-[44/52] --> d_loss_real: 0.866, d_loss_fake: 0.927, g_loss:0.969, g_recon_loss:0.387
    Epoch:[1]-[45/52] --> d_loss_real: 0.709, d_loss_fake: 0.952, g_loss:0.959, g_recon_loss:0.199
    Epoch:[1]-[46/52] --> d_loss_real: 0.776, d_loss_fake: 1.326, g_loss:0.999, g_recon_loss:0.159
    Epoch:[1]-[47/52] --> d_loss_real: 0.945, d_loss_fake: 0.751, g_loss:0.923, g_recon_loss:0.218
    Epoch:[1]-[48/52] --> d_loss_real: 0.871, d_loss_fake: 1.031, g_loss:0.897, g_recon_loss:0.191
    Epoch:[1]-[49/52] --> d_loss_real: 0.933, d_loss_fake: 0.926, g_loss:0.928, g_recon_loss:0.198
    Epoch:[1]-[50/52] --> d_loss_real: 0.938, d_loss_fake: 0.875, g_loss:0.951, g_recon_loss:0.208
    Epoch:[1]-[51/52] --> d_loss_real: 0.900, d_loss_fake: 0.853, g_loss:0.919, g_recon_loss:0.160
    Epoch (2/5)-------------------------------------------------
    Epoch:[2]-[0/52] --> d_loss_real: 0.924, d_loss_fake: 0.857, g_loss:0.924, g_recon_loss:0.155
    Epoch:[2]-[1/52] --> d_loss_real: 0.920, d_loss_fake: 0.840, g_loss:0.927, g_recon_loss:0.180
    Epoch:[2]-[2/52] --> d_loss_real: 0.910, d_loss_fake: 0.840, g_loss:0.915, g_recon_loss:0.157
    Epoch:[2]-[3/52] --> d_loss_real: 0.885, d_loss_fake: 0.879, g_loss:0.885, g_recon_loss:0.147
    Epoch:[2]-[4/52] --> d_loss_real: 0.855, d_loss_fake: 0.847, g_loss:0.906, g_recon_loss:0.202
    Epoch:[2]-[5/52] --> d_loss_real: 0.842, d_loss_fake: 0.860, g_loss:0.884, g_recon_loss:0.131
    Epoch:[2]-[6/52] --> d_loss_real: 0.827, d_loss_fake: 0.892, g_loss:0.901, g_recon_loss:0.169
    Epoch:[2]-[7/52] --> d_loss_real: 0.863, d_loss_fake: 0.852, g_loss:0.913, g_recon_loss:0.238
    Epoch:[2]-[8/52] --> d_loss_real: 0.887, d_loss_fake: 0.839, g_loss:0.925, g_recon_loss:0.258
    Epoch:[2]-[9/52] --> d_loss_real: 0.941, d_loss_fake: 0.850, g_loss:0.920, g_recon_loss:0.170
    Epoch:[2]-[10/52] --> d_loss_real: 0.893, d_loss_fake: 0.905, g_loss:0.853, g_recon_loss:0.145
    Epoch:[2]-[11/52] --> d_loss_real: 0.832, d_loss_fake: 0.898, g_loss:0.900, g_recon_loss:0.292
    Epoch:[2]-[12/52] --> d_loss_real: 0.828, d_loss_fake: 0.923, g_loss:0.862, g_recon_loss:0.147
    Epoch:[2]-[13/52] --> d_loss_real: 0.863, d_loss_fake: 0.868, g_loss:0.871, g_recon_loss:0.157
    Epoch:[2]-[14/52] --> d_loss_real: 0.864, d_loss_fake: 0.864, g_loss:0.877, g_recon_loss:0.129
    Epoch:[2]-[15/52] --> d_loss_real: 0.860, d_loss_fake: 0.875, g_loss:0.885, g_recon_loss:0.112
    Epoch:[2]-[16/52] --> d_loss_real: 0.885, d_loss_fake: 0.846, g_loss:0.882, g_recon_loss:0.164
    Epoch:[2]-[17/52] --> d_loss_real: 0.892, d_loss_fake: 0.833, g_loss:0.906, g_recon_loss:0.161
    Epoch:[2]-[18/52] --> d_loss_real: 0.915, d_loss_fake: 0.832, g_loss:0.904, g_recon_loss:0.195
    Epoch:[2]-[19/52] --> d_loss_real: 0.881, d_loss_fake: 0.915, g_loss:0.884, g_recon_loss:0.110
    Epoch:[2]-[20/52] --> d_loss_real: 0.855, d_loss_fake: 0.878, g_loss:0.862, g_recon_loss:0.102
    Epoch:[2]-[21/52] --> d_loss_real: 0.848, d_loss_fake: 0.856, g_loss:0.874, g_recon_loss:0.130
    Epoch:[2]-[22/52] --> d_loss_real: 0.850, d_loss_fake: 0.860, g_loss:0.860, g_recon_loss:0.106
    Epoch:[2]-[23/52] --> d_loss_real: 0.877, d_loss_fake: 0.843, g_loss:0.872, g_recon_loss:0.139
    Epoch:[2]-[24/52] --> d_loss_real: 0.862, d_loss_fake: 0.908, g_loss:0.881, g_recon_loss:0.156
    Epoch:[2]-[25/52] --> d_loss_real: 0.860, d_loss_fake: 0.876, g_loss:0.882, g_recon_loss:0.100
    Epoch:[2]-[26/52] --> d_loss_real: 0.865, d_loss_fake: 0.852, g_loss:0.879, g_recon_loss:0.112
    Epoch:[2]-[27/52] --> d_loss_real: 0.868, d_loss_fake: 0.840, g_loss:0.871, g_recon_loss:0.114
    Epoch:[2]-[28/52] --> d_loss_real: 0.853, d_loss_fake: 0.857, g_loss:0.874, g_recon_loss:0.114
    Epoch:[2]-[29/52] --> d_loss_real: 0.859, d_loss_fake: 0.839, g_loss:0.867, g_recon_loss:0.112
    Epoch:[2]-[30/52] --> d_loss_real: 0.858, d_loss_fake: 0.818, g_loss:0.843, g_recon_loss:0.200
    Epoch:[2]-[31/52] --> d_loss_real: 0.837, d_loss_fake: 0.901, g_loss:0.856, g_recon_loss:0.086
    Epoch:[2]-[32/52] --> d_loss_real: 0.852, d_loss_fake: 0.867, g_loss:0.855, g_recon_loss:0.100
    Epoch:[2]-[33/52] --> d_loss_real: 0.836, d_loss_fake: 0.878, g_loss:0.856, g_recon_loss:0.104
    Epoch:[2]-[34/52] --> d_loss_real: 0.841, d_loss_fake: 0.859, g_loss:0.853, g_recon_loss:0.081
    Epoch:[2]-[35/52] --> d_loss_real: 0.844, d_loss_fake: 0.835, g_loss:0.853, g_recon_loss:0.103
    Epoch:[2]-[36/52] --> d_loss_real: 0.842, d_loss_fake: 0.868, g_loss:0.861, g_recon_loss:0.097
    Epoch:[2]-[37/52] --> d_loss_real: 0.848, d_loss_fake: 0.827, g_loss:0.852, g_recon_loss:0.104
    Epoch:[2]-[38/52] --> d_loss_real: 0.850, d_loss_fake: 0.833, g_loss:0.902, g_recon_loss:0.216
    Epoch:[2]-[39/52] --> d_loss_real: 0.918, d_loss_fake: 0.835, g_loss:0.892, g_recon_loss:0.144
    Epoch:[2]-[40/52] --> d_loss_real: 0.877, d_loss_fake: 0.835, g_loss:0.832, g_recon_loss:0.140
    Epoch:[2]-[41/52] --> d_loss_real: 0.804, d_loss_fake: 0.893, g_loss:0.826, g_recon_loss:0.074
    Epoch:[2]-[42/52] --> d_loss_real: 0.809, d_loss_fake: 0.866, g_loss:0.822, g_recon_loss:0.088
    Epoch:[2]-[43/52] --> d_loss_real: 0.807, d_loss_fake: 0.846, g_loss:0.821, g_recon_loss:0.095
    Epoch:[2]-[44/52] --> d_loss_real: 0.812, d_loss_fake: 0.850, g_loss:0.841, g_recon_loss:0.074
    Epoch:[2]-[45/52] --> d_loss_real: 0.824, d_loss_fake: 0.822, g_loss:0.839, g_recon_loss:0.292
    Epoch:[2]-[46/52] --> d_loss_real: 0.841, d_loss_fake: 0.902, g_loss:0.872, g_recon_loss:0.107
    Epoch:[2]-[47/52] --> d_loss_real: 0.839, d_loss_fake: 0.857, g_loss:0.859, g_recon_loss:0.088
    Epoch:[2]-[48/52] --> d_loss_real: 0.833, d_loss_fake: 0.827, g_loss:0.841, g_recon_loss:0.091
    Epoch:[2]-[49/52] --> d_loss_real: 0.821, d_loss_fake: 0.818, g_loss:0.834, g_recon_loss:0.089
    Epoch:[2]-[50/52] --> d_loss_real: 0.819, d_loss_fake: 0.829, g_loss:0.832, g_recon_loss:0.102
    Epoch:[2]-[51/52] --> d_loss_real: 0.835, d_loss_fake: 0.840, g_loss:0.808, g_recon_loss:0.092
    Epoch (3/5)-------------------------------------------------
    Epoch:[3]-[0/52] --> d_loss_real: 0.804, d_loss_fake: 0.869, g_loss:0.826, g_recon_loss:0.089
    Epoch:[3]-[1/52] --> d_loss_real: 0.815, d_loss_fake: 0.856, g_loss:0.849, g_recon_loss:0.087
    Epoch:[3]-[2/52] --> d_loss_real: 0.835, d_loss_fake: 0.820, g_loss:0.833, g_recon_loss:0.113
    Epoch:[3]-[3/52] --> d_loss_real: 0.831, d_loss_fake: 0.856, g_loss:0.872, g_recon_loss:0.086
    Epoch:[3]-[4/52] --> d_loss_real: 0.847, d_loss_fake: 0.816, g_loss:0.829, g_recon_loss:0.129
    Epoch:[3]-[5/52] --> d_loss_real: 0.817, d_loss_fake: 0.863, g_loss:0.826, g_recon_loss:0.088
    Epoch:[3]-[6/52] --> d_loss_real: 0.809, d_loss_fake: 0.792, g_loss:0.796, g_recon_loss:0.127
    Epoch:[3]-[7/52] --> d_loss_real: 0.782, d_loss_fake: 0.891, g_loss:0.850, g_recon_loss:0.087
    Epoch:[3]-[8/52] --> d_loss_real: 0.810, d_loss_fake: 0.841, g_loss:0.792, g_recon_loss:0.079
    Epoch:[3]-[9/52] --> d_loss_real: 0.788, d_loss_fake: 0.872, g_loss:0.814, g_recon_loss:0.092
    Epoch:[3]-[10/52] --> d_loss_real: 0.798, d_loss_fake: 0.854, g_loss:0.830, g_recon_loss:0.090
    Epoch:[3]-[11/52] --> d_loss_real: 0.822, d_loss_fake: 0.840, g_loss:0.817, g_recon_loss:0.122
    Epoch:[3]-[12/52] --> d_loss_real: 0.796, d_loss_fake: 0.838, g_loss:0.814, g_recon_loss:0.096
    Epoch:[3]-[13/52] --> d_loss_real: 0.787, d_loss_fake: 0.861, g_loss:0.836, g_recon_loss:0.075
    Epoch:[3]-[14/52] --> d_loss_real: 0.815, d_loss_fake: 0.804, g_loss:0.828, g_recon_loss:0.207
    Epoch:[3]-[15/52] --> d_loss_real: 0.880, d_loss_fake: 0.782, g_loss:0.939, g_recon_loss:0.113
    Epoch:[3]-[16/52] --> d_loss_real: 0.885, d_loss_fake: 0.762, g_loss:0.888, g_recon_loss:0.113
    Epoch:[3]-[17/52] --> d_loss_real: 0.833, d_loss_fake: 0.776, g_loss:0.828, g_recon_loss:0.093
    Epoch:[3]-[18/52] --> d_loss_real: 0.791, d_loss_fake: 0.902, g_loss:0.707, g_recon_loss:0.112
    Epoch:[3]-[19/52] --> d_loss_real: 0.714, d_loss_fake: 0.940, g_loss:0.769, g_recon_loss:0.090
    Epoch:[3]-[20/52] --> d_loss_real: 0.755, d_loss_fake: 0.927, g_loss:0.844, g_recon_loss:0.091
    Epoch:[3]-[21/52] --> d_loss_real: 0.823, d_loss_fake: 0.844, g_loss:0.801, g_recon_loss:0.086
    Epoch:[3]-[22/52] --> d_loss_real: 0.793, d_loss_fake: 0.828, g_loss:0.780, g_recon_loss:0.095
    Epoch:[3]-[23/52] --> d_loss_real: 0.784, d_loss_fake: 0.850, g_loss:0.811, g_recon_loss:0.082
    Epoch:[3]-[24/52] --> d_loss_real: 0.782, d_loss_fake: 0.819, g_loss:0.780, g_recon_loss:0.079
    Epoch:[3]-[25/52] --> d_loss_real: 0.782, d_loss_fake: 0.865, g_loss:0.822, g_recon_loss:0.080
    Epoch:[3]-[26/52] --> d_loss_real: 0.811, d_loss_fake: 0.803, g_loss:0.791, g_recon_loss:0.092
    Epoch:[3]-[27/52] --> d_loss_real: 0.798, d_loss_fake: 0.849, g_loss:0.851, g_recon_loss:0.080
    Epoch:[3]-[28/52] --> d_loss_real: 0.818, d_loss_fake: 0.781, g_loss:0.804, g_recon_loss:0.105
    Epoch:[3]-[29/52] --> d_loss_real: 0.794, d_loss_fake: 0.822, g_loss:0.814, g_recon_loss:0.072
    Epoch:[3]-[30/52] --> d_loss_real: 0.799, d_loss_fake: 0.762, g_loss:0.765, g_recon_loss:0.144
    Epoch:[3]-[31/52] --> d_loss_real: 0.788, d_loss_fake: 0.838, g_loss:0.872, g_recon_loss:0.094
    Epoch:[3]-[32/52] --> d_loss_real: 0.856, d_loss_fake: 0.792, g_loss:0.830, g_recon_loss:0.087
    Epoch:[3]-[33/52] --> d_loss_real: 0.807, d_loss_fake: 0.795, g_loss:0.776, g_recon_loss:0.101
    Epoch:[3]-[34/52] --> d_loss_real: 0.749, d_loss_fake: 0.859, g_loss:0.779, g_recon_loss:0.084
    Epoch:[3]-[35/52] --> d_loss_real: 0.771, d_loss_fake: 0.891, g_loss:0.790, g_recon_loss:0.138
    Epoch:[3]-[36/52] --> d_loss_real: 0.766, d_loss_fake: 0.852, g_loss:0.811, g_recon_loss:0.078
    Epoch:[3]-[37/52] --> d_loss_real: 0.785, d_loss_fake: 0.809, g_loss:0.781, g_recon_loss:0.081
    Epoch:[3]-[38/52] --> d_loss_real: 0.760, d_loss_fake: 0.882, g_loss:0.816, g_recon_loss:0.078
    Epoch:[3]-[39/52] --> d_loss_real: 0.795, d_loss_fake: 0.781, g_loss:0.786, g_recon_loss:0.158
    Epoch:[3]-[40/52] --> d_loss_real: 0.890, d_loss_fake: 0.672, g_loss:0.965, g_recon_loss:0.113
    Epoch:[3]-[41/52] --> d_loss_real: 0.954, d_loss_fake: 0.734, g_loss:0.837, g_recon_loss:0.103
    Epoch:[3]-[42/52] --> d_loss_real: 0.834, d_loss_fake: 0.999, g_loss:0.643, g_recon_loss:0.145
    Epoch:[3]-[43/52] --> d_loss_real: 0.625, d_loss_fake: 1.018, g_loss:0.752, g_recon_loss:0.110
    Epoch:[3]-[44/52] --> d_loss_real: 0.716, d_loss_fake: 0.868, g_loss:0.782, g_recon_loss:0.093
    Epoch:[3]-[45/52] --> d_loss_real: 0.741, d_loss_fake: 0.839, g_loss:0.795, g_recon_loss:0.089
    Epoch:[3]-[46/52] --> d_loss_real: 0.740, d_loss_fake: 0.844, g_loss:0.778, g_recon_loss:0.088
    Epoch:[3]-[47/52] --> d_loss_real: 0.729, d_loss_fake: 1.006, g_loss:0.762, g_recon_loss:0.090
    Epoch:[3]-[48/52] --> d_loss_real: 0.755, d_loss_fake: 0.832, g_loss:0.789, g_recon_loss:0.092
    Epoch:[3]-[49/52] --> d_loss_real: 0.777, d_loss_fake: 0.767, g_loss:0.771, g_recon_loss:0.092
    Epoch:[3]-[50/52] --> d_loss_real: 0.800, d_loss_fake: 0.789, g_loss:0.863, g_recon_loss:0.097
    Epoch:[3]-[51/52] --> d_loss_real: 0.892, d_loss_fake: 0.745, g_loss:0.750, g_recon_loss:0.120
    Epoch (4/5)-------------------------------------------------
    Epoch:[4]-[0/52] --> d_loss_real: 0.739, d_loss_fake: 0.839, g_loss:0.773, g_recon_loss:0.093
    Epoch:[4]-[1/52] --> d_loss_real: 0.761, d_loss_fake: 0.824, g_loss:0.790, g_recon_loss:0.084
    Epoch:[4]-[2/52] --> d_loss_real: 0.766, d_loss_fake: 0.800, g_loss:0.793, g_recon_loss:0.083
    Epoch:[4]-[3/52] --> d_loss_real: 0.774, d_loss_fake: 0.665, g_loss:1.410, g_recon_loss:0.104
    Epoch:[4]-[4/52] --> d_loss_real: 2.101, d_loss_fake: 0.606, g_loss:1.045, g_recon_loss:0.180
    Epoch:[4]-[5/52] --> d_loss_real: 0.825, d_loss_fake: 0.564, g_loss:0.892, g_recon_loss:0.160
    Epoch:[4]-[6/52] --> d_loss_real: 0.592, d_loss_fake: 1.084, g_loss:0.857, g_recon_loss:0.229
    Epoch:[4]-[7/52] --> d_loss_real: 0.737, d_loss_fake: 0.689, g_loss:0.775, g_recon_loss:0.235
    Epoch:[4]-[8/52] --> d_loss_real: 0.469, d_loss_fake: 1.200, g_loss:0.965, g_recon_loss:0.221
    Epoch:[4]-[9/52] --> d_loss_real: 0.501, d_loss_fake: 1.126, g_loss:0.942, g_recon_loss:0.201
    Epoch:[4]-[10/52] --> d_loss_real: 0.740, d_loss_fake: 0.869, g_loss:0.778, g_recon_loss:0.154
    Epoch:[4]-[11/52] --> d_loss_real: 0.679, d_loss_fake: 0.880, g_loss:0.854, g_recon_loss:0.140
    Epoch:[4]-[12/52] --> d_loss_real: 0.712, d_loss_fake: 0.789, g_loss:0.703, g_recon_loss:0.195
    Epoch:[4]-[13/52] --> d_loss_real: 0.691, d_loss_fake: 1.092, g_loss:0.807, g_recon_loss:0.158
    Epoch:[4]-[14/52] --> d_loss_real: 0.744, d_loss_fake: 0.731, g_loss:0.771, g_recon_loss:0.192
    Epoch:[4]-[15/52] --> d_loss_real: 0.812, d_loss_fake: 0.771, g_loss:0.764, g_recon_loss:0.134
    Epoch:[4]-[16/52] --> d_loss_real: 0.702, d_loss_fake: 0.897, g_loss:0.876, g_recon_loss:0.195
    Epoch:[4]-[17/52] --> d_loss_real: 0.756, d_loss_fake: 0.773, g_loss:0.822, g_recon_loss:0.148
    Epoch:[4]-[18/52] --> d_loss_real: 0.769, d_loss_fake: 0.762, g_loss:0.799, g_recon_loss:0.162
    Epoch:[4]-[19/52] --> d_loss_real: 0.943, d_loss_fake: 0.843, g_loss:0.805, g_recon_loss:0.152
    Epoch:[4]-[20/52] --> d_loss_real: 0.734, d_loss_fake: 0.885, g_loss:0.776, g_recon_loss:0.134
    Epoch:[4]-[21/52] --> d_loss_real: 0.765, d_loss_fake: 0.863, g_loss:0.862, g_recon_loss:0.221
    Epoch:[4]-[22/52] --> d_loss_real: 0.802, d_loss_fake: 0.745, g_loss:0.798, g_recon_loss:0.128
    Epoch:[4]-[23/52] --> d_loss_real: 0.771, d_loss_fake: 0.704, g_loss:0.805, g_recon_loss:0.229
    Epoch:[4]-[24/52] --> d_loss_real: 0.863, d_loss_fake: 0.673, g_loss:0.860, g_recon_loss:0.113
    Epoch:[4]-[25/52] --> d_loss_real: 0.738, d_loss_fake: 0.793, g_loss:0.762, g_recon_loss:0.168
    Epoch:[4]-[26/52] --> d_loss_real: 0.775, d_loss_fake: 0.800, g_loss:0.858, g_recon_loss:0.172
    Epoch:[4]-[27/52] --> d_loss_real: 0.699, d_loss_fake: 0.835, g_loss:0.811, g_recon_loss:0.147
    Epoch:[4]-[28/52] --> d_loss_real: 0.828, d_loss_fake: 0.718, g_loss:0.834, g_recon_loss:0.127
    Epoch:[4]-[29/52] --> d_loss_real: 0.693, d_loss_fake: 0.818, g_loss:0.803, g_recon_loss:0.188
    Epoch:[4]-[30/52] --> d_loss_real: 0.747, d_loss_fake: 0.648, g_loss:0.846, g_recon_loss:0.134
    Epoch:[4]-[31/52] --> d_loss_real: 0.696, d_loss_fake: 0.839, g_loss:0.792, g_recon_loss:0.211
    Epoch:[4]-[32/52] --> d_loss_real: 0.661, d_loss_fake: 0.805, g_loss:0.815, g_recon_loss:0.354
    Epoch:[4]-[33/52] --> d_loss_real: 0.669, d_loss_fake: 0.975, g_loss:0.867, g_recon_loss:0.178
    Epoch:[4]-[34/52] --> d_loss_real: 0.744, d_loss_fake: 0.803, g_loss:0.828, g_recon_loss:0.205
    Epoch:[4]-[35/52] --> d_loss_real: 0.815, d_loss_fake: 0.627, g_loss:0.791, g_recon_loss:0.189
    Epoch:[4]-[36/52] --> d_loss_real: 0.980, d_loss_fake: 0.673, g_loss:0.817, g_recon_loss:0.139
    Epoch:[4]-[37/52] --> d_loss_real: 0.759, d_loss_fake: 0.788, g_loss:0.830, g_recon_loss:0.157
    Epoch:[4]-[38/52] --> d_loss_real: 0.649, d_loss_fake: 0.710, g_loss:0.764, g_recon_loss:0.106
    Epoch:[4]-[39/52] --> d_loss_real: 0.615, d_loss_fake: 0.859, g_loss:0.736, g_recon_loss:0.113
    Epoch:[4]-[40/52] --> d_loss_real: 0.663, d_loss_fake: 0.939, g_loss:0.754, g_recon_loss:0.124
    Epoch:[4]-[41/52] --> d_loss_real: 0.650, d_loss_fake: 1.127, g_loss:0.957, g_recon_loss:0.142
    Epoch:[4]-[42/52] --> d_loss_real: 0.772, d_loss_fake: 0.753, g_loss:0.806, g_recon_loss:0.191
    Epoch:[4]-[43/52] --> d_loss_real: 0.917, d_loss_fake: 0.609, g_loss:0.746, g_recon_loss:0.150
    Epoch:[4]-[44/52] --> d_loss_real: 0.943, d_loss_fake: 0.756, g_loss:0.784, g_recon_loss:0.188
    Epoch:[4]-[45/52] --> d_loss_real: 0.758, d_loss_fake: 0.768, g_loss:0.829, g_recon_loss:0.156
    Epoch:[4]-[46/52] --> d_loss_real: 0.701, d_loss_fake: 0.685, g_loss:0.820, g_recon_loss:0.136
    Epoch:[4]-[47/52] --> d_loss_real: 0.712, d_loss_fake: 0.610, g_loss:0.745, g_recon_loss:0.175
    Epoch:[4]-[48/52] --> d_loss_real: 0.717, d_loss_fake: 0.911, g_loss:0.785, g_recon_loss:0.155
    Epoch:[4]-[49/52] --> d_loss_real: 0.780, d_loss_fake: 0.824, g_loss:0.807, g_recon_loss:0.147
    Epoch:[4]-[50/52] --> d_loss_real: 0.691, d_loss_fake: 0.809, g_loss:0.871, g_recon_loss:0.140
    Epoch:[4]-[51/52] --> d_loss_real: 0.728, d_loss_fake: 0.734, g_loss:0.748, g_recon_loss:0.151



![png](/assets/images/alocc_files/alocc_4_5.png)


## Choose a stopping criterion
The training procedure is stopped when R successfully maps noisy images to clean images carrying the concept of the target class.  When R can reconstruct its input with minimum error. In the following case, we pick the epoch 3.


```python
# This image was generated at the end of the models.py training procedure to help pick a ending epoch to load. 
from IPython.display import Image
Image(filename='plot_g_recon_losses.png') 
# Load the epoch #3 saved weights.
self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5')
```

## Test the reconstruction loss and Discriminator output
The `abnormal` image has a **`larger` reconstruction loss** and **`smaller` discriminator output value**.


```python
def test_reconstruction(label, data_index = 11):
    specific_idx = np.where(y_train == label)[0]
    if data_index >= len(X_train):
        data_index = 0
    datas = X_train[specific_idx].reshape(-1, 28, 28, 1)[data_index:data_index+5]
    model_predicts = self.adversarial_model.predict(datas)
    input_images = [datas[i].reshape((28, 28)) for i in range(5)]
    reconstructed_images = [model_predicts[0][i].reshape((28, 28)) for i in range(5)]
    
    fig= plt.figure(figsize=(50, 20))
    columns = 5
    rows = 2
    for i in range(5):
        fig.add_subplot(rows, columns, i+1)
        plt.title('Input', fontsize = 20)
        plt.imshow(input_images[i], label='Input')
        fig.add_subplot(rows, columns, i+6)
        plt.title('Reconstruction', fontsize = 20)
        plt.imshow(reconstructed_images[i], label='Reconstructed')
    plt.show()
    
    
    # Compute the mean binary_crossentropy loss of reconstructed image.
    errors = list()
    for i in range(5):
        y_true = K.variable(reconstructed_images[i])
        y_pred = K.variable(input_images[i])
        errors.append(K.eval(binary_crossentropy(y_true, y_pred)).mean())
    print('Average reconstruction loss:', np.array(errors).mean())
    print('Average discriminator Output:', model_predicts[1].mean())
```




```python
def draw_histogram():
    data_list = [X_train[np.where(y_train == i+1)[0]].reshape(-1, 28, 28, 1) for i in range(9)]
    D_Outputs = [[self.adversarial_model.predict(n.reshape(-1,28,28,1))[1][0][0] for n in data_list[i]] for i in range(9)]
    
    plt.figure(figsize=(8,6))
    plt.hist(D_Outputs[0], bins=100, alpha=0.7, fc='none', lw=1.5, histtype='step', label="Inlier")
    outlier_list = [item for sublist in D_Outputs[1:] for item in sublist]
    plt.hist(outlier_list, bins=100, alpha=0.7, fc='none', lw=1.5, histtype='step', label="Outlier")

    plt.xlabel("Discriminator Output", size=14)
    plt.ylabel("Count", size=14)
    plt.title("Output multiple Histograms")
    plt.legend(loc='upper right')
    plt.savefig('paper_original_result.png')
```

### Normal case
The network was trained with label == 1.


```python
test_reconstruction(1)
```


![png](/assets/images/alocc_files/alocc_12_0.png)


    Average reconstruction loss: 0.13995047
    Average discriminator Output: 0.46022063


## Abnormal cases
The network was not trained on those labels, so the Generator/R network find it hard to reconstruct the input images reflected in higher reconstruction loss values.

Discriminator also outputs a lower value compared to normal ones.


```python
test_reconstruction(3)
```


![png](/assets/images/alocc_files/alocc_14_0.png)


    Average reconstruction loss: 1.0208124
    Average discriminator Output: 0.6488284



```python
test_reconstruction(5)
```


![png](/assets/images/alocc_files/alocc_15_0.png)


    Average reconstruction loss: 1.1040308
    Average discriminator Output: 0.6433686



```python
test_reconstruction(7)
```


![png](/assets/images/alocc_files/alocc_16_0.png)


    Average reconstruction loss: 0.51506275
    Average discriminator Output: 0.54887545



```python
draw_histogram()
```


![png](/assets/images/alocc_files/alocc_17_0.png)


### mobiis 추가 연구사항 
1. D 모델을 조금더 복잡하게 구성
2. R 모델 학습을 조금 더디게 진행
3. 노이즈 없이 학습

### 1. Enhance D network - Add 1 dense layer 


```python
import sys
import importlib

sys.path.insert(1, 'test/enhance_D/')
importlib.reload(models)

from models import ALOCC_Model
self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28)
```

    
    generator
    Model: "R"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    z (InputLayer)               (None, 28, 28, 1)         0         
    _________________________________________________________________
    g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
    _________________________________________________________________
    batch_normalization_70 (Batc (None, 14, 14, 32)        128       
    _________________________________________________________________
    leaky_re_lu_82 (LeakyReLU)   (None, 14, 14, 32)        0         
    _________________________________________________________________
    g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
    _________________________________________________________________
    batch_normalization_71 (Batc (None, 7, 7, 64)          256       
    _________________________________________________________________
    leaky_re_lu_83 (LeakyReLU)   (None, 7, 7, 64)          0         
    _________________________________________________________________
    g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
    _________________________________________________________________
    batch_normalization_72 (Batc (None, 4, 4, 128)         512       
    _________________________________________________________________
    leaky_re_lu_84 (LeakyReLU)   (None, 4, 4, 128)         0         
    _________________________________________________________________
    conv2d_45 (Conv2D)           (None, 4, 4, 16)          51216     
    _________________________________________________________________
    up_sampling2d_34 (UpSampling (None, 8, 8, 16)          0         
    _________________________________________________________________
    conv2d_46 (Conv2D)           (None, 8, 8, 16)          6416      
    _________________________________________________________________
    up_sampling2d_35 (UpSampling (None, 16, 16, 16)        0         
    _________________________________________________________________
    conv2d_47 (Conv2D)           (None, 14, 14, 32)        4640      
    _________________________________________________________________
    up_sampling2d_36 (UpSampling (None, 28, 28, 32)        0         
    _________________________________________________________________
    conv2d_48 (Conv2D)           (None, 28, 28, 1)         801       
    =================================================================
    Total params: 320,993
    Trainable params: 320,545
    Non-trainable params: 448
    _________________________________________________________________
    
    discriminator
    Model: "D"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    d_input (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
    _________________________________________________________________
    leaky_re_lu_78 (LeakyReLU)   (None, 14, 14, 16)        0         
    _________________________________________________________________
    d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
    _________________________________________________________________
    batch_normalization_67 (Batc (None, 7, 7, 32)          128       
    _________________________________________________________________
    leaky_re_lu_79 (LeakyReLU)   (None, 7, 7, 32)          0         
    _________________________________________________________________
    d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
    _________________________________________________________________
    batch_normalization_68 (Batc (None, 4, 4, 64)          256       
    _________________________________________________________________
    leaky_re_lu_80 (LeakyReLU)   (None, 4, 4, 64)          0         
    _________________________________________________________________
    d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
    _________________________________________________________________
    batch_normalization_69 (Batc (None, 2, 2, 128)         512       
    _________________________________________________________________
    leaky_re_lu_81 (LeakyReLU)   (None, 2, 2, 128)         0         
    _________________________________________________________________
    flatten_12 (Flatten)         (None, 512)               0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 32)                16416     
    _________________________________________________________________
    d_h3_lin (Dense)             (None, 1)                 33        
    =================================================================
    Total params: 573,122
    Trainable params: 286,337
    Non-trainable params: 286,785
    _________________________________________________________________
    
    adversarial_model
    Model: "model_12"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_12 (InputLayer)        (None, 28, 28, 1)         0         
    _________________________________________________________________
    R (Model)                    (None, 28, 28, 1)         320993    
    _________________________________________________________________
    D (Model)                    (None, 1)                 286785    
    =================================================================
    Total params: 607,778
    Trainable params: 320,545
    Non-trainable params: 287,233
    _________________________________________________________________


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'



```python
self.train(epochs=5, batch_size=128, sample_interval=500)
```

    Epoch (0/5)-------------------------------------------------


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'
    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'


    Epoch (1/5)-------------------------------------------------
    Epoch (2/5)-------------------------------------------------
    Epoch (3/5)-------------------------------------------------
    Epoch (4/5)-------------------------------------------------



![png](/assets/images/alocc_files/alocc_21_3.png)



```python
# Load the epoch #3 saved weights.
self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5')
```


```python
draw_histogram()
```


![png](/assets/images/alocc_files/alocc_23_0.png)



```python

```

### 2. Slower R network learning - 기존 1 batch당 두번 학습에서 한번으로 변경


```python
import sys
import importlib

sys.path.insert(1, 'test/slow_R/')
importlib.reload(models)

from models import ALOCC_Model
self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28)
```

    
    generator
    Model: "R"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    z (InputLayer)               (None, 28, 28, 1)         0         
    _________________________________________________________________
    g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
    _________________________________________________________________
    batch_normalization_58 (Batc (None, 14, 14, 32)        128       
    _________________________________________________________________
    leaky_re_lu_68 (LeakyReLU)   (None, 14, 14, 32)        0         
    _________________________________________________________________
    g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
    _________________________________________________________________
    batch_normalization_59 (Batc (None, 7, 7, 64)          256       
    _________________________________________________________________
    leaky_re_lu_69 (LeakyReLU)   (None, 7, 7, 64)          0         
    _________________________________________________________________
    g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
    _________________________________________________________________
    batch_normalization_60 (Batc (None, 4, 4, 128)         512       
    _________________________________________________________________
    leaky_re_lu_70 (LeakyReLU)   (None, 4, 4, 128)         0         
    _________________________________________________________________
    conv2d_37 (Conv2D)           (None, 4, 4, 16)          51216     
    _________________________________________________________________
    up_sampling2d_28 (UpSampling (None, 8, 8, 16)          0         
    _________________________________________________________________
    conv2d_38 (Conv2D)           (None, 8, 8, 16)          6416      
    _________________________________________________________________
    up_sampling2d_29 (UpSampling (None, 16, 16, 16)        0         
    _________________________________________________________________
    conv2d_39 (Conv2D)           (None, 14, 14, 32)        4640      
    _________________________________________________________________
    up_sampling2d_30 (UpSampling (None, 28, 28, 32)        0         
    _________________________________________________________________
    conv2d_40 (Conv2D)           (None, 28, 28, 1)         801       
    =================================================================
    Total params: 320,993
    Trainable params: 320,545
    Non-trainable params: 448
    _________________________________________________________________
    
    discriminator
    Model: "D"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    d_input (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
    _________________________________________________________________
    leaky_re_lu_64 (LeakyReLU)   (None, 14, 14, 16)        0         
    _________________________________________________________________
    d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
    _________________________________________________________________
    batch_normalization_55 (Batc (None, 7, 7, 32)          128       
    _________________________________________________________________
    leaky_re_lu_65 (LeakyReLU)   (None, 7, 7, 32)          0         
    _________________________________________________________________
    d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
    _________________________________________________________________
    batch_normalization_56 (Batc (None, 4, 4, 64)          256       
    _________________________________________________________________
    leaky_re_lu_66 (LeakyReLU)   (None, 4, 4, 64)          0         
    _________________________________________________________________
    d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
    _________________________________________________________________
    batch_normalization_57 (Batc (None, 2, 2, 128)         512       
    _________________________________________________________________
    leaky_re_lu_67 (LeakyReLU)   (None, 2, 2, 128)         0         
    _________________________________________________________________
    flatten_10 (Flatten)         (None, 512)               0         
    _________________________________________________________________
    d_h3_lin (Dense)             (None, 1)                 513       
    =================================================================
    Total params: 541,250
    Trainable params: 270,401
    Non-trainable params: 270,849
    _________________________________________________________________
    
    adversarial_model
    Model: "model_10"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_10 (InputLayer)        (None, 28, 28, 1)         0         
    _________________________________________________________________
    R (Model)                    (None, 28, 28, 1)         320993    
    _________________________________________________________________
    D (Model)                    (None, 1)                 270849    
    =================================================================
    Total params: 591,842
    Trainable params: 320,545
    Non-trainable params: 271,297
    _________________________________________________________________



```python
self.train(epochs=5, batch_size=128, sample_interval=500)
```

    Epoch (0/5)-------------------------------------------------


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'
    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'


    Epoch (1/5)-------------------------------------------------
    Epoch (2/5)-------------------------------------------------
    Epoch (3/5)-------------------------------------------------
    Epoch (4/5)-------------------------------------------------



![png](/assets/images/alocc_files/alocc_27_3.png)



```python
# Load the epoch #4 saved weights.
self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5')
```


```python
draw_histogram()
```


![png](/assets/images/alocc_files/alocc_29_0.png)



```python

```

### 3. Learning without noise


```python
import sys
import importlib

sys.path.insert(1, 'test/without_noise/')
importlib.reload(models)

from models import ALOCC_Model
self = ALOCC_Model(dataset_name='mnist', input_height=28,input_width=28)
```

    
    generator
    Model: "R"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    z (InputLayer)               (None, 28, 28, 1)         0         
    _________________________________________________________________
    g_encoder_h0_conv (Conv2D)   (None, 14, 14, 32)        832       
    _________________________________________________________________
    batch_normalization_34 (Batc (None, 14, 14, 32)        128       
    _________________________________________________________________
    leaky_re_lu_40 (LeakyReLU)   (None, 14, 14, 32)        0         
    _________________________________________________________________
    g_encoder_h1_conv (Conv2D)   (None, 7, 7, 64)          51264     
    _________________________________________________________________
    batch_normalization_35 (Batc (None, 7, 7, 64)          256       
    _________________________________________________________________
    leaky_re_lu_41 (LeakyReLU)   (None, 7, 7, 64)          0         
    _________________________________________________________________
    g_encoder_h2_conv (Conv2D)   (None, 4, 4, 128)         204928    
    _________________________________________________________________
    batch_normalization_36 (Batc (None, 4, 4, 128)         512       
    _________________________________________________________________
    leaky_re_lu_42 (LeakyReLU)   (None, 4, 4, 128)         0         
    _________________________________________________________________
    conv2d_21 (Conv2D)           (None, 4, 4, 16)          51216     
    _________________________________________________________________
    up_sampling2d_16 (UpSampling (None, 8, 8, 16)          0         
    _________________________________________________________________
    conv2d_22 (Conv2D)           (None, 8, 8, 16)          6416      
    _________________________________________________________________
    up_sampling2d_17 (UpSampling (None, 16, 16, 16)        0         
    _________________________________________________________________
    conv2d_23 (Conv2D)           (None, 14, 14, 32)        4640      
    _________________________________________________________________
    up_sampling2d_18 (UpSampling (None, 28, 28, 32)        0         
    _________________________________________________________________
    conv2d_24 (Conv2D)           (None, 28, 28, 1)         801       
    =================================================================
    Total params: 320,993
    Trainable params: 320,545
    Non-trainable params: 448
    _________________________________________________________________
    
    discriminator
    Model: "D"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    d_input (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    d_h0_conv (Conv2D)           (None, 14, 14, 16)        416       
    _________________________________________________________________
    leaky_re_lu_36 (LeakyReLU)   (None, 14, 14, 16)        0         
    _________________________________________________________________
    d_h1_conv (Conv2D)           (None, 7, 7, 32)          12832     
    _________________________________________________________________
    batch_normalization_31 (Batc (None, 7, 7, 32)          128       
    _________________________________________________________________
    leaky_re_lu_37 (LeakyReLU)   (None, 7, 7, 32)          0         
    _________________________________________________________________
    d_h2_conv (Conv2D)           (None, 4, 4, 64)          51264     
    _________________________________________________________________
    batch_normalization_32 (Batc (None, 4, 4, 64)          256       
    _________________________________________________________________
    leaky_re_lu_38 (LeakyReLU)   (None, 4, 4, 64)          0         
    _________________________________________________________________
    d_h3_conv (Conv2D)           (None, 2, 2, 128)         204928    
    _________________________________________________________________
    batch_normalization_33 (Batc (None, 2, 2, 128)         512       
    _________________________________________________________________
    leaky_re_lu_39 (LeakyReLU)   (None, 2, 2, 128)         0         
    _________________________________________________________________
    flatten_6 (Flatten)          (None, 512)               0         
    _________________________________________________________________
    d_h3_lin (Dense)             (None, 1)                 513       
    =================================================================
    Total params: 541,250
    Trainable params: 270,401
    Non-trainable params: 270,849
    _________________________________________________________________
    
    adversarial_model
    Model: "model_6"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_6 (InputLayer)         (None, 28, 28, 1)         0         
    _________________________________________________________________
    R (Model)                    (None, 28, 28, 1)         320993    
    _________________________________________________________________
    D (Model)                    (None, 1)                 270849    
    =================================================================
    Total params: 591,842
    Trainable params: 320,545
    Non-trainable params: 271,297
    _________________________________________________________________



```python
self.train(epochs=5, batch_size=128, sample_interval=500)
```

    Epoch (0/5)-------------------------------------------------


    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'
    /home/dolphin/.virtualenvs/bio/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
      'Discrepancy between trainable weights and collected trainable'


    Epoch (1/5)-------------------------------------------------
    Epoch (2/5)-------------------------------------------------
    Epoch (3/5)-------------------------------------------------
    Epoch (4/5)-------------------------------------------------



![png](/assets/images/alocc_files/alocc_33_3.png)



```python
# Load the epoch #4 saved weights.
self.adversarial_model.load_weights('./checkpoint/ALOCC_Model_3.h5')
```


```python
draw_histogram()
```


![png](/assets/images/alocc_files/alocc_35_0.png)



```python

```


```python

```


```python
reconstruction loss의 loss_weight 를 높혀서 트레이닝을 하면 D모델을 속이려는 성향보다 이미지를 깔끔하게 만드려는 성향이 강해지고
reconstruction loss의 loss_weight 를 낮혀서 트레이닝을 하면 이미지를 깔끔하게 바꾸려는 성향보다 어떻게 하든 D모델을 속이면 되는 쪽으로 학습이 된다.
```


```python

```
