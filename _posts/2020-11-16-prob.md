---
layout: article
title: The Basics of Probability
mathjax: true
toc : true
tags :
 - Probability
 - MLE
---




# Probability Theory


### Example: Fruits in Boxes


<img src = "/assets/images/prob_files/Fig_1.9.PNG">

  * (빨강, 파랑) 박스 선택 확률: (0.4, 0.6)
    + 확률변수 $B \in \lbrace r, b \rbrace$

      <br/>
      $$
      \begin{aligned}
      & \Pr(B = r) = 0.4 \\[11pt]
      & \Pr(B = b) = 0.6
      \end{aligned}
      $$
      <br/>
      <br/>
     
  * 특정 박스에서, 랜덤하게 과일(사과, 오렌지) 하나를 선택
    + 확률변수 $F \in \lbrace a, o \rbrace$
     
      <br/>
      $$
      \begin{aligned}
      & \Pr(F = a|B = r) = 1/4 \\[11pt]
      & \Pr(F = o|B = r) = 3/4 \\[11pt]
      & \Pr(F = a|B = b) = 3/4 \\[11pt]
      & \Pr(F = a|B = b) = 1/4 \\[11pt]
      \end{aligned}
      $$
      <br/>
      <br/>
      
  * The rules of probability
    
    <br/>
    $$
    \begin{aligned}
    & \Pr(X) = \sum_Y \Pr(X,Y) \\[11pt]
    & \Pr(X, Y) = \Pr(Y|X) \Pr(X) = \Pr(X|Y) \Pr(Y)
    \end{aligned}
    $$
    <br/>
    <br/>
    
  * Bayes' theorem
    
    <br/>
    $$
    \begin{aligned}
    \Pr(Y|X) = \frac{\Pr(X|Y) \Pr(Y)}{\Pr(X)}
    \end{aligned}
    $$
    <br/>
    <br/>
    
  * 내 손에 사과가 있을 때, 빨강 박스에서 뽑은걸까? 파랑 박스에서 뽑은걸까?
  
    <br/>
    $$
    \begin{aligned}
    \Pr(F = a) & = \sum_B \Pr(F = a, B) \\[11pt]
    & = \sum_B \Pr(F = a | B) \Pr(B) \\[11pt]
    & = \frac{1}{4} \times \frac{4}{10} + \frac{3}{4} \times \frac{6}{10} = \frac{11}{20}
    \end{aligned}
    $$
    <br/>
    <br/>

    <br/>
    $$
    \begin{aligned}
    \Pr(B = r | F = a) & = \frac{\Pr(F = a | B = r) \Pr(B = r)}{\Pr(F = a)} \\[11pt]
    & = \frac{1}{4} \times \frac{4}{10} \times \frac{20}{11} = \frac{2}{11}
    \end{aligned}
    $$
    <br/>
    <br/>

    <br/>
    $$
    \begin{aligned}
    \Pr(B = b | F = a) = 1 - \Pr(B = r | F = a) = \frac{9}{11}
    \end{aligned}
    $$
    <br/>
    <br/>


### Bernoulli Distribution


<img src = "/assets/images/prob_files/Fig_Bern.png">

  * Discrete random variable $X (\in \{ 0, 1 \}) \sim \text{Bern}(p)$

    <br/>
    $$
    \begin{aligned}
    & \Pr(X = 0) = 1 - p \\[11pt]
    & \Pr(X = 1) = p
    \end{aligned}
    $$
    <br/>
    <br/>

  * Probability mass function (pmf)
    
    <br/>
    $$
    \begin{aligned}
    f(x;p) = p^x (1-p)^{1-x}
    \end{aligned}
    $$
    <br/>
    <br/>
    
  * Expectation

    <br/>
    $$
    \begin{aligned}
    \mathbb{E}[X] = \sum_x x f(x) = p
    \end{aligned}
    $$
    <br/>
    <br/>
    
  * Variance

    <br/>
    $$
    \begin{aligned}
    \mathbb{V}[X] & = \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\[11pt]
    & = \left( \sum_x x^2 f(x) \right) - p^2 = p(1-p)
    \end{aligned}
    $$
    <br/>
    <br/>


### Gaussian Distribution


<img src = "/assets/images/prob_files/Fig_Gaussian.png">

  * Continuous random variable $X (\in \mathbb{R}^d) \sim N_d(\boldsymbol{\mu}, \Sigma)$
    + $d$-dimensional mean vector $\boldsymbol{\mu} = (\mu_1, \cdots, \mu_d)^T$
    + $(d \times d)$ matrix $\Sigma$

    <br/>
    
  * Probability density function (pdf)

    <br/>
    $$
    \begin{aligned}
    f(x;\boldsymbol{\mu}, \Sigma) = \frac{1}{(2 \pi)^{d/2}} \frac{1}{|\Sigma|^{1/2}} \exp \left\{ - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right\}
    \end{aligned}
    $$
    <br/>
    <br/>

  * Expectation

    <br/>
    $$
    \begin{aligned}
    \mathbb{E}[X] = \int_{\mathbf{x}} \mathbf{x} f(x;\boldsymbol{\mu}, \Sigma) d \mathbf{x} = \boldsymbol{\mu}
    \end{aligned}
    $$
    <br/>
    <br/>

  * Variance

    <br/>
    $$
    \begin{aligned}
    \mathbb{V}[X] = \Sigma
    \end{aligned}
    $$
    <br/>
    <br/>


# Maximum Likelihood Estimation (MLE)


### Intuition on MLE
  * 동전을 여러번 던져 다음과 같은 결과를 얻었다면 (앞: Head, 뒤: Tail), 동전 던지기는 아래의 두 분포 중 어떤 것을 따르고 있는 것일까?

    <br/>
    $$
    \begin{aligned}
    \lbrace H, H, H, T, H, H, T, H, H, H, H, T \rbrace
    \end{aligned}
    $$
    <br/>
    <br/>
    
<img src = "/assets/images/prob_files/Fig_MLE_Intuition.png">


### Example: Biased Coin
  * Given a biased coin, determine the probability of getting heads $p$ on the toss of a coin
  * Let $X = 1$ be the head, and $X = 0$ be the tail

    <br/>
    $$
    \begin{aligned}
    X \sim \text{Bern}(p)
    \end{aligned}
    $$
    <br/>
    <br/>

  * Observation $\mathcal{D} = \lbrace x_1, \cdots, x_n \rbrace$
  * Likelihood function

    <br/>
    $$
    \begin{aligned}
    L(\mathcal{D} ; p) & = \prod_{i=1}^n f(x_i ; p) \\[11pt]
    & = \prod_{i=1}^n p^{x_i} (1-p)^{1-{x_i}}
    \end{aligned}
    $$
    <br/>
    <br/>

  * Maximum likelihood estimation

    <br/>
    $$
    \begin{aligned}
    \hat{p} & = \text{argmax}_p \; L(\mathcal{D} ; p) \\[11pt]
    & = \text{argmax}_p \; \log L(\mathcal{D} ; p) \\[11pt]
    & = \text{argmax}_p \sum_{i=1}^n \left(x_i \log{p} + (1 - x_i) \log(1-p) \right)
    \end{aligned}
    $$
    <br/>
    <br/>

  * Maximum likelihood estimator of Bernoulli distribution

    <br/>
    $$
    \begin{aligned}
    \hat{p}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n x_i
    \end{aligned}
    $$
    <br/>
    <br/>


